-*- eval: (progn (pyvenv-workon "ml364") (auto-revert-mode 1))-*-
#+STARTUP: indent
#+OPTIONS: author:Ishai
#+TITLE: Market Bot
#+TODO: TODO IN-PROGRESS WAITING | DONE | DEPRECATED ABANDONED
#+OPTIONS: toc:nil

* Notes

** general

- ML for discrete optimization

  - MF Balcan submodular optimization and ML

    - [[http://www.cs.cmu.edu/~ninamf/papers/learning-submodular-sicomp.pdf][paper]]

    - [[iesg.eecs.berkeley.edu/Colloquium/2015/MariaBalcan_EECS_Colloquium_20150429.mp4?_ga=2.235411201.1670012923.1524356790-1392980825.1513883180][video]]

  - [[http://www.cs.cmu.edu/~ninamf/papers/learn-branch.pdf][learning to branch]] (and bound)

  - [[https://scholar.harvard.edu/files/ericbalkanski/files/learning-to-optimize-combinatorial-functions.pdf][learning to optimize comb prob]]

- How to keep track of iterators?

- NN for discrete domains

  - [[https://www.youtube.com/watch?v=ixtEeS6aCKU&t=0s&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=24][Aviv Tamar lecture]]

  - [[file:refs/VIN.pdf][VIN]]

  - [[https://arxiv.org/abs/1708.07280][Learning Generalized Reactive Policies using Deep Neural Networks]]

    - [[https://github.com/maxgold/generalized-gcn][max gold repo]]

- Intro to queryverse:

  - [[https://juliabox.com/notebook/notebooks/tutorials/intro-to-the-queryverse/AnthoffQueryverseJune2018.ipynb][notebook]]

  - [[https://www.youtube.com/watch?v=2oXSA2w-p28][youtube juliacon18]]

    
** Questions

- Ira

  - is new collect.py 


** Ideas

*** non uniform resampling

- upsample using nufft from [[https://github.com/MikaelSlevinsky/FastTransforms.jl][FastTransforms]]

- downsample using splines / etc


*** Output to postgres


** Next Up

*** future

**** circuit discovery


*** current

**** Upload first resutls


**** Create test code


*** past

**** Examine why first difference fails

- why are there only 0s and very large values in datevalue column?


**** Run model on first difference across time


**** Model of conversion rates

- Try conv rather than matrix mul

- continue with generator, use approach like [[https://github.com/FluxML/model-zoo/blob/7fc4b76a0ca2df173a68393dd2b41181b9c66496/scene_category/train.jl][here]]

- Start with [[phonemes][seq2seq model]]

  - Try to get binning (digitization?) working

- predict conversion rate over some horizon

- use prediction for q value iteration


**** DONE resampling

- Starts with splines in [[https://github.com/JuliaMath/Interpolations.jl][Interpolatoins.jl]]

  - used dierckx instead

- concatenate interpolation into array with indecies signal, 'p' or 'v', value at timestamp

- Make sure to record frequency somewhere


** Quantization

- Look at how magenta nsynth (others) do it.


** ABANDONED Tasks

*** TODO Week of Jan 9th

**** Ira

- Determine which price information to add before dense.

- Normalize new data.

- determine validation startegy

- experiment with architecture changes (sequence length, etc.)


**** Ishai

- market gym

- help with stuff

- anything else?


*** DONE Week of Jan 5th
DEADLINE: <2018-01-05 Fri 10:00>

**** Ishai

***** 


**** Ira

***** Preprocess / de-trend
:PROPERTIES:
:Effort:   4h
:END:

****** Goals

Strategy for preprocessing data that filters out 

******* Periodic components

- R stl library?


******* Trend

- idea: first difference

- other?


******* Bias / Variance (normalization)

- idea: center + scale based on training data


******* Other



****** Refs

- https://robjhyndman.com/papers/icdm2015.pdf
- stl library




***** Qunatization / loss function
:PROPERTIES:
:Effort:   8h
:END:

****** Goal

Should we be representing loss function as cross ent with logits? 

- According to [[https://github.com/ibab/tensorflow-wavenet/blob/master/wavenet/model.py#L665][wavenet]], yes.

- [[https://github.com/ibab/tensorflow-wavenet/blob/master/wavenet/ops.py#L64][wavenet mu-law]] for quantization

- mu law quantization is applied to both inputs and outputs.


* Reading

** Future

- value iteration networks

- [[http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/][Model agnostic meta-learning blog]]

- [[https://arxiv.org/abs/1606.04474][Learning to learn by gradient descent by gradient descent]]

- [[https://arxiv.org/abs/1611.03824][Learning to Learn without Gradient Descent by Gradient Descent]]

  
* Aux
:PROPERTIES:
:header-args:  :exports none
:END:

** Jl Kernel

*** kernel remote - in lan

#+NAME: kernel_name
marketbot-julia

#+NAME: kernel_name_trim
#+BEGIN_SRC elisp :var kern=kernel_name :results value
  (s-trim kern)
#+END_SRC

#+RESULTS: kernel_name_trim
: marketbot-julia

#+BEGIN_SRC sh :session jlssh :results none :var kern=kernel_name_trim
  echo "tp2:/run/user/1000/jupyter/$kern.json"
#+END_SRC

#+BEGIN_SRC sh :session jlssh :results none :var kern=kernel_name_trim
  mkdir /run/user/1000/jupyter/
  scp "tp2:/run/user/1000/jupyter/$kern.json" "/run/user/1000/jupyter/$kern.json" && jupyter console --existing $kern.json --ssh tp2
#+END_SRC


*** Julia Helpers
:PROPERTIES:
:header-args: jupyter-julia :session marketbot-julia-ssh.json  :results raw drawer :tangle mbot.jl :eval never-export
:END:

#+BEGIN_SRC jupyter-julia
  using Logging
  LogLevel(Logging.Info)
  using PyCall
  """
  Load a python module from a given absolute path
  """
  function pyimport_module(filepath, modulename)
      local imp = PyCall.pywrap(PyCall.pyimport("imp"))
      local spec = imp.util[:spec_from_file_location](modulename, filepath)
      local preloadmod = imp.util[:module_from_spec](spec)
      spec[:loader][:exec_module](preloadmod)
      return preloadmod
  end
#+END_SRC
#+RESULTS:
:RESULTS:
# Out[1]:
: pyimport_module
:END:


** Py Kernel

*** kernel remote - in lan

#+NAME: pykernel_name
mbotpy

#+NAME: pykernel_name_trim
#+BEGIN_SRC elisp :var kern=pykernel_name :results value
  (s-trim kern)
#+END_SRC

#+RESULTS: pykernel_name_trim
: mbotpy

#+BEGIN_SRC sh :session pyssh :results none :var kern=pykernel_name_trim
  echo "tp2:/run/user/1000/jupyter/$kern.json"
#+END_SRC

#+BEGIN_SRC sh :session pyssh :results none :var kern=pykernel_name_trim
  scp "tp2:/run/user/1000/jupyter/$kern.json" "/run/user/1000/jupyter/$kern.json"
#+END_SRC


#+BEGIN_SRC sh :session pyssh :results none :var kern=pykernel_name_trim
  jupyter console --existing $kern.json --ssh tp2
#+END_SRC


*** ipython settings
:PROPERTIES:
:header-args: ipython :session mbotpy-ssh.json :results raw drawer :tangle col.py :eval never-export
:END:

#+BEGIN_SRC ipython
  import matplotlib
  # matplotlib.use('Agg')
  %matplotlib inline
  %load_ext autoreload
  %autoreload 2
  # cd /home/ubuntu/Aisin/
  from matplotlib import pyplot as plt
  import pandas as pd
  pd.set_option("display.max_columns",301)
  import os
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[20]:
:END:

#+BEGIN_SRC ipython :async t
  1+2
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[21]:
: 3
:END:


*** other python things
:PROPERTIES:
:header-args: ipython :session mx2_mus_kernel-ssh.json :results raw drawer :tangle col.py :eval never-export
:END:

#+BEGIN_SRC ipython
def decorator_factory(key, value):
    def msg_decorator(f):
        def inner_dec(*args, **kwargs):
            g = f.__globals__  # use f.func_globals for py < 2.6
            sentinel = object()

            oldvalue = g.get(key, sentinel)
            g[key] = value

            try:
                res = f(*args, **kwargs)
            finally:
                if oldvalue is sentinel:
                    del g[key]
                else:
                    g[key] = oldvalue

            return res
        return inner_dec
    return msg_decorator

#+END_SRC

#+RESULTS:
: # Out[3]:




** slack log in

#+BEGIN_SRC elisp :results none
  (slack-register-team
   :name "bettingstrategies"
   :default t
   :client-id "ishaikones@gmail.com"
   :client-secret "barbarboots"
   :token "xoxs-151077066903-150475549493-300542321955-38e3d5a804"
   :subscribed-channels '(general))
#+END_SRC


* README

** MarketBot2.0

This project tries to forecast short term bitcoin price fluctuations using neural networks.

It's primary objective was to provide an opprtunity to try out the new release of [[https://github.com/JuliaLang/julia/][Julia]] (0.7/1.0) as well as try out sequence forecasting using attention.

Using [[src/utils/collect.py][python websockets]] to extract data from coinbase, it uses julia to [[Clean Up][process and upsample]] the collected data and then calls an [[src/pytf][estimator based tensorflow model]] to train on the data.


* snippets

** Julia
:PROPERTIES:
:header-args: jupyter-julia :session marketbot-julia-ssh.json  :results raw drawer :tangle mbot.jl :eval never-export :async t
:END:

*** aux
:PROPERTIES:
:header-args: :eval never
:END:

#+BEGIN_SRC jupyter-julia
  # using Distributed
  # addprocs(7)
#+END_SRC


*** preprocess and explore

**** Julia 0.7

***** V2
<<redo-input>>
#+BEGIN_SRC jupyter-julia
  using Queryverse, JuliaDB
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[2]:
:END:

#+BEGIN_SRC jupyter-julia :results output
  datadir = "/home/ishai/Documents/work/marketbot/src/utils/data/raw"
  run(`ls -lh $(datadir)`)
#+END_SRC

#+RESULTS:
:RESULTS:
total 46M
-rw-rw-r-- 1 ishai ishai 2.5M Aug 14 00:12 20180813-171152.csv
-rw-rw-r-- 1 ishai ishai 1.7M Aug 14 02:50 20180814-001303.csv
-rw-rw-r-- 1 ishai ishai 1.7M Aug 15 03:42 20180815-004411.csv
-rw-rw-r-- 1 ishai ishai 1.6M Aug 15 12:38 20180815-113032.csv
-rw-rw-r-- 1 ishai ishai 457K Aug 15 13:46 20180815-131857.csv
-rw-rw-r-- 1 ishai ishai  14K Aug 15 13:48 20180815-134729.csv
-rw-rw-r-- 1 ishai ishai 1.1M Aug 15 15:15 20180815-134950.csv
-rw-rw-r-- 1 ishai ishai  37M Aug 18 01:37 20180815-160006.csv
:END:

#+BEGIN_SRC jupyter-julia :results output
  fname = maximum(readdir(datadir))
  run(`head $(joinpath(datadir, fname))`)
#+END_SRC

#+RESULTS:
:RESULTS:
2018-08-15T20:00:07.341000Z,289.01000000,0.95307512,ETH-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.05064896,BTC-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.06453496,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.52758604,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.75478104,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00118000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.56368644,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.03650000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00810000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00380000,BTC-USD
:END:

# DateTime does not recognize "2018-08-14T04:13:06.697000Z" so need to adjust string before processing. 
# Haven't figured out how to correctly use [[https://juliacomputing.com/TextParse.jl/stable/#TextParse.CustomParser][CustomParser]], but it looks like a reasonable approach to converting.
#+BEGIN_SRC jupyter-julia :eval never :exports never
  using TextParse
  cDateTime = CustomParser(x->x[1:end-5], DateTime)
#+END_SRC

# annoying thigns so far - 1.how to set types, 2. not much documentation 
# Later - set types using TextParse
#+BEGIN_SRC jupyter-julia
  using Dates
  colnames = ["date", "price", "size", "product_id"]
  t = Queryverse.load(joinpath(datadir, fname),
                      colnames=colnames,
                      colparsers=Dict(:date=>String, :price=>Float64, :size=>Float64, :product_id=>String)) |>
                          @map({:date=>DateTime(_.date[1:end-5]), _.price, _.size, _.product_id}) |> # cast to DateTime
                          table
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[6]:
#+BEGIN_EXAMPLE
  Table with 623229 rows, 4 columns:
  date                    price    size        product_id
  ───────────────────────────────────────────────────────
  2018-08-15T20:00:07.87  6383.82  0.050649    "BTC-USD"
  2018-08-15T20:00:07.87  6383.82  0.064535    "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  1.52759     "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  1.75478     "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.00118     "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.563686    "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0365      "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0081      "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0038      "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.2         "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.00998106  "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0633251   "BTC-USD"
  ⋮
  2018-08-18T05:37:32.6   310.51   1.3413      "ETH-USD"
  2018-08-18T05:37:32.6   310.5    8.46415     "ETH-USD"
  2018-08-18T05:37:32.6   310.5    1.9         "ETH-USD"
  2018-08-18T05:37:32.93  310.51   0.0432801   "ETH-USD"
  2018-08-18T05:37:33.72  5709.16  0.00409268  "BTC-EUR"
  2018-08-18T05:37:34.41  14.22    137.769     "ETC-USD"
  2018-08-18T05:37:34.72  6517.65  0.0376063   "BTC-USD"
  2018-08-18T05:37:35.14  14.19    145.838     "ETC-USD"
  2018-08-18T05:37:36.13  5709.16  0.022136    "BTC-EUR"
  2018-08-18T05:37:36.62  14.22    3.35943     "ETC-USD"
  2018-08-18T05:37:37.01  59.88    0.7215      "LTC-USD"
#+END_EXAMPLE
:END:

Assign primary keys
#+BEGIN_SRC jupyter-julia
   t = table(t, pkey=(:product_id, :date))
#+END_SRC

Lets check if the data is sorted with in groups.
#+BEGIN_SRC jupyter-julia
  a = t |> @groupby(_.product_id, _.date) |> @map({origin=key(_), result=issorted(_)}) |> @map(all(_.result))
  all(a)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[11]:
: true
:END:

This opens a new window
#+BEGIN_SRC jupyter-julia :eval never
  t |> Voyager()
#+END_SRC

[[file:./voyager.png]]

The default view doesn't work for volume (size), lets try line instead
#+BEGIN_SRC jupyter-julia
  @time t |> @vlplot(:line, x=:date, y=:size, color="product_id")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[16]:
[[file:./obipy-resources/TMY56X.png]]
:END:

Not that nice. 

# log Time difference counts
#+BEGIN_SRC jupyter-julia :exports none :eval never
  tdiff = t |> @filter(_.product_id=="ETH-USD") |> @map(_.date) |> collect |> diff |> @map({delta=_});
  tdiff |> @map({delta=log(Dates.value(_.delta))}) |> @vlplot(mark={:bar, clip=true}, x={:delta, bin={maxbins=1000}, scale={domain=[0,15], clip=true}}, y={"count()", axis={title="Time-difference counts"}})
#+END_SRC

Plot time differences between observations
#+BEGIN_SRC jupyter-julia
  t |>
      @groupby(_.product_id) |>
      @map({product_id=key(_), delta=diff(_.date)}) |>
      @mapmany(_.delta, {delta=Dates.value(__), product_id=_.product_id}) |> # __ is the individual element in row
      @filter(_.delta>0) |>
      @vlplot(mark={:line, clip=true},
              x={:delta, axis={title="ΔTime"}, bin={maxbins=100000}, scale={domain=[0,10000], clip=true}},
              y={"count()", axis={title="Counts"}  #, scale={domain=[0,100], clip=true}
                 },
              color=:product_id,
              height=300,
              width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[136]:
[[file:./obipy-resources/bvknN0.png]]
:END:

/log/ time differences (between observations)
#+BEGIN_SRC jupyter-julia
  t |>
      @groupby(_.product_id) |>
      @map({product_id=key(_), delta=diff(_.date)}) |>
      @mapmany(_.delta, {delta=__, product_id=_.product_id}) |> # __ is the individual element in row
      @map({_.product_id, delta=log(Dates.value(_.delta))}) |>
      @vlplot(mark={:line, clip=true},
              x={:delta, axis={title="log(ΔTime)"}, bin={maxbins=100}},
              y={"count()", axis={title="Counts"}, scale={domain=[0,2500], clip=true}},
              color=:product_id, height=300, width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[34]:
[[file:./obipy-resources/IT9gre.png]]
:END:

Irregular signal. Resample to have observations at regular intervals.

Review duplicates:

#+BEGIN_SRC jupyter-julia
  t |> @groupby((_.product_id, _.date)) |> @map({origin=key(_), Count=length(_)}) |> @filter(_.Count > 1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[41]:
#+BEGIN_EXAMPLE
  ?x2 query result
  origin                              │ Count
  ────────────────────────────────────┼──────
  ("BTC-USD", 2018-08-15T20:00:07.87) │ 2
  ("BTC-USD", 2018-08-15T20:00:08.44) │ 12
  ("BTC-USD", 2018-08-15T20:00:08.46) │ 2
  ("BTC-USD", 2018-08-15T20:00:11.05) │ 6
  ("ETH-USD", 2018-08-15T20:00:11.2)  │ 2
  ("BTC-USD", 2018-08-15T20:00:12.16) │ 4
  ("BTC-USD", 2018-08-15T20:00:12.93) │ 11
  ("BTC-USD", 2018-08-15T20:00:13.02) │ 2
  ("BTC-USD", 2018-08-15T20:00:14.43) │ 4
  ("ETH-EUR", 2018-08-15T20:00:17.29) │ 5
  ... with more rows
#+END_EXAMPLE
:END:

These are due to transactions that are cleared simultaneously.
#+BEGIN_SRC jupyter-julia
using Statistics
t = t |> @groupby((_.product_id, _.date)) |> @map({product_id=key(_)[1], date=key(_)[2], price=median(_.price),  size=sum(_.size)}) |> table;
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[8]:
:END:

#+BEGIN_SRC jupyter-julia :eval never :exports none
  using Query, Statistics
  t = @from i in t begin
      @group i by i.product_id, i.date into g
      @select {product_id=key(g)[1], date=key(g)[2], price=median(g.price), size=sum(g.size)}
      @collect table
  end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[79]:
#+BEGIN_EXAMPLE
  Table with 351419 rows, 4 columns:
  product_id  date                    price    size
  ───────────────────────────────────────────────────────
  "BTC-USD"   2018-08-15T20:00:07.87  6383.82  0.115184
  "BTC-USD"   2018-08-15T20:00:08.44  6383.81  4.4218
  "BTC-USD"   2018-08-15T20:00:08.46  6383.58  0.001
  "BTC-USD"   2018-08-15T20:00:08.48  6383.35  0.00049985
  "BTC-USD"   2018-08-15T20:00:08.56  6382.54  0.00810663
  "BTC-USD"   2018-08-15T20:00:08.61  6382.53  0.00105
  "ETC-USD"   2018-08-15T20:00:09.78  12.11    72.8834
  "ETH-USD"   2018-08-15T20:00:10.48  289.01   2.1008
  "BTC-USD"   2018-08-15T20:00:11.05  6374.65  0.197664
  "ETH-USD"   2018-08-15T20:00:11.2   289.0    0.51067
  "BTC-USD"   2018-08-15T20:00:12.16  6373.0   2.0
  "BTC-USD"   2018-08-15T20:00:12.93  6374.29  1.68051
  ⋮
  "ETH-USD"   2018-08-18T05:37:24.32  310.85   4.9996
  "ETC-USD"   2018-08-18T05:37:29.94  14.195   74.6061
  "ETH-USD"   2018-08-18T05:37:32.6   310.52   11.9
  "ETH-USD"   2018-08-18T05:37:32.93  310.51   0.0432801
  "BTC-EUR"   2018-08-18T05:37:33.72  5709.16  0.00409268
  "ETC-USD"   2018-08-18T05:37:34.41  14.22    137.769
  "BTC-USD"   2018-08-18T05:37:34.72  6517.65  0.0376063
  "ETC-USD"   2018-08-18T05:37:35.14  14.19    145.838
  "BTC-EUR"   2018-08-18T05:37:36.13  5709.16  0.022136
  "ETC-USD"   2018-08-18T05:37:36.62  14.22    3.35943
  "LTC-USD"   2018-08-18T05:37:37.01  59.88    0.7215
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  t |> @groupby((_.product_id, _.date)) |> @map({origin=key(_), Count=length(_)}) |> @filter(_.Count > 1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[10]:
#+BEGIN_EXAMPLE
  0x2 query result
  origin │ Count
  ───────┼──────
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  t |> @vlplot(mark={:line, clip=true},
               x={:date},
               y={:price},
               color=:product_id, height=300, width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[94]:
[[file:./obipy-resources/0XjhGy.png]]
:END:

Median time difference:
#+BEGIN_SRC jupyter-julia
  meddiff = t |> @groupby(_.product_id, _.date) |> @map({key(_), median(diff(Dates.values(Float64.(_))))}) |> DataFrame
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[151]:
#+BEGIN_EXAMPLE
  15×2 DataFrames.DataFrame
  │ Row │ key     │ _2_     │
  ├─────┼─────────┼─────────┤
  │ 1   │ BTC-USD │ 1190.0  │
  │ 2   │ ETC-USD │ 890.0   │
  │ 3   │ ETH-USD │ 1780.0  │
  │ 4   │ ETH-EUR │ 8850.0  │
  │ 5   │ BCH-USD │ 6360.0  │
  │ 6   │ LTC-BTC │ 24825.0 │
  │ 7   │ BTC-EUR │ 5910.0  │
  │ 8   │ LTC-USD │ 3090.0  │
  │ 9   │ BTC-GBP │ 15390.0 │
  │ 10  │ LTC-EUR │ 18665.0 │
  │ 11  │ ETH-BTC │ 3595.0  │
  │ 12  │ ETC-EUR │ 29360.0 │
  │ 13  │ BCH-EUR │ 38610.0 │
  │ 14  │ ETC-BTC │ 3990.0  │
  │ 15  │ BCH-BTC │ 16330.0 │
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia 
  t = setcol(t, :datevalue, Queryverse.map(r->Dates.value(r.date), t))
  mindate = minimum(JuliaDB.select(t, :datevalue))
  t = setcol(t, :datevalue, Queryverse.map(r->r.datevalue-mindate, t))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[9]:
#+BEGIN_EXAMPLE
  Table with 351419 rows, 5 columns:
  product_id  date                    price    size        datevalue
  ──────────────────────────────────────────────────────────────────
  "BTC-USD"   2018-08-15T20:00:07.87  6383.82  0.115184    0
  "BTC-USD"   2018-08-15T20:00:08.44  6383.81  4.4218      570
  "BTC-USD"   2018-08-15T20:00:08.46  6383.58  0.001       590
  "BTC-USD"   2018-08-15T20:00:08.48  6383.35  0.00049985  610
  "BTC-USD"   2018-08-15T20:00:08.56  6382.54  0.00810663  690
  "BTC-USD"   2018-08-15T20:00:08.61  6382.53  0.00105     740
  "ETC-USD"   2018-08-15T20:00:09.78  12.11    72.8834     1910
  "ETH-USD"   2018-08-15T20:00:10.48  289.01   2.1008      2610
  "BTC-USD"   2018-08-15T20:00:11.05  6374.65  0.197664    3180
  "ETH-USD"   2018-08-15T20:00:11.2   289.0    0.51067     3330
  "BTC-USD"   2018-08-15T20:00:12.16  6373.0   2.0         4290
  "BTC-USD"   2018-08-15T20:00:12.93  6374.29  1.68051     5060
  ⋮
  "ETH-USD"   2018-08-18T05:37:24.32  310.85   4.9996      207436450
  "ETC-USD"   2018-08-18T05:37:29.94  14.195   74.6061     207442070
  "ETH-USD"   2018-08-18T05:37:32.6   310.52   11.9        207444730
  "ETH-USD"   2018-08-18T05:37:32.93  310.51   0.0432801   207445060
  "BTC-EUR"   2018-08-18T05:37:33.72  5709.16  0.00409268  207445850
  "ETC-USD"   2018-08-18T05:37:34.41  14.22    137.769     207446540
  "BTC-USD"   2018-08-18T05:37:34.72  6517.65  0.0376063   207446850
  "ETC-USD"   2018-08-18T05:37:35.14  14.19    145.838     207447270
  "BTC-EUR"   2018-08-18T05:37:36.13  5709.16  0.022136    207448260
  "ETC-USD"   2018-08-18T05:37:36.62  14.22    3.35943     207448750
  "LTC-USD"   2018-08-18T05:37:37.01  59.88    0.7215      207449140
#+END_EXAMPLE
:END:

Lowest median time difference between samples is for BCH-EUR at around 38 seconds between observations. Still, try using .5 second sampling frequency to take advantage of higher frequency signals.

Start and end of interval
#+BEGIN_SRC jupyter-julia  
  sampling_rate = 500
  tstart = minimum(JuliaDB.select(t, :datevalue))
  tend = maximum(JuliaDB.select(t, :datevalue))
  sample_points = tstart:sampling_rate:tend # iterating in milliseconds
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[10]:
: 0:500:207449000
:END:

Aggregate values in interval 
#+BEGIN_SRC jupyter-julia
  t = setcol(t, :dateval_rounded, ceil.(Int64, JuliaDB.select(t, :datevalue) ./ sampling_rate) .* sampling_rate)
#+END_SRC

#+BEGIN_SRC jupyter-julia
  tnew = t |> @groupby((_.product_id, _.dateval_rounded)) |> @map({product_id=key(_)[1], dateval=key(_)[2], price=median(_.price), size=sum(_.size)}) |> table
#+END_SRC

#+BEGIN_SRC jupyter-julia
#+END_SRC

#+BEGIN_SRC jupyter-julia
  tmptbl = table(columns(table(map(t->(t, Missing, Missing), sample_points))), names=[:dateval, :price, :siz])
  # tmptbl = table(collect(sample_points), names=[:dateval], pkey=:dateval)
  tmpdf = DataFrame(Dict(:dateval=>collect(sample_points)))

#+END_SRC

#+BEGIN_SRC jupyter-julia
  tnew |> @groupby _.product_id |> @map(join(DataFrame(_),tmpdf, on :dataeval))
#+END_SRC

#+BEGIN_SRC jupyter-julia

  tnew |> @groupby _.product_id |> @map({:product_id=key(_), :datevalue=sample_points, :price=})
#+END_SRC

#+BEGIN_SRC jupyter-julia
  @apply tnew :product_id begin
      @where x in 
      sort(_, :dateval_rounded) 
      tnew |> @groupby(())
#+END_SRC

Apply function across groups and unstack.
#+BEGIN_SRC jupyter-julia
  tnew = JuliaDB.groupby(date_interp_t, t, :product_id, flatten=true)
  tnew = IndexedTables.unstack(table(tnew), :datevalue; variable=:product_id, value=:price)
#+END_SRC


#+BEGIN_SRC jupyter-julia :eval never
  using Dierckx, NamedTuples
  date_interp(x, y) = Spline1D(x,y; k=2, bc="extrapolate", s=1e-4)(sample_points)
  date_interp_t(_t) = table(@NT(datevalue=sample_points, price=date_interp(JuliaDB.select(_t, :datevalue), JuliaDB.select(_t, :price))))
  # date_interp_t(_t) = date_interp(_t[:datevalue], _t[:price])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[262]:
: date_interp_t (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  using Interpolations, NamedTuples
  date_interp(x, y) = LinearInterpolation(x,y, extrapolation_bc=Linear())(collect(sample_points))
  date_interp_t(_t) = table(@NT(datevalue=sample_points .- minimum(sample_points), price=date_interp(IndexedTables.select(table(_t), :datevalue), IndexedTables.select(table(_t), :price))))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[264]:
: date_interp_t (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  using StatPlots
  temp = t |> @filter(_.product_id =="BCH-BTC") |> table |> date_interp_t;
  temp2 = t |> @filter(_.product_id =="BCH-BTC") |> table;
  @df temp plot(:datevalue, :price, size=(2000,1000))
  @df temp2 scatter!(:datevalue, :price, markersize=1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[265]:
[[file:obipy-resources/GHjZ7d.png]]
:END:

Apply function across groups and unstack.
#+BEGIN_SRC jupyter-julia
  tnew = JuliaDB.groupby(date_interp_t, t, :product_id, flatten=true)
  tnew = IndexedTables.unstack(table(tnew), :datevalue; variable=:product_id, value=:price)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[267]:
#+BEGIN_EXAMPLE
  Table with 414899 rows, 16 columns:
  Columns:
  [1m#   [22m[1mcolname    [22m[1mtype[22m
  ─────────────────────────────────
  1   datevalue  Int64
  2   BCH-BTC    DataValue{Float64}
  3   BCH-EUR    DataValue{Float64}
  4   BCH-USD    DataValue{Float64}
  5   BTC-EUR    DataValue{Float64}
  6   BTC-GBP    DataValue{Float64}
  7   BTC-USD    DataValue{Float64}
  8   ETC-BTC    DataValue{Float64}
  9   ETC-EUR    DataValue{Float64}
  10  ETC-USD    DataValue{Float64}
  11  ETH-BTC    DataValue{Float64}
  12  ETH-EUR    DataValue{Float64}
  13  ETH-USD    DataValue{Float64}
  14  LTC-BTC    DataValue{Float64}
  15  LTC-EUR    DataValue{Float64}
  16  LTC-USD    DataValue{Float64}
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia :async t
  tnew[end-100:end] |> DataFrame |> tail
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[268]:
#+BEGIN_EXAMPLE
  6×16 DataFrame. Omitted printing of 9 columns
  │ Row │ datevalue │ BCH-BTC │ BCH-EUR │ BCH-USD │ BTC-EUR │ BTC-GBP │ BTC-USD │
  ├─────┼───────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤
  │ 1   │ 207446500 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5251.41 │ 6517.65 │
  │ 2   │ 207447000 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5252.02 │ 6517.65 │
  │ 3   │ 207447500 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5252.63 │ 6517.65 │
  │ 4   │ 207448000 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5253.23 │ 6517.65 │
  │ 5   │ 207448500 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5253.84 │ 6517.65 │
  │ 6   │ 207449000 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5254.44 │ 6517.65 │
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  showcols(DataFrame(tnew))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[22]:
#+BEGIN_EXAMPLE
  16×5 DataFrame
  │ Row │ variable  │ eltype  │ nmissing │ first     │ last       │
  ├─────┼───────────┼─────────┼──────────┼───────────┼────────────┤
  │ 1   │ datevalue │ Float64 │          │ 6.367e13  │ 6.36703e13 │
  │ 2   │ BCH-BTC   │ Float64 │ 0        │ 0.0808327 │ 0.09069    │
  │ 3   │ BCH-EUR   │ Float64 │ 0        │ 458.773   │ 520.15     │
  │ 4   │ BCH-USD   │ Float64 │ 0        │ 519.021   │ 589.46     │
  │ 5   │ BTC-EUR   │ Float64 │ 0        │ 5622.95   │ 5709.16    │
  │ 6   │ BTC-GBP   │ Float64 │ 0        │ 5050.2    │ 5254.44    │
  │ 7   │ BTC-USD   │ Float64 │ 0        │ 6383.82   │ 6517.65    │
  │ 8   │ ETC-BTC   │ Float64 │ 0        │ 0.00191   │ 0.00217    │
  │ 9   │ ETC-EUR   │ Float64 │ 0        │ 10.757    │ 12.4522    │
  │ 10  │ ETC-USD   │ Float64 │ 0        │ 12.11     │ 14.2251    │
  │ 11  │ ETH-BTC   │ Float64 │ 0        │ 0.0451994 │ 0.04781    │
  │ 12  │ ETH-EUR   │ Float64 │ 0        │ 254.655   │ 273.216    │
  │ 13  │ ETH-USD   │ Float64 │ 0        │ 289.046   │ 310.391    │
  │ 14  │ LTC-BTC   │ Float64 │ 0        │ 0.0087506 │ 0.00915569 │
  │ 15  │ LTC-EUR   │ Float64 │ 0        │ 49.328    │ 52.4154    │
  │ 16  │ LTC-USD   │ Float64 │ 0        │ 56.0264   │ 59.8801    │
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  sum(isnan.(IndexedTables.select(tnew, :datevalue)))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[269]:
: 0
:END:

Replace missing + mandatory column wise vs row wise comparison:

#+BEGIN_SRC jupyter-julia :async t :results output
  using Query
  dfnew = tnew |> DataFrame
  @time for i in size(dfnew)[1]-1:-1:1
      for c in 1:size(dfnew)[2]
          if isnan(dfnew[i,c])
              dfnew[i,c] = dfnew[i+1,c]
          end
      end
  end  
#+END_SRC

#+RESULTS:
:RESULTS:
  2.476500 seconds (16.17 M allocations: 367.061 MiB, 52.51% gc time)
:END:

#+BEGIN_SRC jupyter-julia :async t :results output
  using Query
  dfnew = tnew |> DataFrame
  @time for c in 1:size(dfnew)[2]
      for i in size(dfnew)[1]-1:-1:1
          if isnan(dfnew[i,c])
              dfnew[i,c] = dfnew[i+1,c]
          end
      end
  end  
#+END_SRC

#+RESULTS:
:RESULTS:
  1.876617 seconds (26.94 M allocations: 512.453 MiB, 23.32% gc time)
:END:

No more nulls
#+BEGIN_SRC jupyter-julia
  tnew |> @map(isnan.(values(_))) |> table |> columns |> @map(sum(_)) |> maximum
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[270]:
: 0
:END:

Save
#+BEGIN_SRC jupyter-julia
  # tnew |> save("postproc2.feather")
  tnew |> save("postproc2.csv")
#+END_SRC


Annoyingly, there is some non-negligible loss in precision after saving the data
#+BEGIN_SRC jupyter-julia
  df = load("postproc2.csv") |> DataFrame
  println(sum((convert(Array{Float64}, DataFrame(tnew)) .- convert(Array{Float64}, df)).^2, 1))
#+END_SRC

#+RESULTS:
:RESULTS:
[3.04944 2.56476e-30 3.36042e-25 1.9387e-25 4.96308e-24 8.27181e-24 1.65436e-24 2.37737e-33 2.54644e-27 2.22143e-27 8.44039e-31 2.488e-25 2.55263e-25 6.31163e-32 4.29645e-26 3.1302e-26]
┌ Warning: `sum(a::AbstractArray, dims)` is deprecated, use `sum(a, dims=dims)` instead.
│   caller = top-level scope at In[108]:1
└ @ Core In[108]:1
:END:

/3.04944/ - sum of squares for datevalue is non-negligible. Maybe because saved as csv or because originally used float64 for dateavalue.

#+BEGIN_SRC jupyter-julia
  maximum(diff(df[:datevalue])), minimum(diff(df[:datevalue]))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[149]:
: (500.015625, 499.984375)
:END:

Probably should have converted it to Int64

#+BEGIN_SRC jupyter-julia
maximum(diff(convert(Array{Float64}, df), 1)[:,2:end], 1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[157]:
#+BEGIN_EXAMPLE
  1×15 Array{Float64,2}:
  0.000302757  2.05052  1.69563  23.0194  …  1.11495  2.0e-5  0.208  0.164176
#+END_EXAMPLE
:END:

Can probably use a better smoothing approach here.


Using Feather api directly seems to work
#+BEGIN_SRC jupyter-julia
  using Feather
  Feather.write("postproc3.feather", DataFrame(tnew))
#+END_SRC

#+BEGIN_SRC jupyter-julia
  load("postproc3.feather") |> @map(isnan.(values(_))) |> table |> columns |> @map(sum(_)) |> maximum
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[285]:
: 0
:END:

#+BEGIN_SRC jupyter-julia :results output
  df = load("postproc3.feather") |> DataFrame
  println(sum((convert(Array{Float64}, DataFrame(tnew)) .- convert(Array{Float64}, df)).^2, 1))
#+END_SRC

#+RESULTS:
:RESULTS:
[0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]
┌ Warning: `sum(a::AbstractArray, dims)` is deprecated, use `sum(a, dims=dims)` instead.
│   caller = top-level scope at In[289]:2
└ @ Core In[289]:2
:END:


***** V1
# Some of the queryverse commands have changed for 0.7 and won't work here

#+BEGIN_SRC jupyter-julia
  using Queryverse, JuliaDB
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[2]:
:END:

#+BEGIN_SRC jupyter-julia :results output
  datadir = "/home/ishai/Documents/work/marketbot/src/utils/data/raw"
  run(`ls -lh $(datadir)`)
#+END_SRC

#+RESULTS:
:RESULTS:
total 46M
-rw-rw-r-- 1 ishai ishai 2.5M Aug 14 00:12 20180813-171152.csv
-rw-rw-r-- 1 ishai ishai 1.7M Aug 14 02:50 20180814-001303.csv
-rw-rw-r-- 1 ishai ishai 1.7M Aug 15 03:42 20180815-004411.csv
-rw-rw-r-- 1 ishai ishai 1.6M Aug 15 12:38 20180815-113032.csv
-rw-rw-r-- 1 ishai ishai 457K Aug 15 13:46 20180815-131857.csv
-rw-rw-r-- 1 ishai ishai  14K Aug 15 13:48 20180815-134729.csv
-rw-rw-r-- 1 ishai ishai 1.1M Aug 15 15:15 20180815-134950.csv
-rw-rw-r-- 1 ishai ishai  37M Aug 18 01:37 20180815-160006.csv
:END:

#+BEGIN_SRC jupyter-julia :results output
  fname = maximum(readdir(datadir))
  run(`head $(joinpath(datadir, fname))`)
#+END_SRC

#+RESULTS:
:RESULTS:
2018-08-15T20:00:07.341000Z,289.01000000,0.95307512,ETH-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.05064896,BTC-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.06453496,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.52758604,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.75478104,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00118000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.56368644,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.03650000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00810000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00380000,BTC-USD
:END:

# DateTime does not recognize "2018-08-14T04:13:06.697000Z" so need to adjust string before processing. 
# Haven't figured out how to correctly use [[https://juliacomputing.com/TextParse.jl/stable/#TextParse.CustomParser][CustomParser]], but it looks like a reasonable approach to converting.
#+BEGIN_SRC jupyter-julia :eval never :exports never
  using TextParse
  cDateTime = CustomParser(x->x[1:end-5], DateTime)
#+END_SRC

# annoying thigns so far - 1.how to set types, 2. not much documentation 
# Later - set types using TextParse
#+BEGIN_SRC jupyter-julia
  using Dates
  colnames = ["date", "price", "size", "product_id"]
  t = Queryverse.load(joinpath(datadir, fname),
                      colnames=colnames,
                      colparsers=Dict(:date=>String, :price=>Float64, :size=>Float64, :product_id=>String)) |>
                          @map({:date=>DateTime(_.date[1:end-5]), _.price, _.size, _.product_id}) |> # cast to DateTime
                          table
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[6]:
#+BEGIN_EXAMPLE
  Table with 623229 rows, 4 columns:
  date                    price    size        product_id
  ───────────────────────────────────────────────────────
  2018-08-15T20:00:07.87  6383.82  0.050649    "BTC-USD"
  2018-08-15T20:00:07.87  6383.82  0.064535    "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  1.52759     "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  1.75478     "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.00118     "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.563686    "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0365      "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0081      "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0038      "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.2         "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.00998106  "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0633251   "BTC-USD"
  ⋮
  2018-08-18T05:37:32.6   310.51   1.3413      "ETH-USD"
  2018-08-18T05:37:32.6   310.5    8.46415     "ETH-USD"
  2018-08-18T05:37:32.6   310.5    1.9         "ETH-USD"
  2018-08-18T05:37:32.93  310.51   0.0432801   "ETH-USD"
  2018-08-18T05:37:33.72  5709.16  0.00409268  "BTC-EUR"
  2018-08-18T05:37:34.41  14.22    137.769     "ETC-USD"
  2018-08-18T05:37:34.72  6517.65  0.0376063   "BTC-USD"
  2018-08-18T05:37:35.14  14.19    145.838     "ETC-USD"
  2018-08-18T05:37:36.13  5709.16  0.022136    "BTC-EUR"
  2018-08-18T05:37:36.62  14.22    3.35943     "ETC-USD"
  2018-08-18T05:37:37.01  59.88    0.7215      "LTC-USD"
#+END_EXAMPLE
:END:

Assign primary keys
#+BEGIN_SRC jupyter-julia
   t = table(t, pkey=(:product_id, :date))
#+END_SRC

#+RESULTS:
:RESULTS:
1 - 3da0d492-de89-43af-b013-ccb7f0ad59da
:END:

Lets check if the data is sorted with in groups.
#+BEGIN_SRC jupyter-julia
  a = t |> @groupby(_.product_id, _.date) |> @map({origin=key(_), result=issorted(_)}) |> @map(all(_.result))
  all(a)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[11]:
: true
:END:

This opens a new window
#+BEGIN_SRC jupyter-julia :eval never
  t |> Voyager()
#+END_SRC

[[file:./voyager.png]]

The default view doesn't work for volume (size), lets try line instead
#+BEGIN_SRC jupyter-julia
  @time t |> @vlplot(:line, x=:date, y=:size, color="product_id")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[16]:
[[file:./obipy-resources/TMY56X.png]]
:END:

Not that nice. 

# log Time difference counts
#+BEGIN_SRC jupyter-julia :exports none :eval never
  tdiff = t |> @filter(_.product_id=="ETH-USD") |> @map(_.date) |> collect |> diff |> @map({delta=_});
  tdiff |> @map({delta=log(Dates.value(_.delta))}) |> @vlplot(mark={:bar, clip=true}, x={:delta, bin={maxbins=1000}, scale={domain=[0,15], clip=true}}, y={"count()", axis={title="Time-difference counts"}})
#+END_SRC

Plot time differences between observations
#+BEGIN_SRC jupyter-julia
  t |>
      @groupby(_.product_id) |>
      @map({product_id=key(_), delta=diff(_.date)}) |>
      @mapmany(_.delta, {delta=Dates.value(__), product_id=_.product_id}) |> # __ is the individual element in row
      @filter(_.delta>0) |>
      @vlplot(mark={:line, clip=true},
              x={:delta, axis={title="ΔTime"}, bin={maxbins=100000}, scale={domain=[0,10000], clip=true}},
              y={"count()", axis={title="Counts"}  #, scale={domain=[0,100], clip=true}
                 },
              color=:product_id,
              height=300,
              width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[136]:
[[file:./obipy-resources/bvknN0.png]]
:END:

/log/ time differences (between observations)
#+BEGIN_SRC jupyter-julia
  t |>
      @groupby(_.product_id) |>
      @map({product_id=key(_), delta=diff(_.date)}) |>
      @mapmany(_.delta, {delta=__, product_id=_.product_id}) |> # __ is the individual element in row
      @map({_.product_id, delta=log(Dates.value(_.delta))}) |>
      @vlplot(mark={:line, clip=true},
              x={:delta, axis={title="log(ΔTime)"}, bin={maxbins=100}},
              y={"count()", axis={title="Counts"}, scale={domain=[0,2500], clip=true}},
              color=:product_id, height=300, width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[34]:
[[file:./obipy-resources/IT9gre.png]]
:END:

Irregular signal. Resample to have observations at regular intervals.

Review duplicates:

#+BEGIN_SRC jupyter-julia
  t |> @groupby((_.product_id, _.date)) |> @map({origin=key(_), Count=length(_)}) |> @filter(_.Count > 1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[41]:
#+BEGIN_EXAMPLE
  ?x2 query result
  origin                              │ Count
  ────────────────────────────────────┼──────
  ("BTC-USD", 2018-08-15T20:00:07.87) │ 2
  ("BTC-USD", 2018-08-15T20:00:08.44) │ 12
  ("BTC-USD", 2018-08-15T20:00:08.46) │ 2
  ("BTC-USD", 2018-08-15T20:00:11.05) │ 6
  ("ETH-USD", 2018-08-15T20:00:11.2)  │ 2
  ("BTC-USD", 2018-08-15T20:00:12.16) │ 4
  ("BTC-USD", 2018-08-15T20:00:12.93) │ 11
  ("BTC-USD", 2018-08-15T20:00:13.02) │ 2
  ("BTC-USD", 2018-08-15T20:00:14.43) │ 4
  ("ETH-EUR", 2018-08-15T20:00:17.29) │ 5
  ... with more rows
#+END_EXAMPLE
:END:

These are due to transactions that are cleared simultaneously.
#+BEGIN_SRC jupyter-julia
using Statistics
t = t |> @groupby((_.product_id, _.date)) |> @map({product_id=key(_)[1], date=key(_)[2], price=median(_.price),  size=sum(_.size)}) |> table;
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[8]:
:END:

#+BEGIN_SRC jupyter-julia :eval never :exports none
  using Query, Statistics
  t = @from i in t begin
      @group i by i.product_id, i.date into g
      @select {product_id=key(g)[1], date=key(g)[2], price=median(g.price), size=sum(g.size)}
      @collect table
  end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[79]:
#+BEGIN_EXAMPLE
  Table with 351419 rows, 4 columns:
  product_id  date                    price    size
  ───────────────────────────────────────────────────────
  "BTC-USD"   2018-08-15T20:00:07.87  6383.82  0.115184
  "BTC-USD"   2018-08-15T20:00:08.44  6383.81  4.4218
  "BTC-USD"   2018-08-15T20:00:08.46  6383.58  0.001
  "BTC-USD"   2018-08-15T20:00:08.48  6383.35  0.00049985
  "BTC-USD"   2018-08-15T20:00:08.56  6382.54  0.00810663
  "BTC-USD"   2018-08-15T20:00:08.61  6382.53  0.00105
  "ETC-USD"   2018-08-15T20:00:09.78  12.11    72.8834
  "ETH-USD"   2018-08-15T20:00:10.48  289.01   2.1008
  "BTC-USD"   2018-08-15T20:00:11.05  6374.65  0.197664
  "ETH-USD"   2018-08-15T20:00:11.2   289.0    0.51067
  "BTC-USD"   2018-08-15T20:00:12.16  6373.0   2.0
  "BTC-USD"   2018-08-15T20:00:12.93  6374.29  1.68051
  ⋮
  "ETH-USD"   2018-08-18T05:37:24.32  310.85   4.9996
  "ETC-USD"   2018-08-18T05:37:29.94  14.195   74.6061
  "ETH-USD"   2018-08-18T05:37:32.6   310.52   11.9
  "ETH-USD"   2018-08-18T05:37:32.93  310.51   0.0432801
  "BTC-EUR"   2018-08-18T05:37:33.72  5709.16  0.00409268
  "ETC-USD"   2018-08-18T05:37:34.41  14.22    137.769
  "BTC-USD"   2018-08-18T05:37:34.72  6517.65  0.0376063
  "ETC-USD"   2018-08-18T05:37:35.14  14.19    145.838
  "BTC-EUR"   2018-08-18T05:37:36.13  5709.16  0.022136
  "ETC-USD"   2018-08-18T05:37:36.62  14.22    3.35943
  "LTC-USD"   2018-08-18T05:37:37.01  59.88    0.7215
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  t |> @groupby((_.product_id, _.date)) |> @map({origin=key(_), Count=length(_)}) |> @filter(_.Count > 1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[10]:
#+BEGIN_EXAMPLE
  0x2 query result
  origin │ Count
  ───────┼──────
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  t |> @vlplot(mark={:line, clip=true},
               x={:date},
               y={:price},
               color=:product_id, height=300, width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[94]:
[[file:./obipy-resources/0XjhGy.png]]
:END:

Median time difference:
#+BEGIN_SRC jupyter-julia
  meddiff = t |> @groupby(_.product_id, _.date) |> @map({key(_), median(diff(Dates.values(Float64.(_))))}) |> DataFrame
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[151]:
#+BEGIN_EXAMPLE
  15×2 DataFrames.DataFrame
  │ Row │ key     │ _2_     │
  ├─────┼─────────┼─────────┤
  │ 1   │ BTC-USD │ 1190.0  │
  │ 2   │ ETC-USD │ 890.0   │
  │ 3   │ ETH-USD │ 1780.0  │
  │ 4   │ ETH-EUR │ 8850.0  │
  │ 5   │ BCH-USD │ 6360.0  │
  │ 6   │ LTC-BTC │ 24825.0 │
  │ 7   │ BTC-EUR │ 5910.0  │
  │ 8   │ LTC-USD │ 3090.0  │
  │ 9   │ BTC-GBP │ 15390.0 │
  │ 10  │ LTC-EUR │ 18665.0 │
  │ 11  │ ETH-BTC │ 3595.0  │
  │ 12  │ ETC-EUR │ 29360.0 │
  │ 13  │ BCH-EUR │ 38610.0 │
  │ 14  │ ETC-BTC │ 3990.0  │
  │ 15  │ BCH-BTC │ 16330.0 │
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia 
  t = setcol(t, :datevalue, Queryverse.map(r->Dates.value(r.date), t))
  mindate = minimum(JuliaDB.select(t, :datevalue))
  t = setcol(t, :datevalue, Queryverse.map(r->r.datevalue-mindate, t))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[9]:
#+BEGIN_EXAMPLE
  Table with 351419 rows, 5 columns:
  product_id  date                    price    size        datevalue
  ──────────────────────────────────────────────────────────────────
  "BTC-USD"   2018-08-15T20:00:07.87  6383.82  0.115184    0
  "BTC-USD"   2018-08-15T20:00:08.44  6383.81  4.4218      570
  "BTC-USD"   2018-08-15T20:00:08.46  6383.58  0.001       590
  "BTC-USD"   2018-08-15T20:00:08.48  6383.35  0.00049985  610
  "BTC-USD"   2018-08-15T20:00:08.56  6382.54  0.00810663  690
  "BTC-USD"   2018-08-15T20:00:08.61  6382.53  0.00105     740
  "ETC-USD"   2018-08-15T20:00:09.78  12.11    72.8834     1910
  "ETH-USD"   2018-08-15T20:00:10.48  289.01   2.1008      2610
  "BTC-USD"   2018-08-15T20:00:11.05  6374.65  0.197664    3180
  "ETH-USD"   2018-08-15T20:00:11.2   289.0    0.51067     3330
  "BTC-USD"   2018-08-15T20:00:12.16  6373.0   2.0         4290
  "BTC-USD"   2018-08-15T20:00:12.93  6374.29  1.68051     5060
  ⋮
  "ETH-USD"   2018-08-18T05:37:24.32  310.85   4.9996      207436450
  "ETC-USD"   2018-08-18T05:37:29.94  14.195   74.6061     207442070
  "ETH-USD"   2018-08-18T05:37:32.6   310.52   11.9        207444730
  "ETH-USD"   2018-08-18T05:37:32.93  310.51   0.0432801   207445060
  "BTC-EUR"   2018-08-18T05:37:33.72  5709.16  0.00409268  207445850
  "ETC-USD"   2018-08-18T05:37:34.41  14.22    137.769     207446540
  "BTC-USD"   2018-08-18T05:37:34.72  6517.65  0.0376063   207446850
  "ETC-USD"   2018-08-18T05:37:35.14  14.19    145.838     207447270
  "BTC-EUR"   2018-08-18T05:37:36.13  5709.16  0.022136    207448260
  "ETC-USD"   2018-08-18T05:37:36.62  14.22    3.35943     207448750
  "LTC-USD"   2018-08-18T05:37:37.01  59.88    0.7215      207449140
#+END_EXAMPLE
:END:

Lowest median time difference between samples is for BCH-EUR at around 38 seconds between observations. Still, try using .5 second sampling frequency to take advantage of higher frequency signals.

Start and end of interval
#+BEGIN_SRC jupyter-julia  
  tstart = minimum(JuliaDB.select(t, :datevalue))
  tend = maximum(JuliaDB.select(t, :datevalue))
  sample_points = tstart:500:tend # iterating in milliseconds
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[10]:
: 0:500:207449000
:END:


#+BEGIN_SRC jupyter-julia :eval never
  using Dierckx, NamedTuples
  date_interp(x, y) = Spline1D(x,y; k=2, bc="extrapolate", s=1e-4)(sample_points)
  date_interp_t(_t) = table(@NT(datevalue=sample_points, price=date_interp(JuliaDB.select(_t, :datevalue), JuliaDB.select(_t, :price))))
  # date_interp_t(_t) = date_interp(_t[:datevalue], _t[:price])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[262]:
: date_interp_t (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  using Interpolations, NamedTuples
  date_interp(x, y) = LinearInterpolation(x,y, extrapolation_bc=Linear())(collect(sample_points))
  date_interp_t(_t) = table(@NT(datevalue=sample_points .- minimum(sample_points), price=date_interp(IndexedTables.select(table(_t), :datevalue), IndexedTables.select(table(_t), :price))))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[264]:
: date_interp_t (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  using StatPlots
  temp = t |> @filter(_.product_id =="BCH-BTC") |> table |> date_interp_t;
  temp2 = t |> @filter(_.product_id =="BCH-BTC") |> table;
  @df temp plot(:datevalue, :price, size=(2000,1000))
  @df temp2 scatter!(:datevalue, :price, markersize=1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[265]:
[[file:./obipy-resources/GHjZ7d.svg]]
:END:

Apply function across groups and unstack.
#+BEGIN_SRC jupyter-julia
  tnew = JuliaDB.groupby(date_interp_t, t, :product_id, flatten=true)
  tnew = IndexedTables.unstack(table(tnew), :datevalue; variable=:product_id, value=:price)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[267]:
#+BEGIN_EXAMPLE
  Table with 414899 rows, 16 columns:
  Columns:
  [1m#   [22m[1mcolname    [22m[1mtype[22m
  ─────────────────────────────────
  1   datevalue  Int64
  2   BCH-BTC    DataValue{Float64}
  3   BCH-EUR    DataValue{Float64}
  4   BCH-USD    DataValue{Float64}
  5   BTC-EUR    DataValue{Float64}
  6   BTC-GBP    DataValue{Float64}
  7   BTC-USD    DataValue{Float64}
  8   ETC-BTC    DataValue{Float64}
  9   ETC-EUR    DataValue{Float64}
  10  ETC-USD    DataValue{Float64}
  11  ETH-BTC    DataValue{Float64}
  12  ETH-EUR    DataValue{Float64}
  13  ETH-USD    DataValue{Float64}
  14  LTC-BTC    DataValue{Float64}
  15  LTC-EUR    DataValue{Float64}
  16  LTC-USD    DataValue{Float64}
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia :async t
  tnew[end-100:end] |> DataFrame |> tail
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[268]:
#+BEGIN_EXAMPLE
  6×16 DataFrame. Omitted printing of 9 columns
  │ Row │ datevalue │ BCH-BTC │ BCH-EUR │ BCH-USD │ BTC-EUR │ BTC-GBP │ BTC-USD │
  ├─────┼───────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤
  │ 1   │ 207446500 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5251.41 │ 6517.65 │
  │ 2   │ 207447000 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5252.02 │ 6517.65 │
  │ 3   │ 207447500 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5252.63 │ 6517.65 │
  │ 4   │ 207448000 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5253.23 │ 6517.65 │
  │ 5   │ 207448500 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5253.84 │ 6517.65 │
  │ 6   │ 207449000 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5254.44 │ 6517.65 │
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  showcols(DataFrame(tnew))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[22]:
#+BEGIN_EXAMPLE
  16×5 DataFrame
  │ Row │ variable  │ eltype  │ nmissing │ first     │ last       │
  ├─────┼───────────┼─────────┼──────────┼───────────┼────────────┤
  │ 1   │ datevalue │ Float64 │          │ 6.367e13  │ 6.36703e13 │
  │ 2   │ BCH-BTC   │ Float64 │ 0        │ 0.0808327 │ 0.09069    │
  │ 3   │ BCH-EUR   │ Float64 │ 0        │ 458.773   │ 520.15     │
  │ 4   │ BCH-USD   │ Float64 │ 0        │ 519.021   │ 589.46     │
  │ 5   │ BTC-EUR   │ Float64 │ 0        │ 5622.95   │ 5709.16    │
  │ 6   │ BTC-GBP   │ Float64 │ 0        │ 5050.2    │ 5254.44    │
  │ 7   │ BTC-USD   │ Float64 │ 0        │ 6383.82   │ 6517.65    │
  │ 8   │ ETC-BTC   │ Float64 │ 0        │ 0.00191   │ 0.00217    │
  │ 9   │ ETC-EUR   │ Float64 │ 0        │ 10.757    │ 12.4522    │
  │ 10  │ ETC-USD   │ Float64 │ 0        │ 12.11     │ 14.2251    │
  │ 11  │ ETH-BTC   │ Float64 │ 0        │ 0.0451994 │ 0.04781    │
  │ 12  │ ETH-EUR   │ Float64 │ 0        │ 254.655   │ 273.216    │
  │ 13  │ ETH-USD   │ Float64 │ 0        │ 289.046   │ 310.391    │
  │ 14  │ LTC-BTC   │ Float64 │ 0        │ 0.0087506 │ 0.00915569 │
  │ 15  │ LTC-EUR   │ Float64 │ 0        │ 49.328    │ 52.4154    │
  │ 16  │ LTC-USD   │ Float64 │ 0        │ 56.0264   │ 59.8801    │
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  sum(isnan.(IndexedTables.select(tnew, :datevalue)))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[269]:
: 0
:END:

Replace missing + mandatory column wise vs row wise comparison:

#+BEGIN_SRC jupyter-julia :async t :results output
  using Query
  dfnew = tnew |> DataFrame
  @time for i in size(dfnew)[1]-1:-1:1
      for c in 1:size(dfnew)[2]
          if isnan(dfnew[i,c])
              dfnew[i,c] = dfnew[i+1,c]
          end
      end
  end  
#+END_SRC

#+RESULTS:
:RESULTS:
  2.476500 seconds (16.17 M allocations: 367.061 MiB, 52.51% gc time)
:END:

#+BEGIN_SRC jupyter-julia :async t :results output
  using Query
  dfnew = tnew |> DataFrame
  @time for c in 1:size(dfnew)[2]
      for i in size(dfnew)[1]-1:-1:1
          if isnan(dfnew[i,c])
              dfnew[i,c] = dfnew[i+1,c]
          end
      end
  end  
#+END_SRC

#+RESULTS:
:RESULTS:
  1.876617 seconds (26.94 M allocations: 512.453 MiB, 23.32% gc time)
:END:

No more nulls
#+BEGIN_SRC jupyter-julia
  tnew |> @map(isnan.(values(_))) |> table |> columns |> @map(sum(_)) |> maximum
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[270]:
: 0
:END:

Save
#+BEGIN_SRC jupyter-julia
  # tnew |> save("postproc2.feather")
  tnew |> save("postproc2.csv")
#+END_SRC


Annoyingly, there is some non-negligible loss in precision after saving the data
#+BEGIN_SRC jupyter-julia
  df = load("postproc2.csv") |> DataFrame
  println(sum((convert(Array{Float64}, DataFrame(tnew)) .- convert(Array{Float64}, df)).^2, 1))
#+END_SRC

#+RESULTS:
:RESULTS:
[3.04944 2.56476e-30 3.36042e-25 1.9387e-25 4.96308e-24 8.27181e-24 1.65436e-24 2.37737e-33 2.54644e-27 2.22143e-27 8.44039e-31 2.488e-25 2.55263e-25 6.31163e-32 4.29645e-26 3.1302e-26]
┌ Warning: `sum(a::AbstractArray, dims)` is deprecated, use `sum(a, dims=dims)` instead.
│   caller = top-level scope at In[108]:1
└ @ Core In[108]:1
:END:

/3.04944/ - sum of squares for datevalue is non-negligible. Maybe because saved as csv or because originally used float64 for dateavalue.

#+BEGIN_SRC jupyter-julia
  maximum(diff(df[:datevalue])), minimum(diff(df[:datevalue]))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[149]:
: (500.015625, 499.984375)
:END:

Probably should have converted it to Int64

#+BEGIN_SRC jupyter-julia
maximum(diff(convert(Array{Float64}, df), 1)[:,2:end], 1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[157]:
#+BEGIN_EXAMPLE
  1×15 Array{Float64,2}:
  0.000302757  2.05052  1.69563  23.0194  …  1.11495  2.0e-5  0.208  0.164176
#+END_EXAMPLE
:END:

Can probably use a better smoothing approach here.


Using Feather api directly seems to work
#+BEGIN_SRC jupyter-julia
  using Feather
  Feather.write("postproc3.feather", DataFrame(tnew))
#+END_SRC

#+BEGIN_SRC jupyter-julia
  load("postproc3.feather") |> @map(isnan.(values(_))) |> table |> columns |> @map(sum(_)) |> maximum
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[285]:
: 0
:END:

#+BEGIN_SRC jupyter-julia :results output
  df = load("postproc3.feather") |> DataFrame
  println(sum((convert(Array{Float64}, DataFrame(tnew)) .- convert(Array{Float64}, df)).^2, 1))
#+END_SRC

#+RESULTS:
:RESULTS:
[0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0]
┌ Warning: `sum(a::AbstractArray, dims)` is deprecated, use `sum(a, dims=dims)` instead.
│   caller = top-level scope at In[289]:2
└ @ Core In[289]:2
:END:

Thats better...



**** Julia =v0.6.4=

#+BEGIN_SRC jupyter-julia
  using Queryverse, IndexedTables
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[20]:
:END:

#+BEGIN_SRC jupyter-julia :results output
  datadir = "/home/ishai/Documents/work/marketbot/src/utils/data/raw"
  run(`ls -lh $(datadir)`)
#+END_SRC

#+BEGIN_SRC jupyter-julia :results output
  fname = maximum(readdir(datadir))
  run(`head $(joinpath(datadir, fname))`)
#+END_SRC

#+RESULTS:
:RESULTS:
2018-08-15T20:00:07.341000Z,289.01000000,0.95307512,ETH-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.05064896,BTC-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.06453496,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.52758604,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.75478104,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00118000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.56368644,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.03650000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00810000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00380000,BTC-USD
:END:

# DateTime does not recognize "2018-08-14T04:13:06.697000Z" so need to adjust string before processing. 
# Haven't figured out how to correctly use [[https://juliacomputing.com/TextParse.jl/stable/#TextParse.CustomParser][CustomParser]], but it looks like a reasonable approach to converting.
#+BEGIN_SRC jupyter-julia :eval never :exports never
using TextParse
cDateTime = CustomParser(x->x[1:end-5], DateTime)
#+END_SRC

# annoying thigns so far - 1.how to set types, 2. not much documentation 
# Later - set types using TextParse
#+BEGIN_SRC jupyter-julia
  using Dates
  colnames = ["date", "price", "size", "product_id"]
  t = load(joinpath(datadir, fname),
           colnames=colnames,
           colparsers=Dict(:date=>String, :price=>Float64, :size=>Float64, :product_id=>String)) |>
               @map({:date=>DateTime(_.date[1:end-5]), _.price, _.size, _.product_id}) |> # cast to DateTime
               table
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 0549e442-50a5-410e-a486-a56e7c159722
:END:

Assign primary keys
#+BEGIN_SRC jupyter-julia
  t = table(t, pkey=(:product_id, :date))
#+END_SRC

Lets check if the data is sorted with in groups.
#+BEGIN_SRC jupyter-julia
  a = t |> @groupby(_.product_id, _.date) |> @map({origin=_.key, result=issorted(_)}) |> @map(all(_.result))
  all(a)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[38]:
: true
:END:

This opens a new window
#+BEGIN_SRC jupyter-julia :eval never
  t |> Voyager()
#+END_SRC

[[file:./voyager.png]]

The default view doesn't work for volume (size), lets try line instead
#+BEGIN_SRC jupyter-julia
  @time t |> @vlplot(:line, x=:date, y=:size, color="product_id")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[16]:
[[file:./obipy-resources/TMY56X.png]]
:END:

Not that nice. 

# log Time difference counts
#+BEGIN_SRC jupyter-julia :exports none :eval never exports none
  tdiff = t |> @filter(_.product_id=="ETH-USD") |> @Queryverse.map(_.date) |> collect |> diff |> @map({delta=_});
  tdiff |> @map({delta=log(Dates.value(_.delta))}) |> @vlplot(mark={:bar, clip=true}, x={:delta, bin={maxbins=1000}, scale={domain=[0,15], clip=true}}, y={"count()", axis={title="Time-difference counts"}})
#+END_SRC

#+RESULTS:
:RESULTS:
0 - eeea6df9-5b4d-4c64-abdf-c588a79e6276
:END:

Plot time differences between observations
#+BEGIN_SRC jupyter-julia
  t |>
      @groupby(_.product_id) |>
      @map({product_id=_.key, delta=diff(_..date)}) |>
      @mapmany(_.delta, {delta=Dates.value(__), product_id=_.product_id}) |> # __ is the individual element in row
      @filter(_.delta>0) |>
      @vlplot(mark={:line, clip=true},
              x={:delta, axis={title="ΔTime"}, bin={maxbins=100000}, scale={domain=[0,10000], clip=true}},
              y={"count()", axis={title="Counts"}  #, scale={domain=[0,100], clip=true}
                 },
              color=:product_id,
              height=300,
              width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[136]:
[[file:./obipy-resources/bvknN0.png]]
:END:

/log/ time differences (between observations)
#+BEGIN_SRC jupyter-julia
  t |>
      @groupby(_.product_id) |>
      @map({product_id=_.key, delta=diff(_..date)}) |>
      @mapmany(_.delta, {delta=__, product_id=_.product_id}) |> # __ is the individual element in row
      @map({_.product_id, delta=log(Dates.value(_.delta))}) |>
      @vlplot(mark={:line, clip=true},
              x={:delta, axis={title="log(ΔTime)"}, bin={maxbins=100}},
              y={"count()", axis={title="Counts"}, scale={domain=[0,2500], clip=true}},
              color=:product_id, height=300, width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[34]:
[[file:./obipy-resources/IT9gre.png]]
:END:

Irregular signal. Resample to have observations at regular intervals.

Review duplicates:

#+BEGIN_SRC jupyter-julia
  t |> @groupby((_.product_id, _.date)) |> @map({origin=_.key, Count=length(_)}) |> @filter(_.Count > 1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[41]:
#+BEGIN_EXAMPLE
  ?x2 query result
  origin                              │ Count
  ────────────────────────────────────┼──────
  ("BTC-USD", 2018-08-15T20:00:07.87) │ 2
  ("BTC-USD", 2018-08-15T20:00:08.44) │ 12
  ("BTC-USD", 2018-08-15T20:00:08.46) │ 2
  ("BTC-USD", 2018-08-15T20:00:11.05) │ 6
  ("ETH-USD", 2018-08-15T20:00:11.2)  │ 2
  ("BTC-USD", 2018-08-15T20:00:12.16) │ 4
  ("BTC-USD", 2018-08-15T20:00:12.93) │ 11
  ("BTC-USD", 2018-08-15T20:00:13.02) │ 2
  ("BTC-USD", 2018-08-15T20:00:14.43) │ 4
  ("ETH-EUR", 2018-08-15T20:00:17.29) │ 5
  ... with more rows
#+END_EXAMPLE
:END:

These are due to transactions that are cleared simultaneously.

#+BEGIN_SRC jupyter-julia
t = t |> @groupby((_.product_id, _.date)) |> @map({product_id=_.key[1], date=_.key[2], price=median(_..price),  size=sum(_..size)}) |> table;
#+END_SRC

#+BEGIN_SRC jupyter-julia
t |> @groupby((_.product_id, _.date)) |> @map({origin=_.key, Count=length(_)}) |> @filter(_.Count > 1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[10]:
#+BEGIN_EXAMPLE
  0x2 query result
  origin │ Count
  ───────┼──────
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  t |> @vlplot(mark={:line, clip=true},
               x={:date},
               y={:price},
               color=:product_id, height=300, width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[94]:
[[file:./obipy-resources/0XjhGy.png]]
:END:

Median time difference:
#+BEGIN_SRC jupyter-julia :async t
  meddiff = t |> @groupby(_.product_id, _.date) |> @map({_.key, median(diff(Dates.values(Float64.(_))))}) |> DataFrame
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[151]:
#+BEGIN_EXAMPLE
  15×2 DataFrames.DataFrame
  │ Row │ key     │ _2_     │
  ├─────┼─────────┼─────────┤
  │ 1   │ BTC-USD │ 1190.0  │
  │ 2   │ ETC-USD │ 890.0   │
  │ 3   │ ETH-USD │ 1780.0  │
  │ 4   │ ETH-EUR │ 8850.0  │
  │ 5   │ BCH-USD │ 6360.0  │
  │ 6   │ LTC-BTC │ 24825.0 │
  │ 7   │ BTC-EUR │ 5910.0  │
  │ 8   │ LTC-USD │ 3090.0  │
  │ 9   │ BTC-GBP │ 15390.0 │
  │ 10  │ LTC-EUR │ 18665.0 │
  │ 11  │ ETH-BTC │ 3595.0  │
  │ 12  │ ETC-EUR │ 29360.0 │
  │ 13  │ BCH-EUR │ 38610.0 │
  │ 14  │ ETC-BTC │ 3990.0  │
  │ 15  │ BCH-BTC │ 16330.0 │
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  t = setcol(t, :datevalue, Queryverse.map(r->Float64(Dates.value(r.date)), t))
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 4b0016a8-b154-49c7-89b4-5e7fb307af59
:END:

#+BEGIN_SRC jupyter-julia
  t |> @groupby(_.product_id)
#+END_SRC

Lowest median time difference between samples is for BCH-EUR at around 38 seconds between observations. Still, try using .5 second sampling frequency to take advantage of higher frequency signals.

#+BEGIN_SRC jupyter-julia :eval never :export none
  df = t |> DataFrame;
  df[:datevalue] = Float64.(Dates.values(df[:date]));
#+END_SRC

#+BEGIN_SRC jupyter-julia :eval never :export none
  tempdf = filter(r->r[:product_id] == "BTC-USD", df)
#+END_SRC

Start and end of interval
#+BEGIN_SRC jupyter-julia
  tstart = minimum(select(t, :datevalue))
  tend = maximum(select(t, :datevalue))
  sample_points = tstart:500:tend
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[18]:
: 6.367004640787e13:500.0:6.367025385687e13
:END:

#+BEGIN_SRC jupyter-julia :eval never :export none
  using Dierckx
  date_interp(x, y) = Spline1D(x,y; k=2, bc="nearest", s=2356)(sample_points)
  date_interp_t(_t) = table(@NT(datevalue=sample_points, price=date_interp(select(_t, :datevalue), select(_t, :price))))
  # date_interp_t(_t) = date_interp(_t[:datevalue], _t[:price])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[77]:
: date_interp_t (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  using Interpolations
  date_interp(x, y) = LinearInterpolation(x,y, extrapolation_bc=Interpolations.Linear())(sample_points)
  date_interp_t(_t) = table(@NT(datevalue=sample_points, price=date_interp(select(table(_t), :datevalue), select(table(_t), :price))))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[19]:
: date_interp_t (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  using StatPlots
  temp = t |> @filter(_.product_id =="BCH-BTC") |> table |> date_interp_t;
  temp2 = t |> @filter(_.product_id =="BCH-BTC") |> table;
  @df temp plot(:datevalue, :price)
  @df temp2 scatter!(:datevalue, :price, markersize=1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[90]:
[[file:./obipy-resources/a4NkoU.svg]]
:END:

# There is definitely a better way to do this...
#+BEGIN_SRC jupyter-julia :eval never :export none
  dfs = []
  groups = DataFrames.groupby(df, :product_id)
  for _df in groups
      _df2 = DataFrame(price=date_interp(_df[:datevalue], _df[:price]),
                       size=date_interp(_df[:datevalue], _df[:size]),
                       datevalue=sample_points)
                       _df2[:product_id] = _df[:product_id][1]
      push!(dfs, _df2)
  end
#+END_SRC

Apply function across groups and unstack.
#+BEGIN_SRC jupyter-julia
  tnew = IndexedTables.groupby(date_interp_t, t, :product_id, flatten=true)
  tnew = IndexedTables.unstack(table(tnew), :datevalue; variable=:product_id, value=:price)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[21]:
#+BEGIN_EXAMPLE
  Table with 414899 rows, 16 columns:
  Columns:
  [1m#   [22m[1mcolname    [22m[1mtype[22m
  ────────────────────────────────────────────
  1   datevalue  Float64
  2   BCH-BTC    DataValues.DataValue{Float64}
  3   BCH-EUR    DataValues.DataValue{Float64}
  4   BCH-USD    DataValues.DataValue{Float64}
  5   BTC-EUR    DataValues.DataValue{Float64}
  6   BTC-GBP    DataValues.DataValue{Float64}
  7   BTC-USD    DataValues.DataValue{Float64}
  8   ETC-BTC    DataValues.DataValue{Float64}
  9   ETC-EUR    DataValues.DataValue{Float64}
  10  ETC-USD    DataValues.DataValue{Float64}
  11  ETH-BTC    DataValues.DataValue{Float64}
  12  ETH-EUR    DataValues.DataValue{Float64}
  13  ETH-USD    DataValues.DataValue{Float64}
  14  LTC-BTC    DataValues.DataValue{Float64}
  15  LTC-EUR    DataValues.DataValue{Float64}
  16  LTC-USD    DataValues.DataValue{Float64}
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia :async t
  tnew[end-100:end] |> DataFrame |> tail
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[159]:
#+BEGIN_EXAMPLE
  6×16 DataFrames.DataFrame. Omitted printing of 9 columns
  │ Row │ datevalue  │ BCH-BTC │ BCH-EUR │ BCH-USD │ BTC-EUR │ BTC-GBP │ BTC-USD │
  ├─────┼────────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤
  │ 1   │ 6.36703e13 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5251.41 │ 6517.65 │
  │ 2   │ 6.36703e13 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5252.02 │ 6517.65 │
  │ 3   │ 6.36703e13 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5252.63 │ 6517.65 │
  │ 4   │ 6.36703e13 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5253.23 │ 6517.65 │
  │ 5   │ 6.36703e13 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5253.84 │ 6517.65 │
  │ 6   │ 6.36703e13 │ 0.09069 │ 520.15  │ 589.46  │ 5709.16 │ 5254.44 │ 6517.65 │
#+END_EXAMPLE
:END:

Messy way to get sum of nulls
#+BEGIN_SRC jupyter-julia
  tnew |> @map(isnan.(_)) |> table |> columns |> @map(sum(_)) |> table
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[364]:
#+BEGIN_EXAMPLE
  Table with 16 rows, 1 columns:
  _1_
  ───
  0
  0
  127
  0
  0
  0
  1
  0
  0
  0
  0
  0
  0
  23
  78
  38
#+END_EXAMPLE
:END:

Replace missing + mandatory column wise vs row wise comparison:

#+BEGIN_SRC jupyter-julia :async t :results output
  using Query
  dfnew = tnew |> DataFrame
  @time for i in size(dfnew)[1]-1:-1:1
      for c in 1:size(dfnew)[2]
          if isnan(dfnew[i,c])
              dfnew[i,c] = dfnew[i+1,c]
          end
      end
  end  
#+END_SRC

#+RESULTS:
:RESULTS:
  2.476500 seconds (16.17 M allocations: 367.061 MiB, 52.51% gc time)
:END:

#+BEGIN_SRC jupyter-julia :async t :results output
  using Query
  dfnew = tnew |> DataFrame
  @time for c in 1:size(dfnew)[2]
      for i in size(dfnew)[1]-1:-1:1
          if isnan(dfnew[i,c])
              dfnew[i,c] = dfnew[i+1,c]
          end
      end
  end  
#+END_SRC

#+RESULTS:
:RESULTS:
  1.876617 seconds (26.94 M allocations: 512.453 MiB, 23.32% gc time)
:END:

No more nulls
#+BEGIN_SRC jupyter-julia
  dfnew |> table |> @map(isnan.(_)) |> table |> columns |> @map(sum(_)) |> maximum
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[24]:
: 0
:END:

Save to feather
#+BEGIN_SRC jupyter-julia :eval never
  t2 = dfnew |> table |> @tee(save("postproc.feather"))
#+END_SRC


*** Batch generator - for julia models

This part is done in =v0.7.1= inorder to future proof use of generator.
#+BEGIN_SRC jupyter-julia
  using Logging
  LogLevel(Logging.Info)
  using DataFrames, Queryverse
  df = load("postproc3.feather") |> DataFrame
  delete!(df, :datevalue)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[5]:
#+BEGIN_EXAMPLE
  414899×15 DataFrame. Omitted printing of 9 columns
  │ Row    │ BCH-BTC   │ BCH-EUR │ BCH-USD │ BTC-EUR │ BTC-GBP │ BTC-USD │
  ├────────┼───────────┼─────────┼─────────┼─────────┼─────────┼─────────┤
  │ 1      │ 0.0808327 │ 458.773 │ 519.021 │ 5622.95 │ 5050.2  │ 6383.82 │
  │ 2      │ 0.0808352 │ 458.752 │ 519.019 │ 5622.95 │ 5050.2  │ 6383.81 │
  │ 3      │ 0.0808377 │ 458.732 │ 519.017 │ 5622.95 │ 5050.2  │ 6381.69 │
  │ 4      │ 0.0808402 │ 458.711 │ 519.016 │ 5622.95 │ 5050.2  │ 6380.07 │
  │ 5      │ 0.0808427 │ 458.69  │ 519.014 │ 5622.95 │ 5050.2  │ 6378.46 │
  │ 6      │ 0.0808453 │ 458.669 │ 519.012 │ 5622.95 │ 5050.2  │ 6376.84 │
  │ 7      │ 0.0808478 │ 458.649 │ 519.01  │ 5622.95 │ 5050.2  │ 6375.23 │
  │ 8      │ 0.0808503 │ 458.628 │ 519.009 │ 5622.95 │ 5050.2  │ 6374.17 │
  │ 9      │ 0.0808528 │ 458.607 │ 519.007 │ 5622.95 │ 5050.2  │ 6373.43 │
  │ 10     │ 0.0808553 │ 458.587 │ 519.005 │ 5622.95 │ 5050.2  │ 6373.35 │
  │ 11     │ 0.0808578 │ 458.566 │ 519.004 │ 5622.95 │ 5050.2  │ 6374.19 │
  ⋮
  │ 414888 │ 0.09069   │ 520.15  │ 589.46  │ 5709.25 │ 5247.78 │ 6517.65 │
  │ 414889 │ 0.09069   │ 520.15  │ 589.46  │ 5709.23 │ 5248.38 │ 6517.65 │
  │ 414890 │ 0.09069   │ 520.15  │ 589.46  │ 5709.21 │ 5248.99 │ 6517.65 │
  │ 414891 │ 0.09069   │ 520.15  │ 589.46  │ 5709.19 │ 5249.59 │ 6517.65 │
  │ 414892 │ 0.09069   │ 520.15  │ 589.46  │ 5709.17 │ 5250.2  │ 6517.65 │
  │ 414893 │ 0.09069   │ 520.15  │ 589.46  │ 5709.16 │ 5250.81 │ 6517.65 │
  │ 414894 │ 0.09069   │ 520.15  │ 589.46  │ 5709.16 │ 5251.41 │ 6517.65 │
  │ 414895 │ 0.09069   │ 520.15  │ 589.46  │ 5709.16 │ 5252.02 │ 6517.65 │
  │ 414896 │ 0.09069   │ 520.15  │ 589.46  │ 5709.16 │ 5252.63 │ 6517.65 │
  │ 414897 │ 0.09069   │ 520.15  │ 589.46  │ 5709.16 │ 5253.23 │ 6517.65 │
  │ 414898 │ 0.09069   │ 520.15  │ 589.46  │ 5709.16 │ 5253.84 │ 6517.65 │
  │ 414899 │ 0.09069   │ 520.15  │ 589.46  │ 5709.16 │ 5254.44 │ 6517.65 │
#+END_EXAMPLE
:END:


Batch generator
#+BEGIN_SRC jupyter-julia
  len_in = 50
  len_out = 5
  nbatch = 32
  nchannels = size(df)[2]
  end_index = size(df)[1] - (len_in + len_in)
  start_indices = 1:end_index
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[141]:
: 1:414799
:END:


#+BEGIN_SRC jupyter-julia
  using Random: shuffle
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[7]:
:END:


Create input/output batch tuple generator with dimension T_{in|out} of arrays of size (D x B).
- T_{in|out} :: input or output dimension
- D :: number of features
- B :: batch size
#+BEGIN_SRC jupyter-julia
  using Flux
  """
    get_generator
    Returns a batch generator where each batch is a pair of arrays with shape T(_in-1)|(_out) x B x D.
    Applies first difference over time axis on original observations.
  """
  function get_generator()
    ix_batches = Iterators.partition(shuffle(start_indices), nbatch)
    # single entry in an input / output batch with shape: width_in/out,batch size, channels
    range_in = 0:(len_seq_in-1)
    range_out = (len_seq_in):(len_seq_in+len_seq_out-1)
    function get_observations(ix)
      obs = convert(Array{Float32}, df[ix .+ ([range_in..., range_out...]),:])
      diff(obs, dims=1)
    end
    function get_batch(b)
      """
      Return time major batches
      """
      batch = Flux.stack(get_observations.(b), 2)
      batch[1 .+ range_in[1:end-1],:,:], batch[range_out,:,:]
    end
    Base.Generator(get_batch, ix_batches)
  end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[23]:
: get_generator
:END:


*** Very large differences

conversion from Float64 to Float32 does not behave 

#+BEGIN_SRC jupyter-julia
  Δobs = diff(convert(Array{Float32}, df), 1)
  bigdf = DataFrame(Δobs[(maximum(abs.(Δobs), dims=2) .> 10)[:],:], names(df))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[164]:
#+BEGIN_EXAMPLE
  112×16 DataFrame. Omitted printing of 11 columns
  │ Row │ datevalue │ BCH-BTC     │ BCH-EUR     │ BCH-USD      │ BTC-EUR      │
  ├─────┼───────────┼─────────────┼─────────────┼──────────────┼──────────────┤
  │ 1   │ 0.0       │ 2.5034e-6   │ -0.0207214  │ -0.085083    │ -10.9497     │
  │ 2   │ 0.0       │ 2.51085e-6  │ -0.0221863  │ 0.00311279   │ -0.000488281 │
  │ 3   │ 4.1943e6  │ -1.41561e-7 │ -3.05176e-5 │ 0.00805664   │ 0.179688     │
  │ 4   │ 0.0       │ 1.71363e-7  │ -0.00250244 │ -0.000305176 │ -0.0107422   │
  │ 5   │ 0.0       │ 1.49012e-7  │ -0.00213623 │ 0.000305176  │ -0.122559    │
  │ 6   │ 4.1943e6  │ 9.68575e-8  │ -0.0022583  │ 0.0136719    │ 0.000976563  │
  │ 7   │ 0.0       │ 9.68575e-8  │ 0.00518799  │ 0.0286865    │ 23.019       │
  │ 8   │ 4.1943e6  │ -9.90927e-7 │ -0.00180054 │ -0.00250244  │ -0.0107422   │
  │ 9   │ 0.0       │ 3.8594e-6   │ -0.0647583  │ 0.0          │ -0.167969    │
  │ 10  │ 4.1943e6  │ -2.68221e-7 │ -0.00518799 │ -0.00479126  │ 0.000488281  │
  │ 11  │ 0.0       │ 1.11759e-7  │ -0.00848389 │ -0.010376    │ 10.0908      │
  ⋮
  │ 101 │ 4.1943e6  │ 5.96046e-8  │ 0.00488281  │ 0.000183105  │ 0.0          │
  │ 102 │ 0.0       │ 1.49012e-7  │ 0.00253296  │ 0.0          │ 0.0463867    │
  │ 103 │ 0.0       │ 0.0         │ 0.0189209   │ 0.231689     │ 0.000488281  │
  │ 104 │ 0.0       │ 2.37674e-6  │ 0.0         │ -0.0319824   │ -0.000488281 │
  │ 105 │ 4.1943e6  │ -2.5332e-7  │ 0.0         │ 0.000183105  │ 0.0          │
  │ 106 │ 4.1943e6  │ 4.69387e-7  │ 0.00012207  │ -0.299866    │ 0.0          │
  │ 107 │ 0.0       │ 0.0         │ 0.00140381  │ 0.0          │ 0.00439453   │
  │ 108 │ 4.1943e6  │ 3.57628e-7  │ 0.0032959   │ 0.0358276    │ -0.271973    │
  │ 109 │ 0.0       │ -9.83477e-7 │ -0.00195313 │ -0.00732422  │ -0.0708008   │
  │ 110 │ 4.1943e6  │ 2.08616e-7  │ 0.00292969  │ 0.0247803    │ -0.000488281 │
  │ 111 │ 4.1943e6  │ 1.49012e-8  │ 0.000976563 │ 0.0022583    │ 0.0          │
  │ 112 │ 4.1943e6  │ -1.41561e-7 │ 0.000244141 │ 0.0          │ 0.0          │
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  using StatPlots
  # @df DataFrame(Δobs[(maximum(Δobs, dims=2) .> 10)[:],:], names(df)) plot(names(df))
  @df bigdf plot(:datevalue, names(df)[2:5])
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 62a8374a-cdc2-4add-88b2-6899efea63b2
:END:

#+BEGIN_SRC jupyter-julia
  using Plots
  p = plot(1:size(bigdf)[1], bigdf[names(df)[2]])
  plot!(p, 1:size(bigdf)[1], bigdf[names(df)[3]])
  # for i = 3:size(bigdf)[2]
  #     plot!(p, 1:size(bigdf)[1], bigdf[names(df)[i]])
  # end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[168]:
[[file:./obipy-resources/jhtL52.svg]]
:END:


*** Model

A recurrent model which takes a sequence of annotations, attends, and returns a predicted output.

Approximately follows [[https://arxiv.org/abs/1409.0473][Bahdanau et al]]. but used an lstm cell for the output sequence which may be different from what's described in the paper.

**** tensorflow - pycall

***** train

#+BEGIN_SRC jupyter-julia
  len_in = 100
  len_out = 20
  nbatch = 32
  nepochs = 10
  model_dir = "tmpZc797s" # mktempdir(abspath(""))
  frac_test = .3
  @info "model is stored in $model_dir"
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[4]:
:END:

#+BEGIN_SRC jupyter-julia
  using PyCall
  using Queryverse, DataFrames
  @pyimport pydoc
  df = load("postproc3.feather") |> DataFrame
  delete!(df, :datevalue)
  data = convert(Array{Float64}, df)
  len_train = floor(Int64, size(data)[1] * (1-frac_test))  
  train_data = data[1:len_train,:]
  test_data = data[len_train+1:end,:]
  # data = diff(data, 1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[5]:
#+BEGIN_EXAMPLE
  124470×15 Array{Float64,2}:
  0.0834541  474.229  537.9   5680.0   …  0.00888     50.3186  57.37
  0.0834541  474.229  537.9   5680.0      0.00888     50.3186  57.37
  0.0834542  474.23   537.9   5680.0      0.00888     50.3185  57.37
  0.0834542  474.23   537.9   5680.0      0.00888     50.3184  57.3699
  0.0834542  474.23   537.9   5680.0      0.00888     50.3184  57.3698
  0.0834543  474.23   537.9   5680.0   …  0.00888     50.3183  57.3698
  0.0834543  474.231  537.9   5680.0      0.00888     50.3183  57.3697
  0.0834543  474.231  537.9   5680.0      0.00888     50.3182  57.3696
  0.0834544  474.231  537.9   5680.0      0.00888     50.3181  57.3695
  0.0834544  474.231  537.9   5680.0      0.00888     50.3181  57.3695
  0.0834544  474.232  537.9   5680.0   …  0.00888     50.318   57.3694
  0.0834545  474.232  537.9   5680.0      0.00888     50.3179  57.3693
  0.0834545  474.232  537.9   5680.0      0.00888     50.3179  57.3693
  ⋮                                    ⋱
  0.09069    520.15   589.46  5709.25     0.00915594  52.4138  59.8844
  0.09069    520.15   589.46  5709.23     0.00915592  52.4139  59.884
  0.09069    520.15   589.46  5709.21  …  0.0091559   52.4141  59.8836
  0.09069    520.15   589.46  5709.19     0.00915587  52.4142  59.8832
  0.09069    520.15   589.46  5709.17     0.00915585  52.4144  59.8828
  0.09069    520.15   589.46  5709.16     0.00915583  52.4145  59.8824
  0.09069    520.15   589.46  5709.16     0.00915581  52.4146  59.882
  0.09069    520.15   589.46  5709.16  …  0.00915578  52.4148  59.8817
  0.09069    520.15   589.46  5709.16     0.00915576  52.4149  59.8813
  0.09069    520.15   589.46  5709.16     0.00915574  52.4151  59.8809
  0.09069    520.15   589.46  5709.16     0.00915572  52.4152  59.8805
  0.09069    520.15   589.46  5709.16     0.00915569  52.4154  59.8801
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  params = Dict("lr_start"=>.0001,
                "lr_decay"=>.99999,
                "len_in"=>len_in,
                "len_out"=>len_out,
                "nepochs"=>nepochs,
                "Nh"=>10, # size of hidden layer
                "Nc"=>12, # size of context
                "D"=>size(train_data)[2])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[7]:
#+BEGIN_EXAMPLE
  Dict{String,Real} with 8 entries:
  "Nc"       => 12
  "len_out"  => 20
  "lr_decay" => 0.99999
  "lr_start" => 0.0001
  "nepochs"  => 10
  "Nh"       => 10
  "D"        => 15
  "len_in"   => 100
#+END_EXAMPLE
:END:

Load local file
#+BEGIN_SRC jupyter-julia
  localpath = "src/pybot/input.py"
  filepath = abspath(joinpath(dirname(@__FILE__),localpath))
  specname = "pybot.input" # need pybot inorder to allow relative import
  pymodule = pyimport_module(filepath, specname)
  functionname = "input_wrapper_function"
  pyinput_fn = pymodule[Symbol(functionname)](train_data, len_in, len_out, nbatch, nepochs)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[12]:
: PyObject <function input_wrapper_function.<locals>._input_fn at 0x7fb0894648c8>
:END:

#+BEGIN_SRC jupyter-julia
  localpath = "pybot/model.py"
  filepath = abspath(joinpath(dirname(@__FILE__),localpath))
  modulename = "pybot.model" # need pybot inorder to allow relative import
  functionname = "model_fn"
  modelmod = pyimport_module(filepath, modulename)
  pymodel_fn = modelmod[Symbol(functionname)]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[24]:
: PyObject <function model_fn at 0x7fe6182a2378>
:END:

#+BEGIN_SRC jupyter-julia :eval never :exports none :tangle n
  imp.reload(modelmod)
#+END_SRC

#+BEGIN_SRC jupyter-julia :eval never :exports none :tangle n
  model_fn = pydoc.locate("pybot.model.model_fn")
#+END_SRC

#+BEGIN_SRC jupyter-julia
  @pyimport tensorflow as pytf
  pytf.logging[:set_verbosity](pytf.logging[:DEBUG])
  config = pytf.estimator[:RunConfig](save_summary_steps=10000,
                                      tf_random_seed=1234,
                                      save_checkpoints_secs=600)
  estimator = pytf.estimator[:Estimator](model_fn=pymodel_fn, params=params,
                                         model_dir=model_dir, config=config)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[9]:
: PyObject <tensorflow.python.estimator.estimator.Estimator object at 0x7fc604033208>
:END:

#+BEGIN_SRC jupyter-julia
   estimator[:train](pyinput_fn)
#+END_SRC

#+BEGIN_SRC jupyter-julia :eval never :exports none :tangle n
  rm(model_dir, recursive=true)
#+END_SRC


***** eval

#+BEGIN_SRC jupyter-julia
  localpath = "src/pybot/input.py"
  filepath = abspath(joinpath(dirname(@__FILE__),localpath))
  specname = "pybot.input" # need pybot inorder to allow relative import
  pymodule = pyimport_module(filepath, specname)
  functionname = "input_wrapper_function"
  nepochs = 1
  nbatch = nothing  
  pyinput_fn_test = pymodule[Symbol(functionname)](test_data, len_in, len_out, nbatch, nepochs)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[27]:
: PyObject <function input_wrapper_function.<locals>._input_fn at 0x7fe5780c7b70>
:END:

#+BEGIN_SRC jupyter-julia
  val = estimator[:evaluate](pyinput_fn_test)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[54]:
#+BEGIN_EXAMPLE
  Dict{Any,Any} with 4 entries:
  "MAE"         => 0.0208618
  "global_step" => 0
  "MSE"         => 0.0113027
  "loss"        => 0.0113027
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  preds = estimator[:predict](pyinput_fn_test)
  preds = PyCall.builtin[:list](preds)
#+END_SRC

#+BEGIN_SRC jupyter-julia
  ytrue(k::Int64) = diff(test_data[(k+len_in-1):(k+len_in+len_out-1),:], 1)
  err(k::Int64) = mean(abs.(sum(ytrue(k),1) .- sum(preds[k], 1)))
  relerr(k::Int64) = mean(abs.((sum(ytrue(k),1) .- sum(preds[k], 1))./(1e-6.+sum(ytrue(k),1))))
  errvec = map(err, 1:124350);
  relerrvec = map(relerr, 1:124350);
#+END_SRC

#+BEGIN_SRC jupyter-julia
  using Gadfly
  p = Gadfly.plot(layer(x=1:124350, y=errvec, Geom.smooth, Theme(default_color="orange")),
                  layer(x=1:124350, y=errvec, Geom.point, Theme(point_size=0.3mm,
                                                                default_color="blue")),
                  Guide.xlabel("Time index"),
                  Guide.ylabel("Absolute error"));
  draw(PNG("tmp.png", 6inch, 4inch), p)
#+END_SRC

#+RESULTS:
[[file:obipy-resources/tmp.png]]

Though still not negligible, mean error hovers below 0.5.

#+BEGIN_SRC jupyter-julia
  using Gadfly
  p = Gadfly.plot(layer(x=1:124350, y=relerrvec, Geom.smooth, Theme(default_color="orange")),
                  layer(x=1:124350, y=relerrvec, Geom.point, Theme(point_size=0.3mm,
                                                                   default_color="blue")),
                  Guide.xlabel("Time index"),
                  Guide.ylabel("Relative error"));
  draw(PNG("tmp2.png", 6inch, 4inch), p)
#+END_SRC


**** TensorFlow.jl

***** works

#+BEGIN_SRC jupyter-julia
  using TensorFlow
  const tf = TensorFlow
  sess = Session(Graph())
  summary = tf.summary
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[53]:
: Session(Ptr{Nothing} @0x00007ff1e1a52fd0)
:END:

Time major input
#+BEGIN_SRC jupyter-julia
  D = size(df)[2] # number of features in input
  xin = tf.placeholder(Float32, shape=[len_seq_in-1, nbatch, D])
  yin = tf.placeholder(Float32, shape=[len_seq_out, nbatch, D])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[54]:
: <Tensor placeholder_2:1 shape=(5, 32, 15) dtype=Float32>
:END:

parameters
#+BEGIN_SRC jupyter-julia
  learning_rate = .0001
  n_epochs= 10
  Nh = 10 # size of hidden layer
  Nc = 12 # size of context
  Nfwd = Nh÷2
  Nbwd = Nh÷2
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[22]:
: 5
:END:

#+BEGIN_SRC jupyter-julia
  forward  = nn.rnn_cell.LSTMCell(Nfwd)
  backward = nn.rnn_cell.LSTMCell(Nbwd)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[23]:
: TensorFlow.nn.rnn_cell.LSTMCell{Float32}(5, 1.0f0)
:END:

The easy way for making bidirectional rnn
#+BEGIN_SRC jupyter-julia
  x = tf.unstack(xin, axis=1)
  fout, fstate = nn.rnn(forward, x, scope="fwRNN")
  bout, bstate = nn.rnn(backward, reverse(x, 1), scope="bwRNN")
  reverse!(bout)
  enc = tf.stack([tf.concat([f,b], 2) for (f,b) in zip(fout, bout)])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[24]:
: <Tensor Pack:1 shape=(50, 32, 10) dtype=Float32>
:END:
  
#+BEGIN_SRC jupyter-julia
  using Distributions
  variable_scope("align"; initializer=Normal(0, .001)) do
       global Wat = get_variable("Wat", [Nh*2, Nc], Float32)
       global Was = get_variable("Was", [1, 1, Nh, Nc], Float32)
       global B = get_variable("B", [Nc], Float32, initializer=tf.ones(Nc))
       global vt = get_variable("vt", [Nc], Float32)
  end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[25]:
: Variable{Float32}(<Tensor align/vt:1 shape=(12) dtype=Float32>, <Tensor align/vt/Assign:1 shape=(12) dtype=Float32>)
:END:

#+BEGIN_SRC jupyter-julia
  score(e,t) = dropdims(tf.tanh(tf.concat([e,t], 2)*Wat + B)*expand_dims(vt, 2))
  align(E,t) = tf.stack(map(e->score(e,t), tf.unstack(E, axis=1)), axis=1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[26]:
: align (generic function with 1 method)
:END:

Use forward output state to initialize output rnn cell.
#+BEGIN_SRC jupyter-julia
  variable_scope("to_recur_state"; initializer=Normal(0, .001)) do
       global Wts = get_variable("W", [Nh, Nh*2], Float32)
       global Bts = get_variable("B", [Nh*2], Float32)
  end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[27]:
: Variable{Float32}(<Tensor to_recur_state/B:1 shape=(20) dtype=Float32>, <Tensor to_recur_state/B/Assign:1 shape=(20) dtype=Float32>)
:END:

#+BEGIN_SRC jupyter-julia
  variable_scope("from_recur_state"; initializer=Normal(0, .001)) do
       global Wtso = get_variable("Wo", [Nh, D], Float32)
       global Btso = get_variable("Bo", [D], Float32)
  end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[28]:
: Variable{Float32}(<Tensor from_recur_state/Bo:1 shape=(15) dtype=Float32>, <Tensor from_recur_state/Bo/Assign:1 shape=(15) dtype=Float32>)
:END:

Transfer state to output rnn and split 
#+BEGIN_SRC jupyter-julia
  state_init = tf.concat([fstate.c, fstate.h], 2)*Wts + Bts
  # range corresponding to c and h
  crange = 1:Nh
  hrange = 1+Nh:2*Nh
  cₜ = state_init[:, crange]
  hₜ = state_init[:, hrange]
  recur = variable_scope("recur") do
    nn.rnn_cell.LSTMCell(Nh)
  end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[29]:
: TensorFlow.nn.rnn_cell.LSTMCell{Float32}(10, 1.0f0)
:END:

#+BEGIN_SRC jupyter-julia
  preds = []
  for i = 1:len_seq_out
      global hₜ
      global cₜ
      α = expand_dims(align(enc, hₜ), 3)
      yi = tf.reduce_sum(α .* enc; axis=1)
      si = tf.nn.rnn_cell.LSTMStateTuple(cₜ, hₜ)
      yo, so = variable_scope("recuriter"; reuse=i>1) do
        recur(yi, si)
      end
      cₜ, hₜ = so.c, so.h
      push!(preds, yo*Wtso + Btso)
  end
  ŷ = tf.stack(preds)
  loss_MSE = reduce_mean((yin.-ŷ).^2)
  loss_summary = summary.scalar("MSE loss", loss_MSE)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[30]:
: <Tensor reduce_6:1 shape=() dtype=Float32>
:END:


***** Run model

#+BEGIN_SRC jupyter-julia
  optimizer=train.AdamOptimizer(learning_rate)
  # Gradient decent with gradient clipping
  # gvs = train.compute_gradients(optimizer, loss_MSE)
  # capped_gvs = [(clip_by_norm(grad, 5.), var) for (grad, var) in gvs]
  # opt_step = train.apply_gradients(optimizer,capped_gvs)
  opt_step = train.minimize(optimizer, loss_MSE)
#+END_SRC

#+RESULTS:

#+BEGIN_SRC jupyter-julia
  run(sess, global_variables_initializer())
#+END_SRC

#+BEGIN_SRC jupyter-julia :eval never 
  merged_summary_op = summary.merge_all()
  # Create a summary writer
  summary_dir = mktempdir()
  @info "summary directory: $(summary_dir)"
  summary_writer = summary.FileWriter(summary_dir)

  step = 0
  @showprogress for epoch = 1:n_epochs
      batchgen = get_generator()
      epoch_loss = Float64[]
      for i in batchgen
          if size(i[1])[2] == nbatch
              @debug x, y, size(i[1]), size(i[2])
              loss_o, summaries, _ = run(sess, (loss_MSE, merged_summary_op, opt_step), Dict(xin=>i[1], yin=>i[2]))
              step = step+1
              write(summary_writer, summaries, step)
              push!(epoch_loss, loss_o)
          end
      end
    
      push!(basic_train_loss, mean(epoch_loss))    
  end
#+END_SRC

#+BEGIN_SRC jupyter-julia :eval never  
  plot(basic_train_loss, label="training loss")
#+END_SRC


**** Flux

#+BEGIN_SRC jupyter-julia
  using Flux: glorot_uniform, zeros, chunk, LSTM, flip, Dense, param, reset!
  Nin = size(df)[2]
  Nh = 10 # size of hidden layer
  Nc = 10 # size of context
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[19]:
: 10
:END:

A recurrent model which takes an input and returns a prediction
#+BEGIN_SRC jupyter-julia
  # A recurrent model which takes a token and returns a context-dependent
  # annotation.
  forward  = LSTM(Nin, Int(ceil(Nh÷2)))
  backward = LSTM(Nin, Int(floor(Nh÷2)))
  encode(inputs) = vcat.(forward.(inputs), flip(backward, inputs))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[20]:
: encode (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  Wₐₛ = param(glorot_uniform(Nc, Nh))
  Wₐₜ= param(glorot_uniform(Nc, Nh))
  vₜ = param(glorot_uniform(Nc))
  score(s,t) = vₜ'*tanh.(Wₐₛ*s + Wₐₛ*t)
  score_t(t) = s->score(s,t)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[21]:
: score_t (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  """
      t: state of the current output unit
      s: array of input states
  """
  function alignvec(s,t)
      score_s = score_t(t) 
      score_vec = exp.(vcat(score_s.(s)...))
      score_vec ./ reduce(+, score_vec, dims=1)
  end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[22]:
: alignvec
:END:

#+BEGIN_SRC jupyter-julia
  recur = LSTM(Nc, Nh)
  state_transfer = Dense(size(vcat(forward.state...))[1], size(vcat(recur.state...))[1])
  ranges = []
  output_transfer = Dense(Nh, Nin)
  let rstart=1
      for s in recur.state
          rend = rstart+length(s)-1
          push!(ranges, rstart:rend)
          rstart = rend+1
      end
  end

  function decode(enc)
      recurstate = state_transfer(vcat(forward.state...))
      recur.state = [recurstate[r,:] for r in ranges]
      preds = []
      hₜ = recur.state[1]
      # restack enc along time and unstack along batch inorder to multiply by alpha
      # probably better way to do this...
      enc′ = unstack(vcat((e->reshape(e, (1, size(e)...))).(enc)...), 3)
      for i in 1:len_seq_out
          α = alignvec(enc, hₜ)
          # α .* enc, there is probably cleaner way of doing this:
          c = cat(map(x->(x[1]' * x[2])'[:,:], zip(unstack(α, 2), enc′))..., dims=2)
          hₜ = recur(c)
          push!(preds, output_transfer(hₜ)) 
      end
      return preds
  end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[23]:
: decode (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  state = (forward, backward, recur, output_transfer, state_transfer)
  function model(x)
      ŷ = decode(encode(x))
      reset!(state)
      return ŷ
  end
  meansquarederror(ŷ, y) = sum((ŷ .- y).^2)/size(y, 2)
  loss(x, y) = meansquarederror(model(x), y)
#+END_SRC

#+BEGIN_SRC jupyter-julia  
  opt = ADAM(params(state))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[37]:
: #43 (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia :eval never 
@epochs 1 begin
    tic()
    train = get_generator()
    for i in train
        l = loss(i[1], i[2])
        println("The loss for current minibatch is $l")
        Flux.back!(l)
        opt()
    end
    toc()
end
#+END_SRC

#+BEGIN_SRC jupyter-julia :eval never 
  evalcb = () -> @show loss(data[500]...)
  Flux.train!(loss, data, opt, cb = throttle(evalcb, 10))
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 6a3acdf0-0375-4ed8-b671-3b4aeb3d459e
:END:

#+BEGIN_SRC jupyter-julia
  # Prediction

  using StatsBase: wsample

  function predict(s)
    ts = encode(tokenise(s, alphabet))
    ps = Any[:start]
    for i = 1:50
      dist = decode1(ts, onehot(ps[end], phones))
      next = wsample(phones, dist.data)
      next == :end && break
      push!(ps, next)
    end
    return ps[2:end]
  end

#+END_SRC

#+RESULTS:
:RESULTS:
# Out[284]:
#+BEGIN_EXAMPLE
  32-element Array{Any,1}:
  :AH0
  :L
  :AH0
  :AA1
  :N
  :K
  :K
  :L
  :IH0
  :V
  :F
  :R
  :IY1
  ⋮
  :N
  :N
  :ER0
  :F
  :IH0
  :N
  :T
  :IH0
  :N
  :IH0
  :L
  :IY0
#+END_EXAMPLE
:END:


** Python 
:PROPERTIES:
:header-args: ipython :session mbotpy-ssh.json :results raw drawer :tangle mbot.py :eval never-export :async t
:END:

*** new

**** Get data

#+BEGIN_SRC ipython
  import pandas as pd
  df = pd.read_feather("postproc3.feather")
  df.drop("datevalue", inplace=True, axis=1)
#+END_SRC

#+RESULTS:
:RESULTS:
1 - 23b3f99a-a456-49c3-be34-d0e3d05da8ab
:END:

Batch generator
#+BEGIN_SRC jupyter-julia
  len_seq_in = 50
  len_seq_out = 5
  nbatch = 32
  nchannels = size(df)[2]
  end_index = size(df)[1] - (len_seq_in + len_out)
  # start_indices = 1:end_index
#+END_SRC


**** Tensorflow

***** Input

#+BEGIN_SRC ipython
  import tensorflow as tf
  import numpy as np


  def input_wrapper_function(data, len_in, len_out, nbatch, nepochs):
      end_ix = data.shape[1] - (len_in + len_out)
      start_indices = np.range(end_ix)

      tdata = tf.constant(data, dtype=tf.float32)
      train_ds = tf.data.Dataset.from_tensor_slices(start_indices)
      train_ds = train_ds.shuffle(buffer_size=40000)

      def get_input_output(ix):
          range_in = tf.range(ix, ix + len_in)
          range_out = tf.range(ix + len_in, ix + len_in + len_out)
          return tf.gather(tdata, range_in, axis=0), tf.gather(tdata, range_out, axis=0)

      train_ds = train_ds.map(get_input_output, num_parallel_calls=8).prefetch(5000)
      if nbatch is None:
          nbatch = start_indices.shape[0]
      train_ds = train_ds.batch(nbatch).repeat(nepochs)

      def _input_fn():
          return train_ds.make_one_shot_iterator().get_next()

      return _input_fn
#+END_SRC


***** predictor

#+BEGIN_SRC ipython 
  from pydoc import locate
  import tensorflow as tf
  nn = tf.nn
  dense = tf.layers.dense
  ModeKeys = tf.estimator.ModeKeys
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[23]:
:END:

temporary
#+BEGIN_SRC ipython :tangle f
  D = df.shape[1]
  seqin = tf.placeholder(tf.float32, shape=[None, None, D])
  seqout = tf.placeholder(tf.float32, shape=[None, None, D])
  params = {}
  params["Nh"] = 10
  params["Nc"] = 12
  params["learning_rate"] = 1e-4
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[24]:
:END:

#+BEGIN_SRC ipython
  Nh = params["Nh"]
  Nc = params["Nc"]

  Nfwd = Nh // 2
  Nbwd = Nh // 2
  forward  = nn.rnn_cell.LSTMCell(Nfwd)
  backward = nn.rnn_cell.LSTMCell(Nbwd)

  outputs, output_states = nn.bidirectional_dynamic_rnn(forward, backward, seqin, time_major=True, dtype=tf.float32)
  statfw, statbw = output_states
  enc = tf.concat(outputs, axis=-1)
  statfw = tf.concat(statfw, 1) # concatenate 
  state_init = tf.layers.dense(statfw, 2*Nh)

  # Transfer state to output rnn and split 
  ct = state_init[:, :Nh]
  ht = state_init[:, Nh:2*Nh]

  recur = nn.rnn_cell.LSTMCell(Nh)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[25]:
:END:

#+BEGIN_SRC ipython
  def align(t):
      T = tf.tile(tf.expand_dims(t, 0),[tf.shape(enc)[0], 1, 1])
      stilde = tf.layers.dense(tf.concat([T, enc], -1), Nc, activation = tf.tanh)
      return nn.softmax(tf.squeeze(tf.layers.dense(stilde, 1, use_bias=False), axis=2), axis=0)

  def body(i, s, y_tm1, ta):
      """
      predict based on previous hidden state and prediction
      i: current timestep
      s: last step's state. shape = ((B x n_hidden), (B x n_hidden))
      ta: dynamic tensorarray to store outputs (can be static, atm)
      """
      alpha = align(s.h)
      c = tf.reduce_sum(tf.multiply(enc, tf.expand_dims(alpha, 1)), axis=0)
      ytilde = tf.concat([y_tm1, c], axis=-1)  # concatenate context and previous
      y_t, s = recur(tf.layers.dense(ytilde, Nh), s)
      ta = ta.write(i, y_t)
      return i+1, s, y_t, ta
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[26]:
:END:

#+BEGIN_SRC ipython
  cond = lambda i,s,y,ta: i < tf.shape(enc)[0]
  i0 = tf.constant(0, dtype=tf.int32)
  y0 = enc[-1,:,:]# tf.zeros_like(enc[0,:,:])
  s0 = nn.rnn_cell.LSTMStateTuple(c=ct, h=ht)
  ta0 = tf.TensorArray(dtype=tf.float32, size=0, dynamic_size=True,
                           element_shape=tf.TensorShape([None,Nh]))  # array of tensors
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[27]:
:END:

#+BEGIN_SRC ipython
  i,s,y,ta = tf.while_loop(cond, body, [i0,s0,y0,ta0])
  yhat = tf.layers.dense(ta.stack(), D)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[28]:
:END:


***** loss

#+BEGIN_SRC jupyter-julia
  loss = tf.losses.mean_squared_error(seqout, yhat)
  loss_summary = tf.summary.scalar("MSE_loss", loss)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[30]:
: <Tensor reduce_6:1 shape=() dtype=Float32>
:END:


***** optimizer

Gradient decent with gradient clipping
#+BEGIN_SRC ipython
  train = tf.train
  optimizer=train.AdamOptimizer(learning_rate)
  gvs = optimizer.compute_gradients(loss)
  capped_gvs = [(tf.clip_by_norm(grad, 5.), var) for (grad, var) in gvs] 
  opt_step = optimizer.apply_gradients(capped_gvs)
  # opt_step = train.minimize(optimizer, loss_MSE)  
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[55]:
:END:


*** old

**** does something

#+BEGIN_SRC ipython
  cd ~/Documents/work/marketbot
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[5]:
:END:

#+BEGIN_SRC ipython
  import tensorflow as tf
  tf.reset_default_graph()
  lstm = locate('src.models.lstm2')
  # lstm2.__main__()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  lstm.main()
#+END_SRC

#+BEGIN_SRC ipython
  predict_input_fn = lstm._input_fn_wrapper('data/clean/data.csv', lstm.ModeKeys.PREDICT, 10, lstm.DEFAULT_TRAIN_PARAMS)
  rnn = lstm.estimator()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  rnn.predict(input_fn=predict_input_fn)
#+END_SRC



**** namespace

#+BEGIN_SRC ipython
  import tensorflow as tf
  tf.reset_default_graph()
  sess = tf.InteractiveSession()

  with tf.variable_scope('a'):
      with tf.variable_scope('b'):
          v = tf.get_variable('v', initializer=tf.ones(3))
  with tf.variable_scope('', reuse=True):
      v2 = tf.get_variable('a/b/v')
  init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
  sess.run(init)  
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython :exports both
  V1 = tf.random_normal((10,3))
  V2 = tf.get_variable('abc', initializer=V1)
  with tf.variable_scope('', reuse=True):
    abc = tf.get_variable('abc', (10,3))
    abc.assign(tf.ones((10,3)))
  init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
  sess.run(init)
  abc.eval() - V2.eval()
#+END_SRC

#+RESULTS:
:RESULTS:
#+BEGIN_EXAMPLE
  array([[0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.]], dtype=float32)
#+END_EXAMPLE
:END:


**** trying out eager

#+BEGIN_SRC ipython
  import tensorflow as tf
  tfe = tf.contrib.eager
  tfe.enable_eager_execution()
#+END_SRC

#+BEGIN_SRC ipython
  import numpy as np
  A = np.random.randn(5,3)
  mu, std = tf.nn.normalize_moments(A.shape[0], *tf.nn.moments(tf.cast(A, dtype=tf.float32), axes=0), None)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:



**** Run ira_rnn

#+BEGIN_SRC ipython
  %cd marketbot
#+END_SRC

#+RESULTS:
:RESULTS:
:END:
  
#+BEGIN_SRC ipython
  from importlib import reload
  import src.models.lstm as lstm
  reload(lstm)
  lstm.__main__()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  lstm = locate('src.models.lstm')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


**** import_fn

#+BEGIN_SRC ipython
  from pydoc import locate
  import tensorflow as tf
  import numpy as np
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  path = 'marketbot/data/clean/data3.csv'
  data = np.genfromtxt(path, delimiter=',')[2:, 1:]
  data.shape
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : (85985, 3)
  :END:
  
  #+BEGIN_SRC ipython
  data = tf.convert_to_tensor(data)
  horizon = 10
  window = 100
  targets = data[horizon + window - 1 :, 1] / data[window - 1 : -horizon, 1] - 1
  features = tf.contrib.signal.frame(data, window, 1, axis=0)[:-horizon, :]
  tf.data.Dataset.from_tensor_slices({'train': features, 'repsonse':targets})
#+END_SRC




**** setup

***** Load

#+BEGIN_SRC ipython
  import pandas as pd
  import os 
  from datetime import datetime as dt
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

****** ak preprocess

#+BEGIN_SRC ipython
  from marketbot.src import dataloader
  agg_df = dataloader.get_data('marketbot/data/data2.csv')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

******* temp
:PROPERTIES:
:header-args: :eval never
:END:

#+BEGIN_SRC ipython
path='marketbot/data/data2.csv'; interval_length=1; window_length=100; stride_length=1; predict_length=10
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  df = pd.read_csv(path, index_col='sequence')
  df['time'] = pd.to_datetime(df['time'])
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  def mean_price(g):
      """ computes size-weighted price of trades """
      vol = g['volume'].sum()
      price = (g['volume'] * g['price']).sum() / (vol + 1e-8)
      return pd.Series([vol, price], ['volume', 'price'])
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


#+BEGIN_SRC ipython
  def discretize(df, interval_length):
      """ interval_length = number of seconds to aggregate in """
      if interval_length == 0:
          grouped = df.groupby('time')
      else:
          grouped = df.resample('{}S'.format(interval_length), on='time')
      return grouped.apply(mean_price).reset_index()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


#+BEGIN_SRC ipython
  agg_df = discretize(df, interval_length)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  agg_df['price2'] = np.nan
  agg_df['volume2'] = np.nan
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

whereever price == 0 we have a missing observation
#+BEGIN_SRC ipython
  agg_df.loc[(agg_df.price != 0), "price2"] = agg_df[(agg_df.price != 0)].price
  agg_df.loc[(agg_df.price != 0), "volume2"] = agg_df[(agg_df.price != 0)].volume
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  agg_df.volume2 = agg_df.volume2.fillna(method='ffill')
  agg_df.price2 = agg_df.price2.fillna(method='ffill')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  agg_df['change2'] = agg_df.price2.pct_change()
  agg_df['outcomes2'] = agg_df.price2.pct_change(periods= predict_length).shift(-predict_length)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:



#+BEGIN_SRC ipython
  agg_df.volume = agg_df.volume.fillna(0.0)
  agg_df.price = agg_df.price.fillna(method='ffill')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  agg_df['change'] = agg_df.price.pct_change()
  agg_df['outcomes'] = agg_df.price.pct_change(periods= predict_length).shift(-predict_length)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


****** original preprocesss
:PROPERTIES:
:header-args: :eval never
:END:

#+BEGIN_SRC ipython
  df = pd.read_csv('marketbot/data/data2.csv')
  df['time'] = pd.to_datetime(df.time)
  df['timestamp'] = df.time.apply(dt.timestamp)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  from matplotlib import pyplot as plt
  import numpy as np
  data = df.groupby('timestamp').apply(lambda g: g[['price', 'size']].mean(axis=0))
  assert np.diff(data.index).min() > 0  # data is sorted
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


***** model config

#+BEGIN_SRC ipython
  os.chdir('marketbot/src')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  from collections import namedtuple
  flagdct = {'batch_size': 64,
             'data_dir': '/tmp/dat/',
             'hidden_dim': 200,
             'l1reg_coeff': 1e-10,
             'l2reg_coeff': 1e-9,
             # 'l1reg_coeff': 1,
             # 'l2reg_coeff': 1,
             'latent_dim': 160,
             'logdir': '/tmp/log/',
             'n_epochs': 100000,
             'n_iterations': 100000,
             'n_samples_predict': 20,
             'n_samples_train': 10,
             'print_every': 1000, 
             'huber_loss_delta': .1,
             'use_update_ops': False}  # update_ops control dependency is necessary for batch norm
  FLAGS = namedtuple('FLAGS',flagdct.keys())(**flagdct)
  ff_params = dict(dim_hidden=20, rnn_stack_height=3)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


**** explore

#+BEGIN_SRC ipython
  %matplotlib inline
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython :exports both
  import numpy as np
  df['logdiffprice'] = np.log(df.price).diff()
  df[['logdiffprice', 'size', 'timestamp', 'price']].plot(x=['timestamp', 'timestamp'], y=['price', 'logdiffprice'], s=.7, kind='scatter', figsize=(30, 15))
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7fec4cb0c748>
[[file:./obipy-resources/8538-rS.png]]
:END:

#+BEGIN_SRC ipython :exports both
  from matplotlib import pyplot as plt
  fig, axes = plt.subplots(2, 1, sharex=True, figsize=(10,10))
  df[['size', 'timestamp', 'price']].plot(x='timestamp', y='price', s=.7, kind='scatter',
                                          figsize=(30, 15), ax=axes[0])
  df[['size', 'timestamp', 'logdiffprice']].plot(x='timestamp', y='logdiffprice', s=.7,
                                                 kind='scatter', figsize=(30, 15), ax=axes[1])
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7fec4c9b1400>
[[file:./obipy-resources/8538L2Y.png]]
:END:



#+BEGIN_SRC ipython :exports both
  df[['logdiffprice', 'size', 'timestamp']].plot(x='timestamp', y='logdiffprice', figsize=(30, 15))
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7fec4d75d518>
[[file:./obipy-resources/8538kXG.png]]
:END:

#+BEGIN_SRC ipython :exports both
  fig, axes = plt.subplots(2, 1, sharex=True, figsize=(10,10))
  for i, col in enumerate(data.columns.values):
     data.reset_index().plot.scatter(x='timestamp', y=col, ax=axes[i], s=.7)
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./obipy-resources/19656F3W.png]]
:END:

#+BEGIN_SRC ipython :exports both
  from pandas.plotting import scatter_matrix
  scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')
#+END_SRC

#+RESULTS:
:RESULTS:
#+BEGIN_EXAMPLE
  array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f271599bbe0>,
  <matplotlib.axes._subplots.AxesSubplot object at 0x7f2715aaaeb8>],
  [<matplotlib.axes._subplots.AxesSubplot object at 0x7f27159c8be0>,
  <matplotlib.axes._subplots.AxesSubplot object at 0x7f271582e438>]], dtype=object)
#+END_EXAMPLE
[[file:./obipy-resources/19656SBd.png]]
:END:


**** debug

***** playground

#+BEGIN_SRC ipython
  import os
  os.chdir('marketbot')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  import  src.dataloader as dataloader
  get_data = dataloader.get_data
  path='data/data2.csv'; predict_length=10; interval_length=1; window_length=100
  features, outcomes = get_data(path, interval_length, predict_length)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  import src.playground1 as pg
  h = 10
  split=.5
  env = pg.Environment(features, h, split)  
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  tr = env.reset()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython :exports both
  cur, loss, doneflag, info = env.step(0.0)
  cur, loss, doneflag, info
#+END_SRC

#+RESULTS:
:RESULTS:
: (array([  3.00000000e-02,   1.31317100e+04,   2.22044605e-16]), 0.0, False, {})
:END:

#+BEGIN_SRC ipython
  from importlib import reload
  reload(pg)
#+END_SRC

#+RESULTS:
:RESULTS:
: <module 'src.playground1' from '/home/ishai/Documents/work/irabitcoin/marketbot/src/playground1.py'>
:END:


***** run

****** setup

#+BEGIN_SRC ipython
  import os
  os.chdir('marketbot')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

Create input provider
#+BEGIN_SRC ipython
  import  src.dataloader as dataloader
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  get_data = dataloader.get_data
  WindowGen = dataloader.WindowGen
  quantize = dataloader.quantize
  path='data/data2.csv'; predict_length=10; interval_length=1; window_length=100
  features, outcomes = get_data(path, interval_length, predict_length)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  import numpy as np
  amin=-0.01
  amax=0.01
  step=1e-5
  Y_n_categories = int(np.round((amax-amin)/step))
#+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

#+BEGIN_SRC ipython
  q_outcomes = quantize(outcomes, amin=amin, amax=amax, step=step)
  q_outcomes = np.expand_dims(q_outcomes, axis=1)
  gen = WindowGen(features, q_outcomes, window_length, predict_length, Y_n_categories)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython :eval never
  from importlib import reload
  reload(dataloader)
#+END_SRC

#+BEGIN_SRC ipython
  import marketbot.src.model1.runner as runner
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  from collections import namedtuple
  flagdct = {'batch_size': 128,
             'data_dir': '/tmp/dat/',
             'hidden_dim': 200,
             'l1reg_coeff': 1e-10,
             'l2reg_coeff': 1e-9,
             # 'l1reg_coeff': 1,
             # 'l2reg_coeff': 1,
             'latent_dim': 160,
             'logdir': '/tmp/log/',
             'n_epochs': 100000,
             'n_iterations': 100000,
             'n_samples_predict': 20,
             'n_samples_train': 10,
             'print_every': 1000, 
             'huber_loss_delta': .1,
             'use_update_ops': False}  # update_ops control dependency is necessary for batch norm
  FLAGS = namedtuple('FLAGS',flagdct.keys())(**flagdct)
  ff_params = dict(dim_hidden=20, rnn_stack_height=3)

  catmodel = runner.Learner(ff_params, FLAGS)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  catmodel.initialize_train_graph(gen)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


****** train

#+BEGIN_SRC ipython
  catmodel.train(100)
#+END_SRC

#+BEGIN_SRC ipython
  from importlib import reload
  reload(runner)
#+END_SRC

#+RESULTS:
:RESULTS:
: <module 'marketbot.src.model1.runner' from '/home/ishai/Documents/work/irabitcoin/marketbot/src/model1/runner.py'>
:END:


****** predict

#+BEGIN_SRC ipython
  
#+END_SRC


***** genertor

#+BEGIN_SRC ipython
  import utils
  example_generator = utils.GrabSequence(X=data.values, t_ix=data.index.values, input_seq_len=100, time_gap_to_predict=10, stride=1)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  from importlib import reload
  reload(utils)
#+END_SRC

#+RESULTS:
:RESULTS:
: <module 'utils' from '/home/ishai/Documents/work/bitcoin/IraBC/marketbot/src/catnet/utils.py'>
:END:

#+BEGIN_SRC ipython
  import tensorflow as tf
  tf.reset_default_graph()
  g = tf.Graph()
  sess = tf.Session(graph=g)
  with g.as_default():
      train_ds = tf.data.Dataset.from_generator(example_generator, (tf.float32, tf.float32),
                                                (tf.TensorShape([None, 2]), tf.TensorShape([2])))
      train_ds.shuffle(buffer_size=100000)
      train_ds = train_ds.batch(100)
      iterator = tf.data.Iterator.from_structure(train_ds.output_types, train_ds.output_shapes)
      training_init_op = iterator.make_initializer(train_ds)
      batch = iterator.get_next()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  with sess.as_default():
      sess.run(training_init_op)
      while (True):
          b = sess.run(batch)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


**** run

#+BEGIN_SRC ipython
  from runner import Learner
  catmodel = Learner(ff_params, FLAGS)
  catmodel.fit(data.values, t_ix=data.index.values)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


** BROKEN Julia original - With JuliaDB
:PROPERTIES:
:header-args: jupyter-julia :session marketbot-julia-ssh.json :results raw drawer :tangle mbot.jl :eval never-export :async t
:END:

#+BEGIN_SRC jupyter-julia
  using Queryverse, JuliaDB
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[3]:
:END:

#+BEGIN_SRC jupyter-julia :results output
  datadir = "/home/ishai/Documents/work/marketbot/src/utils/data/raw"
  run(`ls -lh $(datadir)`)
#+END_SRC

#+RESULTS:
:RESULTS:
total 21M
-rw-rw-r-- 1 ishai ishai 2.5M Aug 14 00:12 20180813-171152.csv
-rw-rw-r-- 1 ishai ishai 1.7M Aug 14 02:50 20180814-001303.csv
-rw-rw-r-- 1 ishai ishai 1.7M Aug 15 03:42 20180815-004411.csv
-rw-rw-r-- 1 ishai ishai 1.6M Aug 15 12:38 20180815-113032.csv
-rw-rw-r-- 1 ishai ishai 457K Aug 15 13:46 20180815-131857.csv
-rw-rw-r-- 1 ishai ishai  14K Aug 15 13:48 20180815-134729.csv
-rw-rw-r-- 1 ishai ishai 1.1M Aug 15 15:15 20180815-134950.csv
-rw-rw-r-- 1 ishai ishai  12M Aug 16 13:46 20180815-160006.csv
:END:

#+BEGIN_SRC jupyter-julia :results output
  fname = maximum(readdir(datadir))
  run(`head $(joinpath(datadir, fname))`)
#+END_SRC

#+RESULTS:
:RESULTS:
2018-08-15T20:00:07.341000Z,289.01000000,0.95307512,ETH-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.05064896,BTC-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.06453496,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.52758604,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.75478104,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00118000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.56368644,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.03650000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00810000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00380000,BTC-USD
:END:

# DateTime does not recognize "2018-08-14T04:13:06.697000Z" so need to adjust string before processing. 
# Haven't figured out how to correctly use [[https://juliacomputing.com/TextParse.jl/stable/#TextParse.CustomParser][CustomParser]], but it looks like a nice approach to converting.
#+BEGIN_SRC jupyter-julia :eval never :exports never
using TextParse
cDateTime = CustomParser(x->x[1:end-5], DateTime)
#+END_SRC

# annoying thigns so far - 1.how to set types, 2. not much documentation 
# Later - set types using TextParse
#+BEGIN_SRC jupyter-julia
  colnames = ["date", "price", "size", "product_id"]
  t = loadtable(joinpath(datadir, fname), colnames=colnames, colparsers=Dict(:date=>String, :price=>Float64, :size=>Float64, :product_id=>String))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[18]:
#+BEGIN_EXAMPLE
  Distributed Table with 203134 rows in 1 chunks:
  date                           price    size        product_id
  ──────────────────────────────────────────────────────────────
  "2018-08-15T20:00:07.878000Z"  6383.82  0.050649    "BTC-USD"
  "2018-08-15T20:00:07.878000Z"  6383.82  0.064535    "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  1.52759     "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  1.75478     "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.00118     "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.563686    "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.0365      "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.0081      "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.0038      "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.2         "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.00998106  "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.0633251   "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.0158401   "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.23702     "BTC-USD"
  "2018-08-15T20:00:08.469000Z"  6383.81  0.00049985  "BTC-USD"
  "2018-08-15T20:00:08.469000Z"  6383.35  0.00050015  "BTC-USD"
  "2018-08-15T20:00:08.481000Z"  6383.35  0.00049985  "BTC-USD"
  "2018-08-15T20:00:08.562000Z"  6382.54  0.00810663  "BTC-USD"
  "2018-08-15T20:00:08.614000Z"  6382.53  0.00105     "BTC-USD"
  "2018-08-15T20:00:09.785000Z"  12.11    72.8834     "ETC-USD"
  "2018-08-15T20:00:10.486000Z"  289.01   2.1008      "ETH-USD"
  "2018-08-15T20:00:11.054000Z"  6376.89  0.0148466   "BTC-USD"
  "2018-08-15T20:00:11.054000Z"  6375.61  0.01        "BTC-USD"
  "2018-08-15T20:00:11.054000Z"  6375.0   0.007843    "BTC-USD"
  ⋮
#+END_EXAMPLE
:END:

Convert date column to datetime
#+BEGIN_SRC jupyter-julia
  t = setcol(t, :date, :date => x->DateTime(x[1:end-5]))
  t[1:10]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[9]:
#+BEGIN_EXAMPLE
  Table with 10 rows, 4 columns:
  date                    price    size       product_id
  ──────────────────────────────────────────────────────
  2018-08-14T04:13:04.88  269.3    1.0        "ETH-USD"
  2018-08-14T04:13:05.09  6009.43  0.0193853  "BTC-USD"
  2018-08-14T04:13:05.16  269.3    0.5        "ETH-USD"
  2018-08-14T04:13:05.17  6009.43  0.0159219  "BTC-USD"
  2018-08-14T04:13:06.54  6009.43  0.02295    "BTC-USD"
  2018-08-14T04:13:06.68  237.04   1.00002    "ETH-EUR"
  2018-08-14T04:13:06.69  52.48    0.5        "LTC-USD"
  2018-08-14T04:13:06.69  52.48    0.79       "LTC-USD"
  2018-08-14T04:13:06.69  52.48    13.71      "LTC-USD"
  2018-08-14T04:13:07     52.49    13.0       "LTC-USD"
#+END_EXAMPLE
:END:

This doesn't work here
#+BEGIN_SRC jupyter-julia :eval never
  t[1:100] |> Voyager()
#+END_SRC

#+RESULTS:
:RESULTS:
0 - e258681d-4db9-4b84-8d4a-10dc58b547c0
:END:

#+BEGIN_SRC jupyter-julia
  @time t |> @vlplot(:point, x=:date, y=:price, color="product_id")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[13]:
[[file:./obipy-resources/veUMiI.png]]
:END:

Lets look at transaction sizes
#+BEGIN_SRC jupyter-julia
  @time t |> @vlplot(:point, x=:date, y=:size, color="product_id")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[14]:
[[file:./obipy-resources/XvWwfT.png]]
:END:

Not that nice. 

Lets check if the data is sorted with in groups.
#+BEGIN_SRC jupyter-julia
  all(select(JuliaDB.groupby(issorted, t, :product_id; select = :date), :issorted))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[15]:
: true
:END:

#+BEGIN_SRC jupyter-julia
  @time t |> @vlplot(:line, x=:date, y=:size, color="product_id")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[16]:
[[file:./obipy-resources/TMY56X.png]]
:END:

#+BEGIN_SRC jupyter-julia :exports none :eval never exports none
  tdiff = t |> @filter(_.product_id=="ETH-USD") |> @Queryverse.map(_.date) |> collect |> diff |> @map({delta=_});
  tdiff |> @map({delta=log(Dates.value(_.delta))}) |> @vlplot(mark={:bar, clip=true}, x={:delta, bin={maxbins=1000}, scale={domain=[0,15], clip=true}}, y={"count()", axis={title="Time-difference counts"}})
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[214]:
[[file:./obipy-resources/tVVUQm.png]]
:END:

Lets checkout time differences between observations
#+BEGIN_SRC jupyter-julia
  tdiff = t |>
      @groupby(_.product_id) |>
      @map({product_id=_.key, delta=diff(_..date)}) |>
      @mapmany(_.delta, {delta=__, product_id=_.product_id}) |>
      @map({_.product_id, delta=log(Dates.value(_.delta))}) |>
      @vlplot(:line,
              x={:delta, axis={title="log(ΔTime)"}, bin={maxbins=100}},
              y={"count()", axis={title="Counts"}, scale={domain=[0,400], clip=true}},
              color=:product_id, height=300, width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[241]:
[[file:./obipy-resources/LF5PnB.png]]
:END:

Some considerable variability in sampling rate. Will need to resample.

#+BEGIN_SRC jupyter-julia
  tnew = t |>
      @groupby(_.product_id) |>
      @map({product_id=_.key, delta=diff(_..date)})

#+END_SRC



** test flux rnn

*** 0.7.1
:PROPERTIES:
:header-args: jupyter-julia :session marketbot-julia-ssh.json :results raw drawer :tangle mbot.jl :eval never-export :kernel julia-0.7 :async t
:END:

#+BEGIN_SRC jupyter-julia
  # uncomment to run on gpu, if available
  #using CuArrays

  using Flux
  using Flux: onehot, argmax, chunk, batchseq, throttle, crossentropy
  using StatsBase: wsample
  using Base.Iterators: partition

  cd(@__DIR__)
  addr = "https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt"
  savepath = "/home/ishai/git_repos/flux-model-zoo/text/char-rnn/"*"input.txt"
  isfile(savepath) || download(addr, savepath)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[224]:
: true
:END:

#+BEGIN_SRC jupyter-julia
  text = collect(readstring(savepath))
  alphabet = [unique(text)..., '_']
  text = map(ch -> onehot(ch, alphabet), text)
  stop = onehot('_', alphabet)

  N = length(alphabet)
  seqlen = 50
  nbatch = 32

  Xs = collect(partition(batchseq(chunk(text, nbatch), stop), seqlen));
  Ys = collect(partition(batchseq(chunk(text[2:end], nbatch), stop), seqlen));
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[229]:
:END:

#+BEGIN_SRC jupyter-julia
  m = Chain(
    LSTM(N, 128),
    LSTM(128, 128),
    Dense(128, N),
    softmax)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[239]:
: Chain(Recur(LSTMCell(68, 128)), Recur(LSTMCell(128, 128)), Dense(128, 68), NNlib.softmax)
:END:

#+BEGIN_SRC jupyter-julia
  m = gpu(m)

  function loss(xs, ys)
    l = sum(crossentropy.(m.(gpu.(xs)), gpu.(ys)))
    Flux.truncate!(m)
    return l
  end

  opt = ADAM(params(m), 0.01)
  tx, ty = (gpu.(Xs[5]), gpu.(Ys[5]))
  evalcb = () -> @show loss(tx, ty)

  Flux.train!(loss, zip(Xs, Ys), opt,
              cb = throttle(evalcb, 30))

  # Sampling
  m = cpu(m)

  function sample(m, alphabet, len; temp = 1)
    Flux.reset!(m)
    buf = IOBuffer()
    c = rand(alphabet)
    for i = 1:len
      write(buf, c)
      c = wsample(alphabet, m(onehot(c, alphabet)).data)
    end
    return String(take!(buf))
  end

  sample(m, alphabet, 1000) |> println

  # evalcb = function ()
  #   @show loss(Xs[5], Ys[5])
  #   println(sample(deepcopy(m), alphabet, 100))
  # end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[120]:
: 30
:END:



** test flux phonemes
:PROPERTIES:
:header-args: jupyter-julia :session marketbot-julia-ssh.json :results raw drawer :tangle mbot.jl :eval never-export :async t
:END:

*** 0.7.1

#+BEGIN_SRC jupyter-julia
  # Based on https://arxiv.org/abs/1409.0473

  using Flux: flip, crossentropy, reset!, throttle, glorot_uniform

  modeldir = "/home/ishai/git_repos/flux-model-zoo/text/phonemes/"
  include(joinpath(modeldir,"0-data.jl"));

  Nin = length(alphabet)
  Nh = 30 # size of hidden layer

  # A recurrent model which takes a token and returns a context-dependent
  # annotation.
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[49]:
: 30
:END:

#+BEGIN_SRC jupyter-julia
  forward  = LSTM(Nin, Nh÷2)
  backward = LSTM(Nin, Nh÷2)
  encode(tokens) = vcat.(forward.(tokens), flip(backward, tokens))

  # alignnet = Dense(2Nh, 1)
  # align(s, t) = alignnet(combine(t, s))

  # alignnet1 = Dense(Nh, 1)
  # alignnet2 = Dense(Nh, 1)
  # align(s, t) = alignnet1(s) .+ alignnet2(s)
  Ws = param(glorot_uniform(1, Nh))
  Wt = param(glorot_uniform(1, Nh))
  b = param(zeros(1))
  align(s,t) = Ws*s .+ Wt*t .+ b
    
  # A recurrent model which takes a sequence of annotations, attends, and returns
  # a predicted output token.

  recur   = LSTM(Nh+length(phones), Nh)
  toalpha = Dense(Nh, length(phones))

  function asoftmax(xs)
    xs = [exp.(x) for x in xs]
    s = sum(xs)
    return [x ./ s for x in xs]
  end

  function decode1(tokens, phone)
    weights = asoftmax([align(recur.state[2], t) for t in tokens])
    context = sum(map((a, b) -> a .* b, weights, tokens))
    y = recur(vcat(Int32.(phone), context))
    return softmax(toalpha(y))
  end

  decode(tokens, phones) = [decode1(tokens, phone) for phone in phones]

  # The full model

  state = (forward, backward, recur, toalpha) # Dense doesn't hold a state... right?

  function model(x, y)
    ŷ = decode(encode(x), y)
    reset!(state)
    return ŷ
  end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[50]:
: model (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  loss(x, yo, y) = sum(crossentropy.(model(x, yo), y))

  evalcb = () -> @show loss(data[500]...)
  opt = ADAM(params(state))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[51]:
: #43 (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  Flux.train!(loss, data, opt, cb = throttle(evalcb, 10))
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 6a3acdf0-0375-4ed8-b671-3b4aeb3d459e
:END:

#+BEGIN_SRC jupyter-julia
  # Prediction

  using StatsBase: wsample

  function predict(s)
    ts = encode(tokenise(s, alphabet))
    ps = Any[:start]
    for i = 1:50
      dist = decode1(ts, onehot(ps[end], phones))
      next = wsample(phones, dist.data)
      next == :end && break
      push!(ps, next)
    end
    return ps[2:end]
  end

  predict("PHYLOGENY")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[284]:
#+BEGIN_EXAMPLE
  32-element Array{Any,1}:
  :AH0
  :L
  :AH0
  :AA1
  :N
  :K
  :K
  :L
  :IH0
  :V
  :F
  :R
  :IY1
  ⋮
  :N
  :N
  :ER0
  :F
  :IH0
  :N
  :T
  :IH0
  :N
  :IH0
  :L
  :IY0
#+END_EXAMPLE
:END:


*** 0.6.4

#+BEGIN_SRC jupyter-julia
  # Based on https://arxiv.org/abs/1409.0473

  using Flux: flip, crossentropy, reset!, throttle, glorot_uniform

  modeldir = "/home/ishai/git_repos/flux-model-zoo/text/phonemes/"
  include(joinpath(modeldir,"0-data.jl"));

  Nin = length(alphabet)
  Nh = 30 # size of hidden layer

  # A recurrent model which takes a token and returns a context-dependent
  # annotation.

  forward  = LSTM(Nin, Nh÷2)
  backward = LSTM(Nin, Nh÷2)
  encode(tokens) = vcat.(forward.(tokens), flip(backward, tokens))

  # alignnet = Dense(2Nh, 1)
  # align(s, t) = alignnet(combine(t, s))

  # alignnet1 = Dense(Nh, 1)
  # alignnet2 = Dense(Nh, 1)
  # align(s, t) = alignnet1(s) .+ alignnet2(s)
  Ws = param(glorot_uniform(1, Nh))
  Wt = param(glorot_uniform(1, Nh))
  b = param(zeros(1))
  align(s,t) = Ws*s .+ Wt*t .+ b
    
  # A recurrent model which takes a sequence of annotations, attends, and returns
  # a predicted output token.

  recur   = LSTM(Nh+length(phones), Nh)
  toalpha = Dense(Nh, length(phones))

  function asoftmax(xs)
    xs = [exp.(x) for x in xs]
    s = sum(xs)
    return [x ./ s for x in xs]
  end

  function decode1(tokens, phone)
    weights = asoftmax([align(recur.state[2], t) for t in tokens])
    context = sum(map((a, b) -> a .* b, weights, tokens))
    y = recur(vcat(Int32.(phone), context))
    return softmax(toalpha(y))
  end

  decode(tokens, phones) = [decode1(tokens, phone) for phone in phones]

  # The full model

  state = (forward, backward, recur, toalpha) # Dense doesn't hold a state... right?

  function model(x, y)
    ŷ = decode(encode(x), y)
    reset!(state)
    return ŷ
  end

  loss(x, yo, y) = sum(crossentropy.(model(x, yo), y))

  evalcb = () -> @show loss(data[500]...)
  opt = ADAM(params(state))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[262]:
: (::#93) (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  Flux.train!(loss, data, opt, cb = throttle(evalcb, 10))
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 6a3acdf0-0375-4ed8-b671-3b4aeb3d459e
:END:

#+BEGIN_SRC jupyter-julia
  # Prediction

  using StatsBase: wsample

  function predict(s)
    ts = encode(tokenise(s, alphabet))
    ps = Any[:start]
    for i = 1:50
      dist = decode1(ts, onehot(ps[end], phones))
      next = wsample(phones, dist.data)
      next == :end && break
      push!(ps, next)
    end
    return ps[2:end]
  end

  predict("PHYLOGENY")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[284]:
#+BEGIN_EXAMPLE
  32-element Array{Any,1}:
  :AH0
  :L
  :AH0
  :AA1
  :N
  :K
  :K
  :L
  :IH0
  :V
  :F
  :R
  :IY1
  ⋮
  :N
  :N
  :ER0
  :F
  :IH0
  :N
  :T
  :IH0
  :N
  :IH0
  :L
  :IY0
#+END_EXAMPLE
:END:



** temp

:header-args: jupyter-julia :session marketbot-julia-ssh.json :results raw drawer :tangle mbot.jl :eval never-export :async t
:END:

*** 


*** 
#+BEGIN_SRC jupyter-julia
  recurstate = state_transfer(vcat(forward.state...))
#+END_SRC

#+BEGIN_SRC jupyter-julia
  recur.state = [recurstate[r,:] for r in ranges]
  preds = []
  hₜ = recur.state[1]
  # restack enc along time and unstack along batch inorder to multiply by alpha
  # probably better way to do this...
  enc′ = unstack(vcat((e->reshape(e, (1, size(e)...))).(enc)...), 3)
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 6b77ca48-019e-4ca5-9d9a-dd16116706fa
:END:

#+BEGIN_SRC jupyter-julia
  # for i in 1:len_seq_out
  function temp(hₜ)
      α = alignvec(enc, hₜ)
      # α .* enc, there is probably cleaner and faster way of doing this:
      c = cat(map(x->(x[1]' * x[2])'[:,:], zip(unstack(α, 2), enc′))..., dims=2)
      hₜ = recur(c)
      push!(preds, output_transfer(hₜ)) 
  end  
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[39]:
: temp (generic function with 2 methods)
:END:


*** 
#+BEGIN_SRC jupyter-julia
  accum(h, x) = (h+x, x)
  rnn = Flux.Recur(accum, 0)
  rnn(2) # 2
  rnn(3) # 3
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[276]:
: 3
:END:

#+BEGIN_SRC jupyter-julia
  rnn.state # 5
  rnn.(1:10) # apply to a sequence
  rnn.state # 60
#+END_SRC


* Feed
:PROPERTIES:
:header-args: ipython :session marketbot-ssh.json :results raw drawer :tangle zftestnn.py :eval never-export :async t
:END:

** test rest api

Lets look at all products available at coinbase
#+BEGIN_SRC ipython
import requests
ret = requests.get('https://api-public.sandbox.pro.coinbase.com/products')
products = json.loads(ret.text)
len(products)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[94]:
: 15
:END:

Example product
#+BEGIN_SRC ipython
products[0]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[96]:
#+BEGIN_EXAMPLE
  {'id': 'ETH-BTC',
  'base_currency': 'ETH',
  'quote_currency': 'BTC',
  'base_min_size': '0.01',
  'base_max_size': '1000000',
  'quote_increment': '0.00001',
  'display_name': 'ETH/BTC',
  'status': 'online',
  'margin_enabled': False,
  'status_message': None,
  'min_market_funds': None,
  'max_market_funds': None,
  'post_only': False,
  'limit_only': False,
  'cancel_only': False}
#+END_EXAMPLE
:END:

Produt ids
#+BEGIN_SRC ipython
pairs = [{'base':p['base_currency'], 'quote':p['quote_currency'], 'id':p['id']} for p in products]
[p['id'] for p in pairs]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[108]:
#+BEGIN_EXAMPLE
  ['ETH-BTC',
  'ETH-USD',
  'LTC-BTC',
  'LTC-USD',
  'ETH-EUR',
  'LTC-EUR',
  'BCH-USD',
  'BCH-BTC',
  'BCH-EUR',
  'BTC-USD',
  'BTC-GBP',
  'BTC-EUR',
  'ETC-USD',
  'ETC-EUR',
  'ETC-BTC']
#+END_EXAMPLE
:END:

#+BEGIN_SRC ipython
  uri = 'wss://ws-feed.pro.coinbase.com'
  subscribe_request = {
      'type': 'subscribe',
      'product_ids': ['BTC-USD'],
      'channels': [
          'matches',
          'heartbeat']
  }
#+END_SRC

#+BEGIN_SRC ipython
  from pprint import PrettyPrinter
  pprint = PrettyPrinter(indent=4).pprint
  from websocket import create_connection
  import json
  URI = 'wss://ws-feed.pro.coinbase.com'
  ws = create_connection(URI)
  SUBSCRIBE_REQUEST = {
      'type': 'subscribe',
      'product_ids': [p['id'] for p in pairs],
      'channels': [
          'matches',
          'heartbeat']
  }
  ws.send(json.dumps(SUBSCRIBE_REQUEST))
#+END_SRC

Sample output  
#+BEGIN_SRC ipython :results output
  pprint([json.loads(ws.recv()) for i in range(3)])
#+END_SRC

#+RESULTS:
:RESULTS:
[   {   'maker_order_id': '18b2c024-358a-4ec1-8d6a-020dfe878d97',
        'price': '51.66000000',
        'product_id': 'LTC-USD',
        'sequence': 2560366729,
        'side': 'sell',
        'size': '1.86851264',
        'taker_order_id': 'f36d7c09-ccd7-490c-b4cb-0c0704dcd715',
        'time': '2018-08-14T03:38:40.285000Z',
        'trade_id': 33028617,
        'type': 'last_match'},
    {   'maker_order_id': 'ce5fc695-f0f6-40cb-8279-42fea37a32ac',
        'price': '232.00000000',
        'product_id': 'ETH-EUR',
        'sequence': 983928315,
        'side': 'sell',
        'size': '6.18652428',
        'taker_order_id': '68adcfd3-00a3-44e1-89b0-f29ff33e3333',
        'time': '2018-08-14T03:38:23.275000Z',
        'trade_id': 4502984,
        'type': 'last_match'},
    {   'maker_order_id': '66bb907e-25b0-45c9-8637-1fd3aaf39a1d',
        'price': '45.35000000',
        'product_id': 'LTC-EUR',
        'sequence': 715075307,
        'side': 'sell',
        'size': '2.39867409',
        'taker_order_id': 'a9afab9e-e2a3-4395-ba34-ee30f71847dd',
        'time': '2018-08-14T03:37:18.769000Z',
        'trade_id': 3394856,
        'type': 'last_match'}]
:END:


**  test websocket (singular)

#+BEGIN_SRC ipython
  URI = 'wss://ws-feed.pro.coinbase.com'
  SUBSCRIBE_REQUEST = {
      'type': 'subscribe',
      'product_ids': ['BTC-USD'],
      'channels': [
          'matches',
          'heartbeat']
  }
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[54]:
:END:

#+BEGIN_SRC ipython
  from websocket import create_connection
  import json
  ws = create_connection(URI)
  ws.send(json.dumps(SUBSCRIBE_REQUEST))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[55]:
: 93
:END:

#+BEGIN_SRC ipython
  result =  ws.recv()
  print ("Received '%s'" % result)
  ws.close()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[56]:
:END:


** Test websockets
reference - 

#+BEGIN_SRC ipython
  import time
  import websockets
  import asyncio
  URI = 'wss://ws-feed.gdax.com'
  SUBSCRIBE_REQUEST = {
      'type': 'subscribe',
      'product_ids': ['BTC-USD'],
      'channels': [
          'matches',
          'heartbeat']
  }
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[26]:
:END:

#+BEGIN_SRC ipython
  import logging

  async def _listen(ws):
      while True:
          try:
              msg = await asyncio.wait_for(ws.recv(), timeout=20)
          except asyncio.TimeoutError:
              # No data in 20 seconds, check the connection.
              try:
                  pong_waiter = await ws.ping()
                  await asyncio.wait_for(pong_waiter, timeout=10)
              except asyncio.TimeoutError:
                  # No response to ping in 10 seconds, disconnect.
                  break
          else:
              handle_message(json.loads(msg))
      return

  async def _connect(path):
      async with websockets.connect(path) as ws:
          print('connecting')
          await ws.send(json.dumps(SUBSCRIBE_REQUEST))
          print('Connected')
          message = await ws.recv()
          print(' {} '.format(message))
          await _listen(ws)

  def match_counter():
      global NUM_MATCHES
      NUM_MATCHES += 1
      if NUM_MATCHES % 10 == 0:
          sys.stdout.write("\r matches entered: {}".format(NUM_MATCHES))
          sys.stdout.flush()

  def handle_message(msg):
      if msg['type'] == 'match':
          match_counter()
          logging.info(msg)
          with open(FILEPATH, 'a+') as f:
              writer = csv.writer(f)
              writer.writerow([msg[key] for key in COLUMNS])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[8]:
:END:

#+BEGIN_SRC ipython
  asyncio.get_event_loop().run_until_complete(_connect(URI))
  foo = asyncio.ensure_future(_connect(URI))  
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[35]:
:END:

#+BEGIN_SRC ipython
  SUBSCRIBE_REQUEST = {
      'type': 'subscribe',
      'product_ids': ['BTC-USD'],
      'channels': [
          'matches',
          'heartbeat']
  }
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[27]:
:END:

#+BEGIN_SRC ipython
  import json
  ws.send(json.dumps(SUBSCRIBE_REQUEST))
#+END_SRC

#+BEGIN_SRC ipython
from src.utils import collect
#+END_SRC
