-*- eval: (progn (pyvenv-workon "ml364") (auto-revert-mode 1))-*-
#+STARTUP: indent
#+OPTIONS: author:Ishai
#+TITLE: Market Bot
#+TODO: TODO IN-PROGRESS WAITING DONE ONGOING WORKING BROKEN
#+OPTIONS: toc:nil

* Tasks

** TODO Week of Jan 9th

*** Ira

- Determine which price information to add before dense.

- Normalize new data.

- determine validation startegy

- experiment with architecture changes (sequence length, etc.)


*** Ishai

- market gym

- help with stuff

- anything else?


** DONE Week of Jan 5th
DEADLINE: <2018-01-05 Fri 10:00>

*** Ishai

**** 


*** Ira

**** Preprocess / de-trend
:PROPERTIES:
:Effort:   4h
:END:

***** Goals

Strategy for preprocessing data that filters out 

****** Periodic components

- R stl library?


****** Trend

- idea: first difference

- other?


****** Bias / Variance (normalization)

- idea: center + scale based on training data


****** Other



***** Refs

- https://robjhyndman.com/papers/icdm2015.pdf
- stl library




**** Qunatization / loss function
:PROPERTIES:
:Effort:   8h
:END:

***** Goal

Should we be representing loss function as cross ent with logits? 

- According to [[https://github.com/ibab/tensorflow-wavenet/blob/master/wavenet/model.py#L665][wavenet]], yes.

- [[https://github.com/ibab/tensorflow-wavenet/blob/master/wavenet/ops.py#L64][wavenet mu-law]] for quantization

- mu law quantization is applied to both inputs and outputs.


* Notes

** general

- ML for discrete optimization

  - MF Balcan submodular optimization and ML

    - [[http://www.cs.cmu.edu/~ninamf/papers/learning-submodular-sicomp.pdf][paper]]

    - [[iesg.eecs.berkeley.edu/Colloquium/2015/MariaBalcan_EECS_Colloquium_20150429.mp4?_ga=2.235411201.1670012923.1524356790-1392980825.1513883180][video]]

  - [[http://www.cs.cmu.edu/~ninamf/papers/learn-branch.pdf][learning to branch]] (and bound)

  - [[https://scholar.harvard.edu/files/ericbalkanski/files/learning-to-optimize-combinatorial-functions.pdf][learning to optimize comb prob]]

- How to keep track of iterators?

- NN for discrete domains

  - [[https://www.youtube.com/watch?v=ixtEeS6aCKU&t=0s&list=PLkFD6_40KJIznC9CDbVTjAF2oyt8_VAe3&index=24][Aviv Tamar lecture]]

  - [[file:refs/VIN.pdf][VIN]]

  - [[https://arxiv.org/abs/1708.07280][Learning Generalized Reactive Policies using Deep Neural Networks]]

    - [[https://github.com/maxgold/generalized-gcn][max gold repo]]

- Intro to queryverse:

  - [[https://juliabox.com/notebook/notebooks/tutorials/intro-to-the-queryverse/AnthoffQueryverseJune2018.ipynb][notebook]]

  - [[https://www.youtube.com/watch?v=2oXSA2w-p28][youtube juliacon18]]

    
** Questions

- Ira

  - is new collect.py 


** Ideas

*** non uniform resampling

- upsample using nufft from [[https://github.com/MikaelSlevinsky/FastTransforms.jl][FastTransforms]]

- downsample using splines / etc


*** Output to postgres


** Next Up

*** Currency exchange rates

- start with static values



*** Model of conversion rates

- continue with generator, use approach like [[https://github.com/FluxML/model-zoo/blob/7fc4b76a0ca2df173a68393dd2b41181b9c66496/scene_category/train.jl][here]]

- Start with [[phonemes][seq2seq model]]

  - Try to get binning (digitization?) working

- predict conversion rate over some horizon

- use prediction for q value iteration


*** DONE resampling

- Starts with splines in [[https://github.com/JuliaMath/Interpolations.jl][Interpolatoins.jl]]

  - used dierckx instead

- concatenate interpolation into array with indecies signal, 'p' or 'v', value at timestamp

- Make sure to record frequency somewhere


** Quantization

- Look at how magenta nsynth (others) do it.


* Reading

** Future

- value iteration networks

- [[http://bair.berkeley.edu/blog/2017/07/18/learning-to-learn/][Model agnostic meta-learning blog]]

- [[https://arxiv.org/abs/1606.04474][Learning to learn by gradient descent by gradient descent]]

- [[https://arxiv.org/abs/1611.03824][Learning to Learn without Gradient Descent by Gradient Descent]]

  
* Aux
:PROPERTIES:
:header-args:  :exports none
:END:

** Jl Kernel

*** kernel remote - in lan

#+NAME: kernel_name
marketbot-julia

#+NAME: kernel_name_trim
#+BEGIN_SRC elisp :var kern=kernel_name :results value
  (s-trim kern)
#+END_SRC

#+RESULTS: kernel_name_trim
: marketbot-julia

#+BEGIN_SRC sh :session jlssh :results none :var kern=kernel_name_trim
  echo "tp2:/run/user/1000/jupyter/$kern.json"
#+END_SRC

#+BEGIN_SRC sh :session jlssh :results none :var kern=kernel_name_trim
  scp "tp2:/run/user/1000/jupyter/$kern.json" "/run/user/1000/jupyter/$kern.json" && jupyter console --existing $kern.json --ssh tp2
#+END_SRC


** Py Kernel

*** kernel remote - in lan

#+NAME: kernel_name
marketbot

#+NAME: kernel_name_trim
#+BEGIN_SRC elisp :var kern=kernel_name :results value
  (s-trim kern)
#+END_SRC

#+RESULTS: kernel_name_trim
: marketbot

#+BEGIN_SRC sh :session pyssh :results none :var kern=kernel_name_trim
  echo "tp2:/run/user/1000/jupyter/$kern.json"
#+END_SRC

#+BEGIN_SRC sh :session pyssh :results none :var kern=kernel_name_trim
  scp "tp2:/run/user/1000/jupyter/$kern.json" "/run/user/1000/jupyter/$kern.json"
#+END_SRC


#+BEGIN_SRC sh :session pyssh :results none :var kern=kernel_name_trim
  jupyter console --existing $kern.json --ssh tp2
#+END_SRC


*** ipython settings
:PROPERTIES:
:header-args: ipython :session marketbot-ssh.json :results raw drawer :tangle col.py :eval never-export
:END:

#+BEGIN_SRC ipython
  import matplotlib
  # matplotlib.use('Agg')
  %matplotlib inline
  %load_ext autoreload
  %autoreload 2
  # cd /home/ubuntu/Aisin/
  from matplotlib import pyplot as plt
  import pandas as pd
  pd.set_option("display.max_columns",301)
  import os
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[2]:
:END:

#+BEGIN_SRC ipython :async t
  1+2
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[3]:
: 3
:END:


*** other python things
:PROPERTIES:
:header-args: ipython :session mx2_mus_kernel-ssh.json :results raw drawer :tangle col.py :eval never-export
:END:

#+BEGIN_SRC ipython
def decorator_factory(key, value):
    def msg_decorator(f):
        def inner_dec(*args, **kwargs):
            g = f.__globals__  # use f.func_globals for py < 2.6
            sentinel = object()

            oldvalue = g.get(key, sentinel)
            g[key] = value

            try:
                res = f(*args, **kwargs)
            finally:
                if oldvalue is sentinel:
                    del g[key]
                else:
                    g[key] = oldvalue

            return res
        return inner_dec
    return msg_decorator

#+END_SRC

#+RESULTS:
: # Out[3]:




** slack log in

#+BEGIN_SRC elisp :results none
  (slack-register-team
   :name "bettingstrategies"
   :default t
   :client-id "ishaikones@gmail.com"
   :client-secret "barbarboots"
   :token "xoxs-151077066903-150475549493-300542321955-38e3d5a804"
   :subscribed-channels '(general))
#+END_SRC


* snippets

** Julia
:PROPERTIES:
:header-args: jupyter-julia :session marketbot-julia-ssh.json :results raw drawer :tangle mbot.jl :eval never-export :kernel julia-0.6
:END:

*** preprocess and explore

This part was done in Julia =v0.6.4=

#+BEGIN_SRC jupyter-julia
  using Queryverse, IndexedTables
#+END_SRC

#+BEGIN_SRC jupyter-julia :results output
  datadir = "/home/ishai/Documents/work/marketbot/src/utils/data/raw"
  run(`ls -lh $(datadir)`)
#+END_SRC

#+RESULTS:
:RESULTS:
total 21M
-rw-rw-r-- 1 ishai ishai 2.5M Aug 14 00:12 20180813-171152.csv
-rw-rw-r-- 1 ishai ishai 1.7M Aug 14 02:50 20180814-001303.csv
-rw-rw-r-- 1 ishai ishai 1.7M Aug 15 03:42 20180815-004411.csv
-rw-rw-r-- 1 ishai ishai 1.6M Aug 15 12:38 20180815-113032.csv
-rw-rw-r-- 1 ishai ishai 457K Aug 15 13:46 20180815-131857.csv
-rw-rw-r-- 1 ishai ishai  14K Aug 15 13:48 20180815-134729.csv
-rw-rw-r-- 1 ishai ishai 1.1M Aug 15 15:15 20180815-134950.csv
-rw-rw-r-- 1 ishai ishai  13M Aug 16 14:22 20180815-160006.csv
:END:

#+BEGIN_SRC jupyter-julia :results output
  fname = maximum(readdir(datadir))
  run(`head $(joinpath(datadir, fname))`)
#+END_SRC

#+RESULTS:
:RESULTS:
2018-08-15T20:00:07.341000Z,289.01000000,0.95307512,ETH-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.05064896,BTC-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.06453496,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.52758604,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.75478104,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00118000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.56368644,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.03650000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00810000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00380000,BTC-USD
:END:

# DateTime does not recognize "2018-08-14T04:13:06.697000Z" so need to adjust string before processing. 
# Haven't figured out how to correctly use [[https://juliacomputing.com/TextParse.jl/stable/#TextParse.CustomParser][CustomParser]], but it looks like a nice approach to converting.
#+BEGIN_SRC jupyter-julia :eval never :exports never
using TextParse
cDateTime = CustomParser(x->x[1:end-5], DateTime)
#+END_SRC

# annoying thigns so far - 1.how to set types, 2. not much documentation 
# Later - set types using TextParse
#+BEGIN_SRC jupyter-julia
  colnames = ["date", "price", "size", "product_id"]
  t = load(joinpath(datadir, fname),
           colnames=colnames,
           colparsers=Dict(:date=>String, :price=>Float64, :size=>Float64, :product_id=>String)) |>
               @map({:date=DateTime(_.date[1:end-5]), _.price, _.size, _.product_id})  |> # cast to DateTime
               table
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[5]:
#+BEGIN_EXAMPLE
Table with 623229 rows, 4 columns:
  date                    price    size        product_id
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  2018-08-15T20:00:07.87  6383.82  0.050649    "BTC-USD"
  2018-08-15T20:00:07.87  6383.82  0.064535    "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  1.52759     "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  1.75478     "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.00118     "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.563686    "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0365      "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0081      "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0038      "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.2         "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.00998106  "BTC-USD"
  2018-08-15T20:00:08.44  6383.81  0.0633251   "BTC-USD"
  â‹®
  2018-08-18T05:37:32.6   310.51   1.3413      "ETH-USD"
  2018-08-18T05:37:32.6   310.5    8.46415     "ETH-USD"
  2018-08-18T05:37:32.6   310.5    1.9         "ETH-USD"
  2018-08-18T05:37:32.93  310.51   0.0432801   "ETH-USD"
  2018-08-18T05:37:33.72  5709.16  0.00409268  "BTC-EUR"
  2018-08-18T05:37:34.41  14.22    137.769     "ETC-USD"
  2018-08-18T05:37:34.72  6517.65  0.0376063   "BTC-USD"
  2018-08-18T05:37:35.14  14.19    145.838     "ETC-USD"
  2018-08-18T05:37:36.13  5709.16  0.022136    "BTC-EUR"
  2018-08-18T05:37:36.62  14.22    3.35943     "ETC-USD"
  2018-08-18T05:37:37.01  59.88    0.7215      "LTC-USD"
#+END_EXAMPLE
:END:

Assign primary keys
#+BEGIN_SRC jupyter-julia
  t = table(t, pkey=(:product_id, :date))
#+END_SRC

Lets check if the data is sorted with in groups.
#+BEGIN_SRC jupyter-julia
  a = t |> @groupby(_.product_id, _.date) |> @map({origin=_.key, result=issorted(_)}) |> @map(all(_.result))
  all(a)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[38]:
: true
:END:

This opens a new window
#+BEGIN_SRC jupyter-julia :eval never
  t |> Voyager()
#+END_SRC

[[file:./voyager.png]]

The default view doesn't work for volume (size), lets try line instead
#+BEGIN_SRC jupyter-julia
  @time t |> @vlplot(:line, x=:date, y=:size, color="product_id")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[16]:
[[file:./obipy-resources/TMY56X.png]]
:END:

Not that nice. 

# log Time difference counts
#+BEGIN_SRC jupyter-julia :exports none :eval never exports none
  tdiff = t |> @filter(_.product_id=="ETH-USD") |> @Queryverse.map(_.date) |> collect |> diff |> @map({delta=_});
  tdiff |> @map({delta=log(Dates.value(_.delta))}) |> @vlplot(mark={:bar, clip=true}, x={:delta, bin={maxbins=1000}, scale={domain=[0,15], clip=true}}, y={"count()", axis={title="Time-difference counts"}})
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[214]:
[[file:./obipy-resources/tVVUQm.png]]
:END:

Plot time differences between observations
#+BEGIN_SRC jupyter-julia
  t |>
      @groupby(_.product_id) |>
      @map({product_id=_.key, delta=diff(_..date)}) |>
      @mapmany(_.delta, {delta=Dates.value(__), product_id=_.product_id}) |> # __ is the individual element in row
      @filter(_.delta>0) |>
      @vlplot(mark={:line, clip=true},
              x={:delta, axis={title="Î”Time"}, bin={maxbins=100000}, scale={domain=[0,10000], clip=true}},
              y={"count()", axis={title="Counts"}  #, scale={domain=[0,100], clip=true}
                 },
              color=:product_id,
              height=300,
              width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[136]:
[[file:./obipy-resources/bvknN0.png]]
:END:

/log/ time differences (between observations)
#+BEGIN_SRC jupyter-julia
  t |>
      @groupby(_.product_id) |>
      @map({product_id=_.key, delta=diff(_..date)}) |>
      @mapmany(_.delta, {delta=__, product_id=_.product_id}) |> # __ is the individual element in row
      @map({_.product_id, delta=log(Dates.value(_.delta))}) |>
      @vlplot(mark={:line, clip=true},
              x={:delta, axis={title="log(Î”Time)"}, bin={maxbins=100}},
              y={"count()", axis={title="Counts"}, scale={domain=[0,2500], clip=true}},
              color=:product_id, height=300, width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[34]:
[[file:./obipy-resources/IT9gre.png]]
:END:

Irregular signal. Resample to have observations at regular intervals.

Review duplicates:

#+BEGIN_SRC jupyter-julia
  t |> @groupby((_.product_id, _.date)) |> @map({origin=_.key, Count=length(_)}) |> @filter(_.Count > 1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[41]:
#+BEGIN_EXAMPLE
  ?x2 query result
  origin                              â”‚ Count
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€
  ("BTC-USD", 2018-08-15T20:00:07.87) â”‚ 2
  ("BTC-USD", 2018-08-15T20:00:08.44) â”‚ 12
  ("BTC-USD", 2018-08-15T20:00:08.46) â”‚ 2
  ("BTC-USD", 2018-08-15T20:00:11.05) â”‚ 6
  ("ETH-USD", 2018-08-15T20:00:11.2)  â”‚ 2
  ("BTC-USD", 2018-08-15T20:00:12.16) â”‚ 4
  ("BTC-USD", 2018-08-15T20:00:12.93) â”‚ 11
  ("BTC-USD", 2018-08-15T20:00:13.02) â”‚ 2
  ("BTC-USD", 2018-08-15T20:00:14.43) â”‚ 4
  ("ETH-EUR", 2018-08-15T20:00:17.29) â”‚ 5
  ... with more rows
#+END_EXAMPLE
:END:

These are due to transactions that are cleared simultaneously.

#+BEGIN_SRC jupyter-julia
t = t |> @groupby((_.product_id, _.date)) |> @map({product_id=_.key[1], date=_.key[2], price=median(_..price),  size=sum(_..size)}) |> table;
t |> @groupby((_.product_id, _.date)) |> @map({origin=_.key, Count=length(_)}) |> @filter(_.Count > 1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[10]:
#+BEGIN_EXAMPLE
  0x2 query result
  origin â”‚ Count
  â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  t |> @vlplot(mark={:line, clip=true},
               x={:date},
               y={:price},
               color=:product_id, height=300, width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[94]:
[[file:./obipy-resources/0XjhGy.png]]
:END:

Median time difference:
#+BEGIN_SRC jupyter-julia :async t
  meddiff = t |> @groupby(_.product_id, _.date) |> @map({_.key, median(diff(Dates.values(Float64.(_))))}) |> DataFrame
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[151]:
#+BEGIN_EXAMPLE
  15Ã—2 DataFrames.DataFrame
  â”‚ Row â”‚ key     â”‚ _2_     â”‚
  â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ 1   â”‚ BTC-USD â”‚ 1190.0  â”‚
  â”‚ 2   â”‚ ETC-USD â”‚ 890.0   â”‚
  â”‚ 3   â”‚ ETH-USD â”‚ 1780.0  â”‚
  â”‚ 4   â”‚ ETH-EUR â”‚ 8850.0  â”‚
  â”‚ 5   â”‚ BCH-USD â”‚ 6360.0  â”‚
  â”‚ 6   â”‚ LTC-BTC â”‚ 24825.0 â”‚
  â”‚ 7   â”‚ BTC-EUR â”‚ 5910.0  â”‚
  â”‚ 8   â”‚ LTC-USD â”‚ 3090.0  â”‚
  â”‚ 9   â”‚ BTC-GBP â”‚ 15390.0 â”‚
  â”‚ 10  â”‚ LTC-EUR â”‚ 18665.0 â”‚
  â”‚ 11  â”‚ ETH-BTC â”‚ 3595.0  â”‚
  â”‚ 12  â”‚ ETC-EUR â”‚ 29360.0 â”‚
  â”‚ 13  â”‚ BCH-EUR â”‚ 38610.0 â”‚
  â”‚ 14  â”‚ ETC-BTC â”‚ 3990.0  â”‚
  â”‚ 15  â”‚ BCH-BTC â”‚ 16330.0 â”‚
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia
  t = setcol(t, :datevalue, Queryverse.map(r->Float64(Dates.value(r.date)), t))
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 4b0016a8-b154-49c7-89b4-5e7fb307af59
:END:

#+BEGIN_SRC jupyter-julia
  t |> @groupby(_.product_id)
#+END_SRC

Lowest median time difference between samples is for BCH-EUR at around 38 seconds between observations. Still, try using .5 second sampling frequency to take advantage of higher frequency signals.

#+BEGIN_SRC jupyter-julia :eval never :export none
  df = t |> DataFrame;
  df[:datevalue] = Float64.(Dates.values(df[:date]));
#+END_SRC

#+BEGIN_SRC jupyter-julia :eval never :export none
  tempdf = filter(r->r[:product_id] == "BTC-USD", df)
#+END_SRC

Start and end of interval
#+BEGIN_SRC jupyter-julia
  tstart = minimum(select(t, :datevalue))
  tend = maximum(select(t, :datevalue))
  sample_points = tstart:500:tend
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[18]:
: 6.367004640787e13:500.0:6.367025385687e13
:END:

#+BEGIN_SRC jupyter-julia :eval never :export none
  using Dierckx
  date_interp(x, y) = Spline1D(x,y; k=2, bc="nearest", s=2356)(sample_points)
  date_interp_t(_t) = table(@NT(datevalue=sample_points, price=date_interp(select(_t, :datevalue), select(_t, :price))))
  # date_interp_t(_t) = date_interp(_t[:datevalue], _t[:price])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[77]:
: date_interp_t (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  using Interpolations
  date_interp(x, y) = LinearInterpolation(x,y, extrapolation_bc=Interpolations.Linear())(sample_points)
  date_interp_t(_t) = table(@NT(datevalue=sample_points, price=date_interp(select(table(_t), :datevalue), select(table(_t), :price))))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[19]:
: date_interp_t (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  using StatPlots
  temp = t |> @filter(_.product_id =="BCH-BTC") |> table |> date_interp_t;
  temp2 = t |> @filter(_.product_id =="BCH-BTC") |> table;
  @df temp plot(:datevalue, :price)
  @df temp2 scatter!(:datevalue, :price, markersize=1)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[90]:
[[file:./obipy-resources/a4NkoU.svg]]
:END:

# There is definitely a better way to do this...
#+BEGIN_SRC jupyter-julia :eval never :export none
  dfs = []
  groups = DataFrames.groupby(df, :product_id)
  for _df in groups
      _df2 = DataFrame(price=date_interp(_df[:datevalue], _df[:price]),
                       size=date_interp(_df[:datevalue], _df[:size]),
                       datevalue=sample_points)
                       _df2[:product_id] = _df[:product_id][1]
      push!(dfs, _df2)
  end
#+END_SRC

Apply function across groups and unstack.
#+BEGIN_SRC jupyter-julia
  tnew = IndexedTables.groupby(date_interp_t, t, :product_id, flatten=true)
  tnew = IndexedTables.unstack(table(tnew), :datevalue; variable=:product_id, value=:price)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[21]:
#+BEGIN_EXAMPLE
  Table with 414899 rows, 16 columns:
  Columns:
  [1m#   [22m[1mcolname    [22m[1mtype[22m
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1   datevalue  Float64
  2   BCH-BTC    DataValues.DataValue{Float64}
  3   BCH-EUR    DataValues.DataValue{Float64}
  4   BCH-USD    DataValues.DataValue{Float64}
  5   BTC-EUR    DataValues.DataValue{Float64}
  6   BTC-GBP    DataValues.DataValue{Float64}
  7   BTC-USD    DataValues.DataValue{Float64}
  8   ETC-BTC    DataValues.DataValue{Float64}
  9   ETC-EUR    DataValues.DataValue{Float64}
  10  ETC-USD    DataValues.DataValue{Float64}
  11  ETH-BTC    DataValues.DataValue{Float64}
  12  ETH-EUR    DataValues.DataValue{Float64}
  13  ETH-USD    DataValues.DataValue{Float64}
  14  LTC-BTC    DataValues.DataValue{Float64}
  15  LTC-EUR    DataValues.DataValue{Float64}
  16  LTC-USD    DataValues.DataValue{Float64}
#+END_EXAMPLE
:END:

#+BEGIN_SRC jupyter-julia :async t
  tnew[end-100:end] |> DataFrame |> tail
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[159]:
#+BEGIN_EXAMPLE
  6Ã—16 DataFrames.DataFrame. Omitted printing of 9 columns
  â”‚ Row â”‚ datevalue  â”‚ BCH-BTC â”‚ BCH-EUR â”‚ BCH-USD â”‚ BTC-EUR â”‚ BTC-GBP â”‚ BTC-USD â”‚
  â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ 1   â”‚ 6.36703e13 â”‚ 0.09069 â”‚ 520.15  â”‚ 589.46  â”‚ 5709.16 â”‚ 5251.41 â”‚ 6517.65 â”‚
  â”‚ 2   â”‚ 6.36703e13 â”‚ 0.09069 â”‚ 520.15  â”‚ 589.46  â”‚ 5709.16 â”‚ 5252.02 â”‚ 6517.65 â”‚
  â”‚ 3   â”‚ 6.36703e13 â”‚ 0.09069 â”‚ 520.15  â”‚ 589.46  â”‚ 5709.16 â”‚ 5252.63 â”‚ 6517.65 â”‚
  â”‚ 4   â”‚ 6.36703e13 â”‚ 0.09069 â”‚ 520.15  â”‚ 589.46  â”‚ 5709.16 â”‚ 5253.23 â”‚ 6517.65 â”‚
  â”‚ 5   â”‚ 6.36703e13 â”‚ 0.09069 â”‚ 520.15  â”‚ 589.46  â”‚ 5709.16 â”‚ 5253.84 â”‚ 6517.65 â”‚
  â”‚ 6   â”‚ 6.36703e13 â”‚ 0.09069 â”‚ 520.15  â”‚ 589.46  â”‚ 5709.16 â”‚ 5254.44 â”‚ 6517.65 â”‚
#+END_EXAMPLE
:END:

Messy way to get sum of nulls
#+BEGIN_SRC jupyter-julia
  tnew |> @map(isnan.(_)) |> table |> columns |> @map(sum(_)) |> table
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[364]:
#+BEGIN_EXAMPLE
  Table with 16 rows, 1 columns:
  _1_
  â”€â”€â”€
  0
  0
  127
  0
  0
  0
  1
  0
  0
  0
  0
  0
  0
  23
  78
  38
#+END_EXAMPLE
:END:

Replace missing + mandatory column wise vs row wise comparison:

#+BEGIN_SRC jupyter-julia :async t :results output
  using Query
  dfnew = tnew |> DataFrame
  @time for i in size(dfnew)[1]-1:-1:1
      for c in 1:size(dfnew)[2]
          if isnan(dfnew[i,c])
              dfnew[i,c] = dfnew[i+1,c]
          end
      end
  end  
#+END_SRC

#+RESULTS:
:RESULTS:
  2.476500 seconds (16.17 M allocations: 367.061 MiB, 52.51% gc time)
:END:

#+BEGIN_SRC jupyter-julia :async t :results output
  using Query
  dfnew = tnew |> DataFrame
  @time for c in 1:size(dfnew)[2]
      for i in size(dfnew)[1]-1:-1:1
          if isnan(dfnew[i,c])
              dfnew[i,c] = dfnew[i+1,c]
          end
      end
  end  
#+END_SRC

#+RESULTS:
:RESULTS:
  1.876617 seconds (26.94 M allocations: 512.453 MiB, 23.32% gc time)
:END:

No more nulls
#+BEGIN_SRC jupyter-julia
  dfnew |> table |> @map(isnan.(_)) |> table |> columns |> @map(sum(_)) |> maximum
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[24]:
: 0
:END:

Save to feather
#+BEGIN_SRC jupyter-julia :eval never
  t2 = dfnew |> table |> @tee(save("postproc.feather"))
#+END_SRC



*** Batch generator

This part is done in =v0.7.1= inorder to future proof use of Iterators.

Load feather
#+BEGIN_SRC jupyter-julia :results none
  using IndexedTables, Feather
  df = Feather.read("postproc.feather") |> DataFrame
#+END_SRC

Batch generator
#+BEGIN_SRC jupyter-julia :results none
  len_seq_in = 50
  len_seq_out = 50
  nbatch = 32
  end_index = size(df)[1] - (len_seq_in + len_seq_in)
  start_indices = 1:end_index
#+END_SRC

#+BEGIN_SRC jupyter-julia :results none
  using Random: shuffle
#+END_SRC

#+BEGIN_SRC jupyter-julia
  function get_generator()
      ix_batches = Iterators.partition(shuffle(start_indices), nbatch)
      # single entry in an input / output batch:
      get_input(ix) = reshape(convert(Array, df[ix:ix+(len_seq_in-1),:]),
                              (1, len_seq_in, size(df)[2]))                  
      get_output(ix) = reshape(convert(Array, df[ix+(len_seq_in):ix+(len_seq_in+len_seq_out-1),:]),
                                     (1, len_seq_out, size(df)[2]))
      get_batch(b) = [vcat(get_input_batch.(b)...), vcat(get_output_batch.(b)...)]
      Base.Generator(get_batch, ix_batches)
   end
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[255]:
: get_generator (generic function with 1 method)
:END:


*** Model

#+BEGIN_SRC jupyter-julia
  evalcb = () -> @show loss(data[500]...)
  opt = ADAM(params(state))
#+END_SRC

#+RESULTS:
:RESULTS:
0 - c2effb12-4926-44f9-aa38-3c3bd23cd7dd
:END:

#+BEGIN_SRC jupyter-julia :eval never 
@epochs 5 begin
    tic()
    train = get_generator()
    for i in train
        l = loss(i[1], i[2])
        println("The loss for current minibatch is $l")
        Flux.back!(l)
        opt()
    end
    toc()
end
#+END_SRC

#+BEGIN_SRC jupyter-julia :eval never 
  Flux.train!(loss, data, opt, cb = throttle(evalcb, 10))
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 6a3acdf0-0375-4ed8-b671-3b4aeb3d459e
:END:

#+BEGIN_SRC jupyter-julia
  # Prediction

  using StatsBase: wsample

  function predict(s)
    ts = encode(tokenise(s, alphabet))
    ps = Any[:start]
    for i = 1:50
      dist = decode1(ts, onehot(ps[end], phones))
      next = wsample(phones, dist.data)
      next == :end && break
      push!(ps, next)
    end
    return ps[2:end]
  end

  predict("PHYLOGENY")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[284]:
#+BEGIN_EXAMPLE
  32-element Array{Any,1}:
  :AH0
  :L
  :AH0
  :AA1
  :N
  :K
  :K
  :L
  :IH0
  :V
  :F
  :R
  :IY1
  â‹®
  :N
  :N
  :ER0
  :F
  :IH0
  :N
  :T
  :IH0
  :N
  :IH0
  :L
  :IY0
#+END_EXAMPLE
:END:


** BROKEN Julia original - With JuliaDB
:PROPERTIES:
:header-args: jupyter-julia :session marketbot-julia-ssh.json :results raw drawer :tangle mbot.jl :eval never-export :async t
:END:

#+BEGIN_SRC jupyter-julia
  using Queryverse, JuliaDB
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[3]:
:END:

#+BEGIN_SRC jupyter-julia :results output
  datadir = "/home/ishai/Documents/work/marketbot/src/utils/data/raw"
  run(`ls -lh $(datadir)`)
#+END_SRC

#+RESULTS:
:RESULTS:
total 21M
-rw-rw-r-- 1 ishai ishai 2.5M Aug 14 00:12 20180813-171152.csv
-rw-rw-r-- 1 ishai ishai 1.7M Aug 14 02:50 20180814-001303.csv
-rw-rw-r-- 1 ishai ishai 1.7M Aug 15 03:42 20180815-004411.csv
-rw-rw-r-- 1 ishai ishai 1.6M Aug 15 12:38 20180815-113032.csv
-rw-rw-r-- 1 ishai ishai 457K Aug 15 13:46 20180815-131857.csv
-rw-rw-r-- 1 ishai ishai  14K Aug 15 13:48 20180815-134729.csv
-rw-rw-r-- 1 ishai ishai 1.1M Aug 15 15:15 20180815-134950.csv
-rw-rw-r-- 1 ishai ishai  12M Aug 16 13:46 20180815-160006.csv
:END:

#+BEGIN_SRC jupyter-julia :results output
  fname = maximum(readdir(datadir))
  run(`head $(joinpath(datadir, fname))`)
#+END_SRC

#+RESULTS:
:RESULTS:
2018-08-15T20:00:07.341000Z,289.01000000,0.95307512,ETH-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.05064896,BTC-USD
2018-08-15T20:00:07.878000Z,6383.82000000,0.06453496,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.52758604,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,1.75478104,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00118000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.56368644,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.03650000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00810000,BTC-USD
2018-08-15T20:00:08.441000Z,6383.81000000,0.00380000,BTC-USD
:END:

# DateTime does not recognize "2018-08-14T04:13:06.697000Z" so need to adjust string before processing. 
# Haven't figured out how to correctly use [[https://juliacomputing.com/TextParse.jl/stable/#TextParse.CustomParser][CustomParser]], but it looks like a nice approach to converting.
#+BEGIN_SRC jupyter-julia :eval never :exports never
using TextParse
cDateTime = CustomParser(x->x[1:end-5], DateTime)
#+END_SRC

# annoying thigns so far - 1.how to set types, 2. not much documentation 
# Later - set types using TextParse
#+BEGIN_SRC jupyter-julia
  colnames = ["date", "price", "size", "product_id"]
  t = loadtable(joinpath(datadir, fname), colnames=colnames, colparsers=Dict(:date=>String, :price=>Float64, :size=>Float64, :product_id=>String))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[18]:
#+BEGIN_EXAMPLE
  Distributed Table with 203134 rows in 1 chunks:
  date                           price    size        product_id
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  "2018-08-15T20:00:07.878000Z"  6383.82  0.050649    "BTC-USD"
  "2018-08-15T20:00:07.878000Z"  6383.82  0.064535    "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  1.52759     "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  1.75478     "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.00118     "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.563686    "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.0365      "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.0081      "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.0038      "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.2         "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.00998106  "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.0633251   "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.0158401   "BTC-USD"
  "2018-08-15T20:00:08.441000Z"  6383.81  0.23702     "BTC-USD"
  "2018-08-15T20:00:08.469000Z"  6383.81  0.00049985  "BTC-USD"
  "2018-08-15T20:00:08.469000Z"  6383.35  0.00050015  "BTC-USD"
  "2018-08-15T20:00:08.481000Z"  6383.35  0.00049985  "BTC-USD"
  "2018-08-15T20:00:08.562000Z"  6382.54  0.00810663  "BTC-USD"
  "2018-08-15T20:00:08.614000Z"  6382.53  0.00105     "BTC-USD"
  "2018-08-15T20:00:09.785000Z"  12.11    72.8834     "ETC-USD"
  "2018-08-15T20:00:10.486000Z"  289.01   2.1008      "ETH-USD"
  "2018-08-15T20:00:11.054000Z"  6376.89  0.0148466   "BTC-USD"
  "2018-08-15T20:00:11.054000Z"  6375.61  0.01        "BTC-USD"
  "2018-08-15T20:00:11.054000Z"  6375.0   0.007843    "BTC-USD"
  â‹®
#+END_EXAMPLE
:END:

Convert date column to datetime
#+BEGIN_SRC jupyter-julia
  t = setcol(t, :date, :date => x->DateTime(x[1:end-5]))
  t[1:10]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[9]:
#+BEGIN_EXAMPLE
  Table with 10 rows, 4 columns:
  date                    price    size       product_id
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  2018-08-14T04:13:04.88  269.3    1.0        "ETH-USD"
  2018-08-14T04:13:05.09  6009.43  0.0193853  "BTC-USD"
  2018-08-14T04:13:05.16  269.3    0.5        "ETH-USD"
  2018-08-14T04:13:05.17  6009.43  0.0159219  "BTC-USD"
  2018-08-14T04:13:06.54  6009.43  0.02295    "BTC-USD"
  2018-08-14T04:13:06.68  237.04   1.00002    "ETH-EUR"
  2018-08-14T04:13:06.69  52.48    0.5        "LTC-USD"
  2018-08-14T04:13:06.69  52.48    0.79       "LTC-USD"
  2018-08-14T04:13:06.69  52.48    13.71      "LTC-USD"
  2018-08-14T04:13:07     52.49    13.0       "LTC-USD"
#+END_EXAMPLE
:END:

This doesn't work here
#+BEGIN_SRC jupyter-julia :eval never
  t[1:100] |> Voyager()
#+END_SRC

#+RESULTS:
:RESULTS:
0 - e258681d-4db9-4b84-8d4a-10dc58b547c0
:END:

#+BEGIN_SRC jupyter-julia
  @time t |> @vlplot(:point, x=:date, y=:price, color="product_id")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[13]:
[[file:./obipy-resources/veUMiI.png]]
:END:

Lets look at transaction sizes
#+BEGIN_SRC jupyter-julia
  @time t |> @vlplot(:point, x=:date, y=:size, color="product_id")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[14]:
[[file:./obipy-resources/XvWwfT.png]]
:END:

Not that nice. 

Lets check if the data is sorted with in groups.
#+BEGIN_SRC jupyter-julia
  all(select(JuliaDB.groupby(issorted, t, :product_id; select = :date), :issorted))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[15]:
: true
:END:

#+BEGIN_SRC jupyter-julia
  @time t |> @vlplot(:line, x=:date, y=:size, color="product_id")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[16]:
[[file:./obipy-resources/TMY56X.png]]
:END:

#+BEGIN_SRC jupyter-julia :exports none :eval never exports none
  tdiff = t |> @filter(_.product_id=="ETH-USD") |> @Queryverse.map(_.date) |> collect |> diff |> @map({delta=_});
  tdiff |> @map({delta=log(Dates.value(_.delta))}) |> @vlplot(mark={:bar, clip=true}, x={:delta, bin={maxbins=1000}, scale={domain=[0,15], clip=true}}, y={"count()", axis={title="Time-difference counts"}})
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[214]:
[[file:./obipy-resources/tVVUQm.png]]
:END:

Lets checkout time differences between observations
#+BEGIN_SRC jupyter-julia
  tdiff = t |>
      @groupby(_.product_id) |>
      @map({product_id=_.key, delta=diff(_..date)}) |>
      @mapmany(_.delta, {delta=__, product_id=_.product_id}) |>
      @map({_.product_id, delta=log(Dates.value(_.delta))}) |>
      @vlplot(:line,
              x={:delta, axis={title="log(Î”Time)"}, bin={maxbins=100}},
              y={"count()", axis={title="Counts"}, scale={domain=[0,400], clip=true}},
              color=:product_id, height=300, width=300)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[241]:
[[file:./obipy-resources/LF5PnB.png]]
:END:

Some considerable variability in sampling rate. Will need to resample.

#+BEGIN_SRC jupyter-julia
  tnew = t |>
      @groupby(_.product_id) |>
      @map({product_id=_.key, delta=diff(_..date)})

#+END_SRC


** Python
:PROPERTIES:
:header-args: ipython :session marketbot-ssh.json :results raw drawer :tangle mbot.py :eval never-export :async t
:END:

*** does something

#+BEGIN_SRC ipython
  cd ~/Documents/work/marketbot
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[5]:
:END:

#+BEGIN_SRC ipython
  import tensorflow as tf
  tf.reset_default_graph()
  lstm = locate('src.models.lstm2')
  # lstm2.__main__()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  lstm.main()
#+END_SRC

#+BEGIN_SRC ipython
  predict_input_fn = lstm._input_fn_wrapper('data/clean/data.csv', lstm.ModeKeys.PREDICT, 10, lstm.DEFAULT_TRAIN_PARAMS)
  rnn = lstm.estimator()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  rnn.predict(input_fn=predict_input_fn)
#+END_SRC



*** namespace

#+BEGIN_SRC ipython
  import tensorflow as tf
  tf.reset_default_graph()
  sess = tf.InteractiveSession()

  with tf.variable_scope('a'):
      with tf.variable_scope('b'):
          v = tf.get_variable('v', initializer=tf.ones(3))
  with tf.variable_scope('', reuse=True):
      v2 = tf.get_variable('a/b/v')
  init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
  sess.run(init)  
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython :exports both
  V1 = tf.random_normal((10,3))
  V2 = tf.get_variable('abc', initializer=V1)
  with tf.variable_scope('', reuse=True):
    abc = tf.get_variable('abc', (10,3))
    abc.assign(tf.ones((10,3)))
  init = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())
  sess.run(init)
  abc.eval() - V2.eval()
#+END_SRC

#+RESULTS:
:RESULTS:
#+BEGIN_EXAMPLE
  array([[0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.],
  [0., 0., 0.]], dtype=float32)
#+END_EXAMPLE
:END:


*** trying out eager

#+BEGIN_SRC ipython
  import tensorflow as tf
  tfe = tf.contrib.eager
  tfe.enable_eager_execution()
#+END_SRC

#+BEGIN_SRC ipython
  import numpy as np
  A = np.random.randn(5,3)
  mu, std = tf.nn.normalize_moments(A.shape[0], *tf.nn.moments(tf.cast(A, dtype=tf.float32), axes=0), None)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:



*** Run ira_rnn

#+BEGIN_SRC ipython
  %cd marketbot
#+END_SRC

#+RESULTS:
:RESULTS:
:END:
  
#+BEGIN_SRC ipython
  from importlib import reload
  import src.models.lstm as lstm
  reload(lstm)
  lstm.__main__()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  lstm = locate('src.models.lstm')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


*** import_fn

#+BEGIN_SRC ipython
  from pydoc import locate
  import tensorflow as tf
  import numpy as np
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  path = 'marketbot/data/clean/data3.csv'
  data = np.genfromtxt(path, delimiter=',')[2:, 1:]
  data.shape
  #+END_SRC

  #+RESULTS:
  :RESULTS:
  : (85985, 3)
  :END:
  
  #+BEGIN_SRC ipython
  data = tf.convert_to_tensor(data)
  horizon = 10
  window = 100
  targets = data[horizon + window - 1 :, 1] / data[window - 1 : -horizon, 1] - 1
  features = tf.contrib.signal.frame(data, window, 1, axis=0)[:-horizon, :]
  tf.data.Dataset.from_tensor_slices({'train': features, 'repsonse':targets})
#+END_SRC




*** setup

**** Load

#+BEGIN_SRC ipython
  import pandas as pd
  import os 
  from datetime import datetime as dt
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

***** ak preprocess

#+BEGIN_SRC ipython
  from marketbot.src import dataloader
  agg_df = dataloader.get_data('marketbot/data/data2.csv')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

****** temp
:PROPERTIES:
:header-args: :eval never
:END:

#+BEGIN_SRC ipython
path='marketbot/data/data2.csv'; interval_length=1; window_length=100; stride_length=1; predict_length=10
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  df = pd.read_csv(path, index_col='sequence')
  df['time'] = pd.to_datetime(df['time'])
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  def mean_price(g):
      """ computes size-weighted price of trades """
      vol = g['volume'].sum()
      price = (g['volume'] * g['price']).sum() / (vol + 1e-8)
      return pd.Series([vol, price], ['volume', 'price'])
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


#+BEGIN_SRC ipython
  def discretize(df, interval_length):
      """ interval_length = number of seconds to aggregate in """
      if interval_length == 0:
          grouped = df.groupby('time')
      else:
          grouped = df.resample('{}S'.format(interval_length), on='time')
      return grouped.apply(mean_price).reset_index()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


#+BEGIN_SRC ipython
  agg_df = discretize(df, interval_length)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  agg_df['price2'] = np.nan
  agg_df['volume2'] = np.nan
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

whereever price == 0 we have a missing observation
#+BEGIN_SRC ipython
  agg_df.loc[(agg_df.price != 0), "price2"] = agg_df[(agg_df.price != 0)].price
  agg_df.loc[(agg_df.price != 0), "volume2"] = agg_df[(agg_df.price != 0)].volume
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  agg_df.volume2 = agg_df.volume2.fillna(method='ffill')
  agg_df.price2 = agg_df.price2.fillna(method='ffill')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  agg_df['change2'] = agg_df.price2.pct_change()
  agg_df['outcomes2'] = agg_df.price2.pct_change(periods= predict_length).shift(-predict_length)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:



#+BEGIN_SRC ipython
  agg_df.volume = agg_df.volume.fillna(0.0)
  agg_df.price = agg_df.price.fillna(method='ffill')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  agg_df['change'] = agg_df.price.pct_change()
  agg_df['outcomes'] = agg_df.price.pct_change(periods= predict_length).shift(-predict_length)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


***** original preprocesss
:PROPERTIES:
:header-args: :eval never
:END:

#+BEGIN_SRC ipython
  df = pd.read_csv('marketbot/data/data2.csv')
  df['time'] = pd.to_datetime(df.time)
  df['timestamp'] = df.time.apply(dt.timestamp)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  from matplotlib import pyplot as plt
  import numpy as np
  data = df.groupby('timestamp').apply(lambda g: g[['price', 'size']].mean(axis=0))
  assert np.diff(data.index).min() > 0  # data is sorted
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


**** model config

#+BEGIN_SRC ipython
  os.chdir('marketbot/src')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  from collections import namedtuple
  flagdct = {'batch_size': 64,
             'data_dir': '/tmp/dat/',
             'hidden_dim': 200,
             'l1reg_coeff': 1e-10,
             'l2reg_coeff': 1e-9,
             # 'l1reg_coeff': 1,
             # 'l2reg_coeff': 1,
             'latent_dim': 160,
             'logdir': '/tmp/log/',
             'n_epochs': 100000,
             'n_iterations': 100000,
             'n_samples_predict': 20,
             'n_samples_train': 10,
             'print_every': 1000, 
             'huber_loss_delta': .1,
             'use_update_ops': False}  # update_ops control dependency is necessary for batch norm
  FLAGS = namedtuple('FLAGS',flagdct.keys())(**flagdct)
  ff_params = dict(dim_hidden=20, rnn_stack_height=3)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


*** explore

#+BEGIN_SRC ipython
  %matplotlib inline
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython :exports both
  import numpy as np
  df['logdiffprice'] = np.log(df.price).diff()
  df[['logdiffprice', 'size', 'timestamp', 'price']].plot(x=['timestamp', 'timestamp'], y=['price', 'logdiffprice'], s=.7, kind='scatter', figsize=(30, 15))
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7fec4cb0c748>
[[file:./obipy-resources/8538-rS.png]]
:END:

#+BEGIN_SRC ipython :exports both
  from matplotlib import pyplot as plt
  fig, axes = plt.subplots(2, 1, sharex=True, figsize=(10,10))
  df[['size', 'timestamp', 'price']].plot(x='timestamp', y='price', s=.7, kind='scatter',
                                          figsize=(30, 15), ax=axes[0])
  df[['size', 'timestamp', 'logdiffprice']].plot(x='timestamp', y='logdiffprice', s=.7,
                                                 kind='scatter', figsize=(30, 15), ax=axes[1])
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7fec4c9b1400>
[[file:./obipy-resources/8538L2Y.png]]
:END:



#+BEGIN_SRC ipython :exports both
  df[['logdiffprice', 'size', 'timestamp']].plot(x='timestamp', y='logdiffprice', figsize=(30, 15))
#+END_SRC

#+RESULTS:
:RESULTS:
: <matplotlib.axes._subplots.AxesSubplot at 0x7fec4d75d518>
[[file:./obipy-resources/8538kXG.png]]
:END:

#+BEGIN_SRC ipython :exports both
  fig, axes = plt.subplots(2, 1, sharex=True, figsize=(10,10))
  for i, col in enumerate(data.columns.values):
     data.reset_index().plot.scatter(x='timestamp', y=col, ax=axes[i], s=.7)
#+END_SRC

#+RESULTS:
:RESULTS:
[[file:./obipy-resources/19656F3W.png]]
:END:

#+BEGIN_SRC ipython :exports both
  from pandas.plotting import scatter_matrix
  scatter_matrix(data, alpha=0.2, figsize=(6, 6), diagonal='kde')
#+END_SRC

#+RESULTS:
:RESULTS:
#+BEGIN_EXAMPLE
  array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f271599bbe0>,
  <matplotlib.axes._subplots.AxesSubplot object at 0x7f2715aaaeb8>],
  [<matplotlib.axes._subplots.AxesSubplot object at 0x7f27159c8be0>,
  <matplotlib.axes._subplots.AxesSubplot object at 0x7f271582e438>]], dtype=object)
#+END_EXAMPLE
[[file:./obipy-resources/19656SBd.png]]
:END:


*** debug

**** playground

#+BEGIN_SRC ipython
  import os
  os.chdir('marketbot')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  import  src.dataloader as dataloader
  get_data = dataloader.get_data
  path='data/data2.csv'; predict_length=10; interval_length=1; window_length=100
  features, outcomes = get_data(path, interval_length, predict_length)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  import src.playground1 as pg
  h = 10
  split=.5
  env = pg.Environment(features, h, split)  
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  tr = env.reset()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython :exports both
  cur, loss, doneflag, info = env.step(0.0)
  cur, loss, doneflag, info
#+END_SRC

#+RESULTS:
:RESULTS:
: (array([  3.00000000e-02,   1.31317100e+04,   2.22044605e-16]), 0.0, False, {})
:END:

#+BEGIN_SRC ipython
  from importlib import reload
  reload(pg)
#+END_SRC

#+RESULTS:
:RESULTS:
: <module 'src.playground1' from '/home/ishai/Documents/work/irabitcoin/marketbot/src/playground1.py'>
:END:


**** run

***** setup

#+BEGIN_SRC ipython
  import os
  os.chdir('marketbot')
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

Create input provider
#+BEGIN_SRC ipython
  import  src.dataloader as dataloader
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  get_data = dataloader.get_data
  WindowGen = dataloader.WindowGen
  quantize = dataloader.quantize
  path='data/data2.csv'; predict_length=10; interval_length=1; window_length=100
  features, outcomes = get_data(path, interval_length, predict_length)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  import numpy as np
  amin=-0.01
  amax=0.01
  step=1e-5
  Y_n_categories = int(np.round((amax-amin)/step))
#+END_SRC

  #+RESULTS:
  :RESULTS:
  :END:

#+BEGIN_SRC ipython
  q_outcomes = quantize(outcomes, amin=amin, amax=amax, step=step)
  q_outcomes = np.expand_dims(q_outcomes, axis=1)
  gen = WindowGen(features, q_outcomes, window_length, predict_length, Y_n_categories)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython :eval never
  from importlib import reload
  reload(dataloader)
#+END_SRC

#+BEGIN_SRC ipython
  import marketbot.src.model1.runner as runner
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  from collections import namedtuple
  flagdct = {'batch_size': 128,
             'data_dir': '/tmp/dat/',
             'hidden_dim': 200,
             'l1reg_coeff': 1e-10,
             'l2reg_coeff': 1e-9,
             # 'l1reg_coeff': 1,
             # 'l2reg_coeff': 1,
             'latent_dim': 160,
             'logdir': '/tmp/log/',
             'n_epochs': 100000,
             'n_iterations': 100000,
             'n_samples_predict': 20,
             'n_samples_train': 10,
             'print_every': 1000, 
             'huber_loss_delta': .1,
             'use_update_ops': False}  # update_ops control dependency is necessary for batch norm
  FLAGS = namedtuple('FLAGS',flagdct.keys())(**flagdct)
  ff_params = dict(dim_hidden=20, rnn_stack_height=3)

  catmodel = runner.Learner(ff_params, FLAGS)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  catmodel.initialize_train_graph(gen)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


***** train

#+BEGIN_SRC ipython
  catmodel.train(100)
#+END_SRC

#+BEGIN_SRC ipython
  from importlib import reload
  reload(runner)
#+END_SRC

#+RESULTS:
:RESULTS:
: <module 'marketbot.src.model1.runner' from '/home/ishai/Documents/work/irabitcoin/marketbot/src/model1/runner.py'>
:END:


***** predict

#+BEGIN_SRC ipython
  
#+END_SRC


**** genertor

#+BEGIN_SRC ipython
  import utils
  example_generator = utils.GrabSequence(X=data.values, t_ix=data.index.values, input_seq_len=100, time_gap_to_predict=10, stride=1)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  from importlib import reload
  reload(utils)
#+END_SRC

#+RESULTS:
:RESULTS:
: <module 'utils' from '/home/ishai/Documents/work/bitcoin/IraBC/marketbot/src/catnet/utils.py'>
:END:

#+BEGIN_SRC ipython
  import tensorflow as tf
  tf.reset_default_graph()
  g = tf.Graph()
  sess = tf.Session(graph=g)
  with g.as_default():
      train_ds = tf.data.Dataset.from_generator(example_generator, (tf.float32, tf.float32),
                                                (tf.TensorShape([None, 2]), tf.TensorShape([2])))
      train_ds.shuffle(buffer_size=100000)
      train_ds = train_ds.batch(100)
      iterator = tf.data.Iterator.from_structure(train_ds.output_types, train_ds.output_shapes)
      training_init_op = iterator.make_initializer(train_ds)
      batch = iterator.get_next()
#+END_SRC

#+RESULTS:
:RESULTS:
:END:

#+BEGIN_SRC ipython
  with sess.as_default():
      sess.run(training_init_op)
      while (True):
          b = sess.run(batch)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:


*** run

#+BEGIN_SRC ipython
  from runner import Learner
  catmodel = Learner(ff_params, FLAGS)
  catmodel.fit(data.values, t_ix=data.index.values)
#+END_SRC

#+RESULTS:
:RESULTS:
:END:



** test flux phonemes
:PROPERTIES:
:header-args: jupyter-julia :session marketbot-julia-ssh.json :results raw drawer :tangle mbot.jl :eval never-export :async t
:END:

*** 0.7.1

#+BEGIN_SRC jupyter-julia
  # Based on https://arxiv.org/abs/1409.0473

  using Flux: flip, crossentropy, reset!, throttle, glorot_uniform

  modeldir = "/home/ishai/git_repos/flux-model-zoo/text/phonemes/"
  include(joinpath(modeldir,"0-data.jl"));

  Nin = length(alphabet)
  Nh = 30 # size of hidden layer

  # A recurrent model which takes a token and returns a context-dependent
  # annotation.
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[9]:
: 30
:END:

#+BEGIN_SRC jupyter-julia
  forward  = LSTM(Nin, NhÃ·2)
  backward = LSTM(Nin, NhÃ·2)
  encode(tokens) = vcat.(forward.(tokens), flip(backward, tokens))

  # alignnet = Dense(2Nh, 1)
  # align(s, t) = alignnet(combine(t, s))

  # alignnet1 = Dense(Nh, 1)
  # alignnet2 = Dense(Nh, 1)
  # align(s, t) = alignnet1(s) .+ alignnet2(s)
  Ws = param(glorot_uniform(1, Nh))
  Wt = param(glorot_uniform(1, Nh))
  b = param(zeros(1))
  align(s,t) = Ws*s .+ Wt*t .+ b
    
  # A recurrent model which takes a sequence of annotations, attends, and returns
  # a predicted output token.

  recur   = LSTM(Nh+length(phones), Nh)
  toalpha = Dense(Nh, length(phones))

  function asoftmax(xs)
    xs = [exp.(x) for x in xs]
    s = sum(xs)
    return [x ./ s for x in xs]
  end

  function decode1(tokens, phone)
    weights = asoftmax([align(recur.state[2], t) for t in tokens])
    context = sum(map((a, b) -> a .* b, weights, tokens))
    y = recur(vcat(Int32.(phone), context))
    return softmax(toalpha(y))
  end

  decode(tokens, phones) = [decode1(tokens, phone) for phone in phones]

  # The full model

  state = (forward, backward, recur, toalpha) # Dense doesn't hold a state... right?

  function model(x, y)
    yÌ‚ = decode(encode(x), y)
    reset!(state)
    return yÌ‚
  end
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 1494fefc-42db-4122-8174-6196bf2456fe
:END:

#+BEGIN_SRC jupyter-julia
  loss(x, yo, y) = sum(crossentropy.(model(x, yo), y))

  evalcb = () -> @show loss(data[500]...)
  opt = ADAM(params(state))
#+END_SRC

#+RESULTS:
:RESULTS:
0 - e99cf4fb-5040-45ee-8a29-8c7c7a2058c5
:END:

#+BEGIN_SRC jupyter-julia
  Flux.train!(loss, data, opt, cb = throttle(evalcb, 10))
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 6a3acdf0-0375-4ed8-b671-3b4aeb3d459e
:END:

#+BEGIN_SRC jupyter-julia
  # Prediction

  using StatsBase: wsample

  function predict(s)
    ts = encode(tokenise(s, alphabet))
    ps = Any[:start]
    for i = 1:50
      dist = decode1(ts, onehot(ps[end], phones))
      next = wsample(phones, dist.data)
      next == :end && break
      push!(ps, next)
    end
    return ps[2:end]
  end

  predict("PHYLOGENY")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[284]:
#+BEGIN_EXAMPLE
  32-element Array{Any,1}:
  :AH0
  :L
  :AH0
  :AA1
  :N
  :K
  :K
  :L
  :IH0
  :V
  :F
  :R
  :IY1
  â‹®
  :N
  :N
  :ER0
  :F
  :IH0
  :N
  :T
  :IH0
  :N
  :IH0
  :L
  :IY0
#+END_EXAMPLE
:END:


*** 0.6.4
#+BEGIN_SRC jupyter-julia
  # Based on https://arxiv.org/abs/1409.0473

  using Flux: flip, crossentropy, reset!, throttle, glorot_uniform

  modeldir = "/home/ishai/git_repos/flux-model-zoo/text/phonemes/"
  include(joinpath(modeldir,"0-data.jl"));

  Nin = length(alphabet)
  Nh = 30 # size of hidden layer

  # A recurrent model which takes a token and returns a context-dependent
  # annotation.

  forward  = LSTM(Nin, NhÃ·2)
  backward = LSTM(Nin, NhÃ·2)
  encode(tokens) = vcat.(forward.(tokens), flip(backward, tokens))

  # alignnet = Dense(2Nh, 1)
  # align(s, t) = alignnet(combine(t, s))

  # alignnet1 = Dense(Nh, 1)
  # alignnet2 = Dense(Nh, 1)
  # align(s, t) = alignnet1(s) .+ alignnet2(s)
  Ws = param(glorot_uniform(1, Nh))
  Wt = param(glorot_uniform(1, Nh))
  b = param(zeros(1))
  align(s,t) = Ws*s .+ Wt*t .+ b
    
  # A recurrent model which takes a sequence of annotations, attends, and returns
  # a predicted output token.

  recur   = LSTM(Nh+length(phones), Nh)
  toalpha = Dense(Nh, length(phones))

  function asoftmax(xs)
    xs = [exp.(x) for x in xs]
    s = sum(xs)
    return [x ./ s for x in xs]
  end

  function decode1(tokens, phone)
    weights = asoftmax([align(recur.state[2], t) for t in tokens])
    context = sum(map((a, b) -> a .* b, weights, tokens))
    y = recur(vcat(Int32.(phone), context))
    return softmax(toalpha(y))
  end

  decode(tokens, phones) = [decode1(tokens, phone) for phone in phones]

  # The full model

  state = (forward, backward, recur, toalpha) # Dense doesn't hold a state... right?

  function model(x, y)
    yÌ‚ = decode(encode(x), y)
    reset!(state)
    return yÌ‚
  end

  loss(x, yo, y) = sum(crossentropy.(model(x, yo), y))

  evalcb = () -> @show loss(data[500]...)
  opt = ADAM(params(state))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[262]:
: (::#93) (generic function with 1 method)
:END:

#+BEGIN_SRC jupyter-julia
  Flux.train!(loss, data, opt, cb = throttle(evalcb, 10))
#+END_SRC

#+RESULTS:
:RESULTS:
0 - 6a3acdf0-0375-4ed8-b671-3b4aeb3d459e
:END:

#+BEGIN_SRC jupyter-julia
  # Prediction

  using StatsBase: wsample

  function predict(s)
    ts = encode(tokenise(s, alphabet))
    ps = Any[:start]
    for i = 1:50
      dist = decode1(ts, onehot(ps[end], phones))
      next = wsample(phones, dist.data)
      next == :end && break
      push!(ps, next)
    end
    return ps[2:end]
  end

  predict("PHYLOGENY")
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[284]:
#+BEGIN_EXAMPLE
  32-element Array{Any,1}:
  :AH0
  :L
  :AH0
  :AA1
  :N
  :K
  :K
  :L
  :IH0
  :V
  :F
  :R
  :IY1
  â‹®
  :N
  :N
  :ER0
  :F
  :IH0
  :N
  :T
  :IH0
  :N
  :IH0
  :L
  :IY0
#+END_EXAMPLE
:END:



** temp
:PROPERTIES:
:header-args: jupyter-julia :session marketbot-julia-ssh.json :results raw drawer :tangle mbot.jl :eval never-export :async t
:END:

#+BEGIN_SRC jupyter-julia
  accum(h, x) = (h+x, x)
  rnn = Flux.Recur(accum, 0)
  rnn(2) # 2
  rnn(3) # 3
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[276]:
: 3
:END:

#+BEGIN_SRC jupyter-julia
  rnn.state # 5
  rnn.(1:10) # apply to a sequence
  rnn.state # 60
#+END_SRC


* Feed
:PROPERTIES:
:header-args: ipython :session marketbot-ssh.json :results raw drawer :tangle zftestnn.py :eval never-export :async t
:END:

** test rest api

Lets look at all products available at coinbase
#+BEGIN_SRC ipython
import requests
ret = requests.get('https://api-public.sandbox.pro.coinbase.com/products')
products = json.loads(ret.text)
len(products)
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[94]:
: 15
:END:

Example product
#+BEGIN_SRC ipython
products[0]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[96]:
#+BEGIN_EXAMPLE
  {'id': 'ETH-BTC',
  'base_currency': 'ETH',
  'quote_currency': 'BTC',
  'base_min_size': '0.01',
  'base_max_size': '1000000',
  'quote_increment': '0.00001',
  'display_name': 'ETH/BTC',
  'status': 'online',
  'margin_enabled': False,
  'status_message': None,
  'min_market_funds': None,
  'max_market_funds': None,
  'post_only': False,
  'limit_only': False,
  'cancel_only': False}
#+END_EXAMPLE
:END:

Produt ids
#+BEGIN_SRC ipython
pairs = [{'base':p['base_currency'], 'quote':p['quote_currency'], 'id':p['id']} for p in products]
[p['id'] for p in pairs]
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[108]:
#+BEGIN_EXAMPLE
  ['ETH-BTC',
  'ETH-USD',
  'LTC-BTC',
  'LTC-USD',
  'ETH-EUR',
  'LTC-EUR',
  'BCH-USD',
  'BCH-BTC',
  'BCH-EUR',
  'BTC-USD',
  'BTC-GBP',
  'BTC-EUR',
  'ETC-USD',
  'ETC-EUR',
  'ETC-BTC']
#+END_EXAMPLE
:END:

#+BEGIN_SRC ipython
  uri = 'wss://ws-feed.pro.coinbase.com'
  subscribe_request = {
      'type': 'subscribe',
      'product_ids': ['BTC-USD'],
      'channels': [
          'matches',
          'heartbeat']
  }
#+END_SRC

#+BEGIN_SRC ipython
  from pprint import PrettyPrinter
  pprint = PrettyPrinter(indent=4).pprint
  from websocket import create_connection
  import json
  URI = 'wss://ws-feed.pro.coinbase.com'
  ws = create_connection(URI)
  SUBSCRIBE_REQUEST = {
      'type': 'subscribe',
      'product_ids': [p['id'] for p in pairs],
      'channels': [
          'matches',
          'heartbeat']
  }
  ws.send(json.dumps(SUBSCRIBE_REQUEST))
#+END_SRC

Sample output  
#+BEGIN_SRC ipython :results output
  pprint([json.loads(ws.recv()) for i in range(3)])
#+END_SRC

#+RESULTS:
:RESULTS:
[   {   'maker_order_id': '18b2c024-358a-4ec1-8d6a-020dfe878d97',
        'price': '51.66000000',
        'product_id': 'LTC-USD',
        'sequence': 2560366729,
        'side': 'sell',
        'size': '1.86851264',
        'taker_order_id': 'f36d7c09-ccd7-490c-b4cb-0c0704dcd715',
        'time': '2018-08-14T03:38:40.285000Z',
        'trade_id': 33028617,
        'type': 'last_match'},
    {   'maker_order_id': 'ce5fc695-f0f6-40cb-8279-42fea37a32ac',
        'price': '232.00000000',
        'product_id': 'ETH-EUR',
        'sequence': 983928315,
        'side': 'sell',
        'size': '6.18652428',
        'taker_order_id': '68adcfd3-00a3-44e1-89b0-f29ff33e3333',
        'time': '2018-08-14T03:38:23.275000Z',
        'trade_id': 4502984,
        'type': 'last_match'},
    {   'maker_order_id': '66bb907e-25b0-45c9-8637-1fd3aaf39a1d',
        'price': '45.35000000',
        'product_id': 'LTC-EUR',
        'sequence': 715075307,
        'side': 'sell',
        'size': '2.39867409',
        'taker_order_id': 'a9afab9e-e2a3-4395-ba34-ee30f71847dd',
        'time': '2018-08-14T03:37:18.769000Z',
        'trade_id': 3394856,
        'type': 'last_match'}]
:END:


**  test websocket (singular)

#+BEGIN_SRC ipython
  URI = 'wss://ws-feed.pro.coinbase.com'
  SUBSCRIBE_REQUEST = {
      'type': 'subscribe',
      'product_ids': ['BTC-USD'],
      'channels': [
          'matches',
          'heartbeat']
  }
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[54]:
:END:

#+BEGIN_SRC ipython
  from websocket import create_connection
  import json
  ws = create_connection(URI)
  ws.send(json.dumps(SUBSCRIBE_REQUEST))
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[55]:
: 93
:END:

#+BEGIN_SRC ipython
  result =  ws.recv()
  print ("Received '%s'" % result)
  ws.close()
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[56]:
:END:


** Test websockets
reference - 

#+BEGIN_SRC ipython
  import time
  import websockets
  import asyncio
  URI = 'wss://ws-feed.gdax.com'
  SUBSCRIBE_REQUEST = {
      'type': 'subscribe',
      'product_ids': ['BTC-USD'],
      'channels': [
          'matches',
          'heartbeat']
  }
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[26]:
:END:

#+BEGIN_SRC ipython
  import logging

  async def _listen(ws):
      while True:
          try:
              msg = await asyncio.wait_for(ws.recv(), timeout=20)
          except asyncio.TimeoutError:
              # No data in 20 seconds, check the connection.
              try:
                  pong_waiter = await ws.ping()
                  await asyncio.wait_for(pong_waiter, timeout=10)
              except asyncio.TimeoutError:
                  # No response to ping in 10 seconds, disconnect.
                  break
          else:
              handle_message(json.loads(msg))
      return

  async def _connect(path):
      async with websockets.connect(path) as ws:
          print('connecting')
          await ws.send(json.dumps(SUBSCRIBE_REQUEST))
          print('Connected')
          message = await ws.recv()
          print(' {} '.format(message))
          await _listen(ws)

  def match_counter():
      global NUM_MATCHES
      NUM_MATCHES += 1
      if NUM_MATCHES % 10 == 0:
          sys.stdout.write("\r matches entered: {}".format(NUM_MATCHES))
          sys.stdout.flush()

  def handle_message(msg):
      if msg['type'] == 'match':
          match_counter()
          logging.info(msg)
          with open(FILEPATH, 'a+') as f:
              writer = csv.writer(f)
              writer.writerow([msg[key] for key in COLUMNS])
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[8]:
:END:

#+BEGIN_SRC ipython
  asyncio.get_event_loop().run_until_complete(_connect(URI))
  foo = asyncio.ensure_future(_connect(URI))  
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[35]:
:END:

#+BEGIN_SRC ipython
  SUBSCRIBE_REQUEST = {
      'type': 'subscribe',
      'product_ids': ['BTC-USD'],
      'channels': [
          'matches',
          'heartbeat']
  }
#+END_SRC

#+RESULTS:
:RESULTS:
# Out[27]:
:END:

#+BEGIN_SRC ipython
  import json
  ws.send(json.dumps(SUBSCRIBE_REQUEST))
#+END_SRC

#+BEGIN_SRC ipython
from src.utils import collect
#+END_SRC
